{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkDr5ozfMdva"
      },
      "outputs": [],
      "source": [
        "!pip install azure-storage-blob azure-identity --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mWdmgSjNIUv"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from io import BytesIO\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from google.colab import userdata\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torchvision import transforms\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoImageProcessor, ConvNextModel, SwinModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_dates = [\"20170418\"] # \"20170422\", \"20170508\", \"20170512\"\n",
        "\n",
        "train_dates = [\"20170418\"]"
      ],
      "metadata": {
        "id": "5heH7jycP9WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myYWEh4NGuwm"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbeAsW0wOfxk"
      },
      "outputs": [],
      "source": [
        "# Authentication details\n",
        "account_name = userdata.get('storage_account_name')\n",
        "account_key = userdata.get('storage_account_key')\n",
        "container_name = userdata.get('blob_container_name')\n",
        "\n",
        "# Connection string to Azure Blob Storage\n",
        "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
        "\n",
        "# Setup to load file from blob\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "container_client = blob_service_client.get_container_client(container_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhxE_YnrNIKK"
      },
      "outputs": [],
      "source": [
        "aircraft_metadata_params = ['DateTime_UTC', 'GPS_MSL_Alt', 'Drift', 'Pitch', 'Roll', 'Vert_Velocity']\n",
        "CTH_col = 'top_height'\n",
        "\n",
        "# Aircraft Metadata\n",
        "def load_metadata(blob_name):\n",
        "    blob_client = container_client.get_blob_client(blob_name)\n",
        "    streamdownloader = blob_client.download_blob()\n",
        "    metadata_df = pd.read_csv(io.BytesIO(streamdownloader.readall()))\n",
        "    return metadata_df\n",
        "\n",
        "# LiDAR Validation Heights\n",
        "def load_validation_heights(blob_name):\n",
        "    blob_client = container_client.get_blob_client(blob_name)\n",
        "    streamdownloader = blob_client.download_blob()\n",
        "    validation_df = pd.read_csv(io.BytesIO(streamdownloader.readall()))\n",
        "    return validation_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc5l4RkHOy-W"
      },
      "source": [
        "# CloudDataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R71xTUXTOZg7"
      },
      "outputs": [],
      "source": [
        "# CloudDataset classes integrating all 3 data sources: FEGS Images, Aircraft Metadata and LiDAR Validation Heights with temporal alignment\n",
        "class CloudDataset(Dataset):\n",
        "    def __init__(self, date_folders, transform=None):\n",
        "        self.date_folders = date_folders\n",
        "        self.transform = transform\n",
        "        self.data_df = self._prepare_dataframe()\n",
        "\n",
        "    def _prepare_dataframe(self):\n",
        "        \"\"\"\n",
        "        Iterates over the date folders in azure blob storage and loads:\n",
        "          1. .jpg Images from each sub-directory in the folder with '_tight_crop' in the name.\n",
        "          2. Aircraft Metadata with 1-1 time alignment with the images.\n",
        "          3. LiDAR Validation Heights, mapped using timestamp, if not available filled with NaN.\n",
        "        Creates a df with following columns:\n",
        "            timestamp, image_path, [...aircraft_metadata_params...], validation_height\n",
        "        \"\"\"\n",
        "        image_paths, timestamps, metadata_rows, validation_heights = [], [], [], []\n",
        "\n",
        "        for folder in self.date_folders:\n",
        "            print(f\"Processing folder: {folder}\")\n",
        "            folder_image_paths, folder_timestamps, folder_metadata_rows, folder_validation_heights = [], [], [], []\n",
        "\n",
        "            blob_list = container_client.list_blobs(name_starts_with=folder)\n",
        "            metadata_path, validation_path = None, None\n",
        "            for blob in blob_list:\n",
        "                # extract image paths of all .jpg images in cropped folders\n",
        "                if blob.name.endswith(\".jpg\") and \"_tight_crop\" in blob.name:\n",
        "                    folder_image_paths.append(blob.name)\n",
        "                    folder_timestamps.append(self._extract_timestamp_from_filename(blob.name))\n",
        "                # extract the aircraft metadata file path\n",
        "                if blob.name.startswith(f\"{folder}/IWG1.\") and \"processed\" in blob.name:\n",
        "                    metadata_path = blob.name\n",
        "                # extract the LiDAR validation file path\n",
        "                if blob.name.startswith(f\"{folder}/goesrplt_CPL_layers_\") and blob.name.endswith(\"_processed.txt\"):\n",
        "                    validation_path = blob.name\n",
        "\n",
        "            # load aircraft metadata and LiDAR validation data\n",
        "            if metadata_path:\n",
        "                metadata_df = load_metadata(metadata_path)\n",
        "            if validation_path:\n",
        "                validation_df = load_validation_heights(validation_path)\n",
        "\n",
        "            # prepare LiDAR validation data\n",
        "            validation_df['datetime_combined'] = validation_df['date'] + ' ' + validation_df['timestamp']\n",
        "            validation_df['datetime_combined'] = validation_df['datetime_combined'].str.split('.').str[0]\n",
        "            validation_df['datetime_combined'] = pd.to_datetime(validation_df['datetime_combined'], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "            # prepare aircraft metadata\n",
        "            metadata_df = metadata_df[aircraft_metadata_params]\n",
        "            metadata_df['DateTime_UTC'] = metadata_df['DateTime_UTC'].str.split('.').str[0]\n",
        "            metadata_timestamps = pd.to_datetime(metadata_df['DateTime_UTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "            metadata_df = metadata_df.set_index(metadata_timestamps)\n",
        "            aligned_metadata = pd.DataFrame(index=pd.to_datetime(folder_timestamps, format=\"%H:%M:%S\"))\n",
        "            aligned_metadata = aligned_metadata.join(metadata_df, how='left')\n",
        "            aligned_metadata = aligned_metadata[aircraft_metadata_params]\n",
        "\n",
        "            folder_metadata_rows.extend(aligned_metadata.values.tolist())\n",
        "\n",
        "            # extract LiDAR validation height exactly matching the timestamp where available, else NaN\n",
        "            for ts in folder_timestamps:\n",
        "                cth = self._map_timestamp_to_lidar(ts, validation_df)\n",
        "                folder_validation_heights.append(cth)\n",
        "\n",
        "            # Create a folder-level DataFrame\n",
        "            folder_data = {\n",
        "                'timestamp': folder_timestamps,\n",
        "                'image_path': folder_image_paths,\n",
        "                **{param: [row[i] for row in folder_metadata_rows] for i, param in enumerate(aircraft_metadata_params)},\n",
        "                'validation_height': folder_validation_heights\n",
        "            }\n",
        "            folder_df = pd.DataFrame(folder_data)\n",
        "\n",
        "            # Remove rows after the last valid validation_height in this folder\n",
        "            last_valid_index = folder_df['validation_height'].last_valid_index()\n",
        "            if last_valid_index is not None:\n",
        "                folder_df_cleaned = folder_df.loc[:last_valid_index].copy()  # Use .copy() to ensure independence\n",
        "            else:\n",
        "                folder_df_cleaned = folder_df.copy()  # In case there are no valid entries\n",
        "\n",
        "            # Extend to the global lists\n",
        "            image_paths.extend(folder_df_cleaned['image_path'].tolist())\n",
        "            timestamps.extend(folder_df_cleaned['timestamp'].tolist())\n",
        "            metadata_rows.extend(folder_df_cleaned[aircraft_metadata_params].values.tolist())\n",
        "            validation_heights.extend(folder_df_cleaned['validation_height'].tolist())\n",
        "\n",
        "            # Print the lengths for the current folder\n",
        "            print(f\"Folder {folder}:\")\n",
        "            print(f\"  Number of images: {len(folder_df_cleaned['image_path'])}\")\n",
        "            print(f\"  Number of timestamps: {len(folder_df_cleaned['timestamp'])}\")\n",
        "            print(f\"  Number of metadata rows: {len(folder_df_cleaned)}\")\n",
        "            print(f\"  Number of validation heights: {len(folder_df_cleaned['validation_height'])}\")\n",
        "\n",
        "        # Print the final lengths after processing all folders\n",
        "        print(\"After processing all folders combined:\")\n",
        "        print(f\"  Total number of images: {len(image_paths)}\")\n",
        "        print(f\"  Total number of timestamps: {len(timestamps)}\")\n",
        "        print(f\"  Total number of metadata rows: {len(metadata_rows)}\")\n",
        "        print(f\"  Total number of validation heights: {len(validation_heights)}\")\n",
        "\n",
        "        # Check for any mismatches\n",
        "        if not (len(image_paths) == len(timestamps) == len(metadata_rows) == len(validation_heights)):\n",
        "            print(\"Error: Length mismatch detected!\")\n",
        "            print(f\"  Images: {len(image_paths)}\")\n",
        "            print(f\"  Timestamps: {len(timestamps)}\")\n",
        "            print(f\"  Metadata rows: {len(metadata_rows)}\")\n",
        "            print(f\"  Validation heights: {len(validation_heights)}\")\n",
        "            return None\n",
        "\n",
        "        # combine all aligned data in a df\n",
        "        data = {\n",
        "            'timestamp': timestamps,\n",
        "            'image_path': image_paths,\n",
        "            **{param: [row[i] for row in metadata_rows] for i, param in enumerate(aircraft_metadata_params)},\n",
        "            'validation_height': validation_heights\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "        df = df.drop(columns=['DateTime_UTC'])\n",
        "\n",
        "        # Add sequence length information for RNN\n",
        "        self._add_sequence_length_column(df)\n",
        "        return df\n",
        "\n",
        "    def _extract_timestamp_from_filename(self, filename):\n",
        "        \"\"\"\n",
        "        Extracts the timestamp from the image filename on the blob.\n",
        "        path/to/blob/YYYYMMDD_HHMMSS_frame_n_cropped.jpg -> %Y%m%d%H%M%S\n",
        "        \"\"\"\n",
        "        filename = os.path.basename(filename)\n",
        "        date_str = filename.split(\"_\")[0]\n",
        "        time_str = filename.split(\"_\")[1]\n",
        "        timestamp = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M%S\")\n",
        "        return timestamp\n",
        "\n",
        "\n",
        "    def _map_timestamp_to_lidar(self, timestamp, validation_df):\n",
        "        \"\"\"\n",
        "        extract LiDAR validation height exactly matching the timestamp where available, else NaN\n",
        "        \"\"\"\n",
        "        validation_df['datetime_combined'] = pd.to_datetime(validation_df['datetime_combined'], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "        timestamp_dt = pd.to_datetime(timestamp, format=\"%Y-%m-%d %H:%M:%S\")\n",
        "        exact_match = validation_df[validation_df['datetime_combined'] == timestamp_dt]\n",
        "        return exact_match[CTH_col].values[0] if not exact_match.empty else np.nan\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve a data record from the dataset for a given index.\n",
        "        Returns loaded and transformed image, metadata and validation height associated with that image if available.\n",
        "        \"\"\"\n",
        "        row = self.data_df.iloc[idx]\n",
        "\n",
        "        image_path = row['image_path']\n",
        "        blob_client = container_client.get_blob_client(image_path)\n",
        "        streamdownloader = blob_client.download_blob()\n",
        "        img_data = streamdownloader.readall()\n",
        "        img = Image.open(io.BytesIO(img_data)).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        metadata = torch.tensor([row[param] for param in aircraft_metadata_params], dtype=torch.float32)\n",
        "        validation_height = torch.tensor([row['validation_height']], dtype=torch.float32)\n",
        "\n",
        "        return img, metadata, validation_height\n",
        "\n",
        "    def _add_sequence_length_column(self, df):\n",
        "        # Initialize a new column to NaN\n",
        "        df['sequence_length'] = np.nan\n",
        "\n",
        "        # Track the start of each sequence\n",
        "        sequence_start = 0\n",
        "\n",
        "        # Iterate through the dataframe to detect when a validation_height exists\n",
        "        for i in range(len(df)):\n",
        "            if not pd.isna(df.loc[i, 'validation_height']):\n",
        "                # We found the end of a sequence, so mark the previous sequence images\n",
        "                sequence_length = i - sequence_start\n",
        "                df.loc[sequence_start:i, 'sequence_length'] = sequence_length + 1  # Using count (1-based index)\n",
        "                sequence_start = i + 1  # Move the start to the next sequence\n",
        "\n",
        "        # Ensure all sequence_length values are integers (if any were missed)\n",
        "        df['sequence_length'] = df['sequence_length'].astype(int)\n",
        "\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbA91uVtO-YY"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "augmentations = transforms.Compose([\n",
        "    #transforms.ColorJitter(brightness=0.9, contrast=1.6, saturation=1.2, hue=0.1),\n",
        "])\n",
        "\n",
        "# Create the full CloudDataset without splitting initially\n",
        "full_dataset = CloudDataset(train_dates, transform=None)\n",
        "\n",
        "# Extract the full dataframe from CloudDataset\n",
        "full_dataframe = full_dataset.data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zW827VtQBV_"
      },
      "outputs": [],
      "source": [
        "full_dataframe.to_csv('train_dataset.csv', index=False, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVX3NDr-PB_z"
      },
      "outputs": [],
      "source": [
        "full_dataframe.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qB2uNPhqnJqw"
      },
      "outputs": [],
      "source": [
        "full_dataframe.tail(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n_NEMQvS9Vj"
      },
      "source": [
        "# Image loading and augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIDG3YP6PEFm"
      },
      "outputs": [],
      "source": [
        "def load_image_from_blob_cv(blob_img):\n",
        "    \"\"\"\n",
        "    Loads the image from the Azure Blob Storage using OpenCV and returns it as a numpy array.\n",
        "    Args:\n",
        "        blob_img (str): name of the blob image in the container\n",
        "    Returns:\n",
        "        (numpy.ndarray): loaded image in greyscale\n",
        "    \"\"\"\n",
        "    blob_client = container_client.get_blob_client(blob_img)\n",
        "    streamdownloader = blob_client.download_blob()\n",
        "    blob_data = streamdownloader.readall()\n",
        "    image_array = np.asarray(bytearray(blob_data), dtype=np.uint8)\n",
        "    img_bgr = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    return img_rgb\n",
        "\n",
        "def augment_color_image(img, contrast_factor=1.2, brightness_beta=10, kernel_size=(5, 5), blur=True):\n",
        "    \"\"\"\n",
        "    Augments the color image to enhance cloud features. Includes:\n",
        "      1. Contrast and Brightness Adjustment\n",
        "      2. Gaussian Blur (optional)\n",
        "      3. CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
        "      4. Sharpening (optional)\n",
        "\n",
        "    Args:\n",
        "        img (numpy.ndarray): color image in RGB format\n",
        "        contrast_factor (float): contrast factor\n",
        "        brightness_beta (int): brightness factor\n",
        "        kernel_size (tuple): kernel size for Gaussian Blur\n",
        "        blur (bool): whether to apply Gaussian Blur\n",
        "\n",
        "    Returns:\n",
        "        (numpy.ndarray): augmented image\n",
        "    \"\"\"\n",
        "    # Step 1: Adjust contrast and brightness\n",
        "    img_enhanced = cv2.convertScaleAbs(img, alpha=contrast_factor, beta=brightness_beta)\n",
        "\n",
        "    # Step 2: Optionally apply Gaussian Blur\n",
        "    if blur:\n",
        "        img_enhanced = cv2.GaussianBlur(img_enhanced, kernel_size, 0)\n",
        "\n",
        "    # Step 3: Convert to LAB color space to apply CLAHE on the L channel\n",
        "    img_lab = cv2.cvtColor(img_enhanced, cv2.COLOR_RGB2LAB)\n",
        "    l, a, b = cv2.split(img_lab)\n",
        "\n",
        "    # Step 4: Apply CLAHE (Clip Limit Adaptive Histogram Equalization)\n",
        "    clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(4, 4))\n",
        "    cl = clahe.apply(l)\n",
        "\n",
        "    # Step 5: Merge CLAHE-enhanced L channel back with A and B channels\n",
        "    img_clahe = cv2.merge((cl, a, b))\n",
        "\n",
        "    # Step 6: Convert back to RGB color space\n",
        "    img_clahe_rgb = cv2.cvtColor(img_clahe, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "    # Step 7: Optionally apply sharpening\n",
        "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
        "    img_sharpened = cv2.filter2D(img_clahe_rgb, -1, kernel)\n",
        "\n",
        "    return img_sharpened\n",
        "\n",
        "# Modify augment_color_image to use the original CLAHE-only method\n",
        "def augment_color_image_clahe_only(img, clip_limit=1.5, tile_grid_size=(4, 4)):\n",
        "    \"\"\"\n",
        "    Augments the color image by applying CLAHE (Contrast Limited Adaptive Histogram Equalization) only.\n",
        "\n",
        "    Args:\n",
        "        img (numpy.ndarray): color image in RGB format\n",
        "        clip_limit (float): clip limit for CLAHE (higher values give more contrast)\n",
        "        tile_grid_size (tuple): size of the grid for applying CLAHE\n",
        "\n",
        "    Returns:\n",
        "        (numpy.ndarray): augmented image with CLAHE applied to the L channel in LAB color space\n",
        "    \"\"\"\n",
        "    # Convert to LAB color space to apply CLAHE on the L channel (lightness)\n",
        "    img_lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
        "    l, a, b = cv2.split(img_lab)\n",
        "\n",
        "    # Apply CLAHE to the L channel\n",
        "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
        "    cl = clahe.apply(l)\n",
        "\n",
        "    # Merge CLAHE-enhanced L channel back with A and B channels\n",
        "    img_clahe = cv2.merge((cl, a, b))\n",
        "\n",
        "    # Convert back to RGB color space\n",
        "    img_clahe_rgb = cv2.cvtColor(img_clahe, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "    return img_clahe_rgb\n",
        "\n",
        "def augment_greyscale_image(img, contrast_factor=1.5, brightness_beta=30, kernel_size=(5, 5), blur=True):\n",
        "    \"\"\"\n",
        "    Augments the Greyscale image to enhance the feature of the cloud. Includes:\n",
        "      1. Contrast and Brightness\n",
        "      2. Gaussian Blur\n",
        "      3. Histogram Equalization\n",
        "      4. Sharpening\n",
        "    Args:\n",
        "        img (numpy.ndarray): greyscale image\n",
        "        contrast_factor (float): contrast factor\n",
        "        brightness_beta (int): brightness factor\n",
        "        kernel_size (tuple): kernel size for Gaussian Blur\n",
        "        blur (bool): whether to apply Gaussian Blur\n",
        "    Returns:\n",
        "        (numpy.ndarray): augmented image\n",
        "    \"\"\"\n",
        "    img_enhanced = cv2.convertScaleAbs(img, alpha=contrast_factor, beta=brightness_beta)\n",
        "    if blur:\n",
        "        img_enhanced = cv2.GaussianBlur(img_enhanced, kernel_size, 0)\n",
        "    img_enhanced = cv2.equalizeHist(img_enhanced)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    img_enhanced = clahe.apply(img_enhanced)\n",
        "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
        "    return cv2.filter2D(img_enhanced, -1, kernel)\n",
        "\n",
        "def undistort_fisheye_image(img, K, D, balance=0.5):\n",
        "    \"\"\"\n",
        "    Undistorts a fisheye image using the intrinsic camera matrix and distortion coefficients.\n",
        "\n",
        "    Args:\n",
        "        img (numpy.ndarray): The distorted fisheye image.\n",
        "        K (numpy.ndarray): The camera matrix.\n",
        "        D (numpy.ndarray): The distortion coefficients.\n",
        "        balance (float): Balance between the undistorted and distorted image. 0.0 means fully zoomed in,\n",
        "                         1.0 means fully zoomed out.\n",
        "\n",
        "    Returns:\n",
        "        undistorted_img (numpy.ndarray): The undistorted image.\n",
        "    \"\"\"\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    # Generate new camera matrix based on free scaling parameter (alpha=1 keeps all the image)\n",
        "    new_K = cv2.fisheye.estimateNewCameraMatrixForUndistortRectify(K, D, (w, h), np.eye(3), balance=balance)\n",
        "\n",
        "    # Undistort the image\n",
        "    map1, map2 = cv2.fisheye.initUndistortRectifyMap(K, D, np.eye(3), new_K, (w, h), cv2.CV_16SC2)\n",
        "    undistorted_img = cv2.remap(img, map1, map2, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n",
        "\n",
        "    return undistorted_img\n",
        "\n",
        "def visualize_augmentations_cv(image_list, enhance=True):\n",
        "    \"\"\"\n",
        "    Visualizes original and augmented greyscale images using OpenCV.\n",
        "    Args:\n",
        "        image_list (list): List of image paths in Azure Blob Storage\n",
        "        enhance (bool): Whether to apply image augmentation\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(len(image_list), 3, figsize=(15, 5 * len(image_list)))\n",
        "\n",
        "    for idx, blob_img in enumerate(image_list):\n",
        "        original_image = load_image_from_blob_cv(blob_img)\n",
        "        greyscale_image = cv2.cvtColor(original_image, cv2.COLOR_RGB2GRAY)\n",
        "        augmented_image = greyscale_image.copy()\n",
        "\n",
        "        if enhance:\n",
        "            augmented_image = augment_greyscale_image(augmented_image, contrast_factor=1.5, brightness_beta=30, kernel_size=(5, 5))\n",
        "\n",
        "        axes[idx, 0].imshow(original_image)\n",
        "        axes[idx, 0].set_title(\"Original Image (RGB)\")\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        axes[idx, 1].imshow(greyscale_image, cmap='gray')\n",
        "        axes[idx, 1].set_title(\"Greyscale Image\")\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        axes[idx, 2].imshow(augmented_image, cmap='gray')\n",
        "        axes[idx, 2].set_title(\"Augmented Image\")\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def random_crop_sequence(image_sequence, crop_size_minimum=224):\n",
        "    \"\"\"\n",
        "    Apply identical random crop to a sequence of images and ensure randomness in cropping location.\n",
        "    The crop size is randomly selected between the minimum size (crop_size_minimum) and the full image size.\n",
        "\n",
        "    Args:\n",
        "        image_sequence: A list or tensor of images in the sequence (each image should be the same size).\n",
        "        crop_size_minimum: The minimum crop size (default is 224).\n",
        "\n",
        "    Returns:\n",
        "        cropped_sequence: The cropped sequence of images.\n",
        "        new_center_coords: The new coordinates of the center pixel after cropping.\n",
        "    \"\"\"\n",
        "    # Get the height and width of the original images\n",
        "    orig_height, orig_width = image_sequence[0].shape[-2:]\n",
        "\n",
        "    # Randomly select the crop size between the minimum and full image size\n",
        "    max_crop_size = min(orig_height, orig_width)\n",
        "    crop_h = random.randint(crop_size_minimum, max_crop_size)\n",
        "    crop_w = crop_h  # Keeping the crop square for simplicity\n",
        "\n",
        "    # Ensure that the crop size is smaller than the original image\n",
        "    assert crop_h <= orig_height and crop_w <= orig_width, \"Crop size must be smaller than the original image size.\"\n",
        "\n",
        "    # Original center coordinates of the image\n",
        "    orig_center_x = orig_width // 2\n",
        "    orig_center_y = orig_height // 2\n",
        "\n",
        "    # Randomly select the top-left corner for the crop\n",
        "    top = random.randint(0, orig_height - crop_h)\n",
        "    left = random.randint(0, orig_width - crop_w)\n",
        "\n",
        "    # Apply the same crop to each image in the sequence\n",
        "    cropped_sequence = [TF.crop(img, top, left, crop_h, crop_w) for img in image_sequence]\n",
        "\n",
        "    # Calculate the new center coordinates based on the original center relative to the cropped region\n",
        "    new_center_x = orig_center_x - left  # Adjust the original center x based on the crop left offset\n",
        "    new_center_y = orig_center_y - top   # Adjust the original center y based on the crop top offset\n",
        "\n",
        "    # Ensure the new center coordinates are still within the cropped image bounds\n",
        "    new_center_x = max(0, min(new_center_x, crop_w - 1))\n",
        "    new_center_y = max(0, min(new_center_y, crop_h - 1))\n",
        "\n",
        "    return cropped_sequence, (new_center_x, new_center_y)\n",
        "\n",
        "def resize_sequence_and_adjust_center(cropped_sequence, new_center_coords, target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Resize the cropped sequence to a target size and adjust the center coordinates accordingly.\n",
        "\n",
        "    Args:\n",
        "        cropped_sequence: The list of cropped images.\n",
        "        new_center_coords: The (x, y) coordinates of the center in the cropped image.\n",
        "        target_size: The desired output size (height, width), default is (224, 224).\n",
        "\n",
        "    Returns:\n",
        "        resized_sequence: The resized sequence of images.\n",
        "        resized_center_coords: The adjusted (x, y) coordinates of the center in the resized images.\n",
        "    \"\"\"\n",
        "    crop_h, crop_w = cropped_sequence[0].shape[-2:]  # Get height and width of the cropped image\n",
        "    target_h, target_w = target_size  # Desired target size (e.g., 224x224)\n",
        "\n",
        "    # Calculate the scaling factors for height and width\n",
        "    scale_x = target_w / crop_w\n",
        "    scale_y = target_h / crop_h\n",
        "\n",
        "    # Resize each image in the sequence to the target size\n",
        "    resized_sequence = [TF.resize(img, size=target_size) for img in cropped_sequence]\n",
        "\n",
        "    # Adjust the center coordinates according to the scaling factor\n",
        "    new_center_x, new_center_y = new_center_coords\n",
        "    resized_center_x = int(new_center_x * scale_x)\n",
        "    resized_center_y = int(new_center_y * scale_y)\n",
        "\n",
        "    return resized_sequence, (resized_center_x, resized_center_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlDGEu4RUMv4"
      },
      "outputs": [],
      "source": [
        "images = [\n",
        "    \"20170418/170418_175706_183328_frames/20170418_175706_frame_0.jpg\",\n",
        "    \"20170418/170418_175706_183328_frames/20170418_175707_frame_60.jpg\",\n",
        "    \"20170418/170418_175706_183328_frames_cropped/20170418_175708_frame_120_cropped.jpg\",\n",
        "    \"20170418/170418_175706_183328_frames_cropped/20170418_175709_frame_180_cropped.jpg\",\n",
        "    \"20170418/170418_175706_183328_frames_cropped/20170418_175710_frame_240_cropped.jpg\",\n",
        "    \"20170418/170418_175706_183328_frames_tight_crop/20170418_175708_frame_120_cropped.jpg\",\n",
        "    \"20170418/170418_175706_183328_frames_tight_crop/20170418_175709_frame_180_cropped.jpg\",\n",
        "    \"20170418/170418_175706_183328_frames_tight_crop/20170418_175710_frame_240_cropped.jpg\",\n",
        "]\n",
        "\n",
        "visualize_augmentations_cv(images, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgGn2pC8c6iw"
      },
      "source": [
        "# CNN-RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7y52OAldzLI"
      },
      "outputs": [],
      "source": [
        "class Cloud2CloudDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 dataframe,\n",
        "                 normalization_params=None,\n",
        "                 transform=None, augmentations=None,\n",
        "                 apply_normalization=True,\n",
        "                 apply_crop_and_scale=True,\n",
        "                 resize_size=224):  # Add resize_size parameter with default value 224\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe (pd.DataFrame): dataframe containing image paths, flight data, and ground truth.\n",
        "            normalization_params (dict, optional): Dictionary containing min and max values for all the fields.\n",
        "            transform (callable, optional): optional transform to apply to each image.\n",
        "            augmentations (callable, optional): optional augmentations to apply to each image.\n",
        "            apply_normalization (bool): whether to apply normalization to the dataframe.\n",
        "            resize_size (int): The target size to resize the images (square dimensions).\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "        self.augmentations = augmentations\n",
        "        self.apply_normalization = apply_normalization\n",
        "        self.apply_crop_and_scale = apply_crop_and_scale\n",
        "        self.resize_size = resize_size  # Save resize_size to be used later\n",
        "        self.columns_to_normalize = ['validation_height', 'GPS_MSL_Alt', 'Drift', 'Pitch', 'Roll', 'Vert_Velocity']\n",
        "\n",
        "        if self.apply_normalization:\n",
        "            # Calculate or use provided normalization parameters\n",
        "            self.normalization_params = normalization_params or self._calculate_normalization_params(dataframe, self.columns_to_normalize)\n",
        "\n",
        "            for col in self.columns_to_normalize:\n",
        "                col_min = self.normalization_params[col]['min']\n",
        "                col_max = self.normalization_params[col]['max']\n",
        "                self.dataframe[col] = (self.dataframe[col] - col_min) / (col_max - col_min)\n",
        "\n",
        "        # Track the indices where sequences start\n",
        "        self.sequence_indices = self._generate_sequence_indices()\n",
        "\n",
        "    def _calculate_normalization_params(self, dataframe, columns_to_normalize):\n",
        "        \"\"\"\n",
        "        Manually calculate min and max values for the columns and store them for consistency across datasets.\n",
        "        \"\"\"\n",
        "        params = {}\n",
        "        for col in columns_to_normalize:\n",
        "            params[col] = {\n",
        "                'min': dataframe[col].min(),\n",
        "                'max': dataframe[col].max()\n",
        "            }\n",
        "        return params\n",
        "\n",
        "    def denormalize_validation_height(self, normalized_height):\n",
        "        \"\"\"\n",
        "        Denormalize the validation height using the stored min and max values for validation_height.\n",
        "\n",
        "        Args:\n",
        "            normalized_height (float or np.array): The normalized validation height(s) to denormalize.\n",
        "\n",
        "        Returns:\n",
        "            float or np.array: The denormalized validation height(s).\n",
        "        \"\"\"\n",
        "        # Get min and max for validation_height\n",
        "        col_min = self.normalization_params['validation_height']['min']\n",
        "        col_max = self.normalization_params['validation_height']['max']\n",
        "\n",
        "        print(f\"  Max: {col_max} Min: {col_min}\")\n",
        "\n",
        "        # Denormalize using the stored min and max values\n",
        "        original_height = normalized_height * (col_max - col_min) + col_min\n",
        "        return original_height\n",
        "\n",
        "    def denormalize_flight_data(self, normalized_flight_data):\n",
        "        \"\"\"\n",
        "        Denormalize flight data using the stored min and max values for each column.\n",
        "\n",
        "        Args:\n",
        "            normalized_flight_data (np.array): Normalized flight data array to denormalize.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Denormalized flight data.\n",
        "        \"\"\"\n",
        "        denormalized_flight_data = []\n",
        "        for i, col in enumerate(self.columns_to_normalize[1:]):  # Skip validation_height\n",
        "            col_min = self.normalization_params[col]['min']\n",
        "            col_max = self.normalization_params[col]['max']\n",
        "            denorm_value = normalized_flight_data[i] * (col_max - col_min) + col_min\n",
        "            denormalized_flight_data.append(denorm_value)\n",
        "        return np.array(denormalized_flight_data)\n",
        "\n",
        "\n",
        "    def _generate_sequence_indices(self):\n",
        "        \"\"\"\n",
        "        Generate a list of indices where each sequence starts.\n",
        "        If a NaN is encountered in the sequence_length, the process stops.\n",
        "        \"\"\"\n",
        "        sequence_indices = []\n",
        "        idx = 0\n",
        "\n",
        "        while idx < len(self.dataframe):\n",
        "            sequence_length = self.dataframe.iloc[idx]['sequence_length']\n",
        "\n",
        "            if pd.isna(sequence_length):\n",
        "                # Stop if sequence_length is NaN (no more sequences)\n",
        "                break\n",
        "\n",
        "            # Convert sequence_length to an integer\n",
        "            sequence_length = int(sequence_length)\n",
        "            sequence_indices.append(idx)\n",
        "            idx += sequence_length  # Move to the start of the next sequence\n",
        "\n",
        "        return sequence_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequence_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the starting index of the sequence\n",
        "        start_idx = self.sequence_indices[idx]\n",
        "\n",
        "        # Get the sequence length for this specific starting point\n",
        "        sequence_length = int(self.dataframe.iloc[start_idx]['sequence_length'])\n",
        "\n",
        "        # Fetch the image sequence from Azure Blob\n",
        "        image_sequence = []\n",
        "\n",
        "        # Distortion coefficients (D) - estimated\n",
        "        D = np.array([-0.34, 0.12, -0.01, 0.0], dtype=np.float64)\n",
        "\n",
        "        for i in range(sequence_length):\n",
        "            image_path = self.dataframe.iloc[start_idx + i]['image_path']\n",
        "\n",
        "            # Load image using OpenCV\n",
        "            img_rgb = load_image_from_blob_cv(image_path)\n",
        "\n",
        "            # Get the height and width of the loaded image\n",
        "            h, w = img_rgb.shape[:2]\n",
        "\n",
        "            # Dynamically adjust the camera matrix (K) based on the image dimensions\n",
        "            K = np.array([[w, 0, w / 2],\n",
        "                          [0, h, h / 2],\n",
        "                          [0, 0, 1]], dtype=np.float64)\n",
        "\n",
        "            # Undistort the fisheye image (if needed, uncomment this line)\n",
        "            # img_rgb = undistort_fisheye_image(img_rgb, K, D, balance=0.5)\n",
        "\n",
        "            # Apply the color augmentation instead of grayscale\n",
        "            #img_rgb_converted = augment_color_image_clahe_only(img_rgb)  # Using augment_color_image function\n",
        "            img_rgb_converted = augment_color_image_clahe_only(img_rgb, clip_limit=3.0, tile_grid_size=(8, 8))\n",
        "\n",
        "            # Apply additional augmentations to the RGB image if any\n",
        "            if self.augmentations:\n",
        "                img_rgb_converted = self.augmentations(img_rgb_converted)\n",
        "\n",
        "            # Convert back to tensor and append to the sequence\n",
        "            img_tensor = TF.to_tensor(img_rgb_converted)\n",
        "            image_sequence.append(img_tensor)\n",
        "\n",
        "        # Apply random cropping and resizing only if `apply_crop_and_scale` is True\n",
        "        if self.apply_crop_and_scale:\n",
        "            # Apply random cropping to the entire sequence with varying crop sizes (minimum of self.resize_size, up to full image size)\n",
        "            cropped_sequence, new_center_coords = random_crop_sequence(image_sequence, crop_size_minimum=self.resize_size)\n",
        "\n",
        "            # Resize the cropped sequence and adjust the center coordinates accordingly\n",
        "            resized_sequence, resized_center_coords = resize_sequence_and_adjust_center(cropped_sequence, new_center_coords)\n",
        "\n",
        "            # Stack images into a tensor\n",
        "            image_sequence = torch.stack(resized_sequence)\n",
        "        else:\n",
        "            # Resize original images (whatever their original size) to 224x224\n",
        "            resized_sequence = [TF.resize(img, size=(self.resize_size, self.resize_size)) for img in image_sequence]\n",
        "\n",
        "            # Stack the resized images\n",
        "            image_sequence = torch.stack(resized_sequence)\n",
        "\n",
        "            # Since no cropping is applied, calculate the center of the resized 224x224 image\n",
        "            resized_center_coords = (self.resize_size // 2, self.resize_size // 2)  # Center of the resized square image\n",
        "\n",
        "        # Fetch the additional flight data from the dataframe\n",
        "        flight_data = self.dataframe.iloc[start_idx:start_idx + sequence_length][['GPS_MSL_Alt', 'Drift', 'Pitch', 'Roll', 'Vert_Velocity']].values\n",
        "\n",
        "        # Fetch the validation height (target), just for printing\n",
        "        validation_height = self.dataframe.iloc[start_idx + sequence_length - 1]['validation_height']\n",
        "\n",
        "        # Print the new center coordinates for debugging\n",
        "        # print(f\"New center coordinates in resized image: {resized_center_coords}\")\n",
        "\n",
        "        return image_sequence, flight_data, validation_height, resized_center_coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o6CBp7ZK9nV"
      },
      "outputs": [],
      "source": [
        "# Create the full Cloud2CloudDataset (no normalization yet)\n",
        "full_cloud2cloud_dataset = Cloud2CloudDataset(dataframe=full_dataframe, transform=transform, augmentations=augmentations, apply_normalization=False, resize_size=512)\n",
        "\n",
        "# Split sequence indices into training and validation sets\n",
        "train_sequence_indices, val_sequence_indices = train_test_split(full_cloud2cloud_dataset.sequence_indices, test_size=0.2, random_state=42)\n",
        "\n",
        "# Extract rows for training dataset based on sequence indices\n",
        "train_rows = []\n",
        "for seq_start in train_sequence_indices:\n",
        "    sequence_length = full_cloud2cloud_dataset.dataframe.iloc[seq_start]['sequence_length']\n",
        "    train_rows.extend(range(seq_start, seq_start + sequence_length))\n",
        "\n",
        "train_dataframe = full_dataframe.iloc[train_rows].reset_index(drop=True)\n",
        "\n",
        "# Create the training dataset and calculate normalization parameters\n",
        "train_cloud2cloud_dataset = Cloud2CloudDataset(dataframe=train_dataframe, transform=transform, augmentations=augmentations, apply_normalization=True, apply_crop_and_scale=True, resize_size=512)\n",
        "\n",
        "# Get normalization parameters from the training dataset\n",
        "normalization_params = train_cloud2cloud_dataset.normalization_params\n",
        "\n",
        "# Extract rows for validation dataset based on sequence indices\n",
        "val_rows = []\n",
        "for seq_start in val_sequence_indices:\n",
        "    sequence_length = full_cloud2cloud_dataset.dataframe.iloc[seq_start]['sequence_length']\n",
        "    val_rows.extend(range(seq_start, seq_start + sequence_length))\n",
        "\n",
        "val_dataframe = full_dataframe.iloc[val_rows].reset_index(drop=True)\n",
        "\n",
        "# Create the validation dataset using the same normalization parameters\n",
        "val_cloud2cloud_dataset = Cloud2CloudDataset(dataframe=val_dataframe, normalization_params=normalization_params, transform=transform, augmentations=augmentations, apply_normalization=True, apply_crop_and_scale=False, resize_size=512)\n",
        "\n",
        "# Create DataLoaders for training and validation sets\n",
        "train_dataloader_single_batch = DataLoader(train_cloud2cloud_dataset, batch_size=1, shuffle=True)\n",
        "val_dataloader_single_batch = DataLoader(val_cloud2cloud_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TA5Ev5KhKWd"
      },
      "outputs": [],
      "source": [
        "# Function to display a grid of images with a red dot at the revised center coordinates only on the last image\n",
        "def show_image_grid_with_center(images, center_coords, titles=None):\n",
        "    # Number of images in the sequence\n",
        "    num_images = images.shape[0]\n",
        "\n",
        "    # Set up the grid (1 row, num_images columns)\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 15))\n",
        "\n",
        "    if num_images == 1:\n",
        "        axes = [axes]  # Ensure axes is iterable even with one image\n",
        "\n",
        "    for i, ax in enumerate(axes):\n",
        "        img = images[i].permute(1, 2, 0).numpy()  # Convert tensor to HWC format\n",
        "        img = np.clip(img, 0, 1)  # Ensure values are between 0 and 1 after normalization\n",
        "        ax.imshow(img)\n",
        "\n",
        "        # Overlay red dot only on the last image in the sequence\n",
        "        if i == num_images - 1:\n",
        "            center_x, center_y = center_coords\n",
        "            ax.plot(center_x, center_y, 'ro')  # Red dot (marker 'ro' for red circle)\n",
        "\n",
        "        ax.axis('off')\n",
        "\n",
        "        if titles:\n",
        "            ax.set_title(titles[i])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Iterate over the first 5 sequences for display with red dot on center\n",
        "for i, (images, flight_data, validation_height, resized_center_coords) in enumerate(train_dataloader_single_batch):\n",
        "    if i >= 5:  # Limiting to 5 sequences\n",
        "        break\n",
        "\n",
        "    # Display the image sequence\n",
        "    print(f\"Sequence {i + 1}:\")\n",
        "\n",
        "    # Display normalized flight data\n",
        "    print(\"Normalized Flight Data:\")\n",
        "    flight_data_np = flight_data.squeeze(0).numpy()  # Convert tensor to NumPy for readability\n",
        "    columns = ['GPS_MSL_Alt', 'Drift', 'Pitch', 'Roll', 'Vert_Velocity']\n",
        "\n",
        "    for step, data in enumerate(flight_data_np):\n",
        "        flight_info = dict(zip(columns, data))\n",
        "        print(f\"  Step {step + 1}: {flight_info}\")\n",
        "\n",
        "    # Denormalize and display flight data\n",
        "    print(\"Denormalized Flight Data:\")\n",
        "    for step, data in enumerate(flight_data_np):\n",
        "        denormalized_data = train_cloud2cloud_dataset.denormalize_flight_data(data)\n",
        "        flight_info_denorm = dict(zip(columns, denormalized_data))\n",
        "        print(f\"  Step {step + 1} (denormalized): {flight_info_denorm}\")\n",
        "\n",
        "    # Display normalized validation height\n",
        "    print(\"Normalized Validation Height:\", validation_height.item())\n",
        "\n",
        "    # Denormalize and display validation height\n",
        "    validation_height_denorm = train_cloud2cloud_dataset.denormalize_validation_height(validation_height.item())\n",
        "    print(f\"Denormalized Validation Height: {validation_height_denorm:.2f} meters\")\n",
        "\n",
        "    # Unpack resized center coordinates (no need to squeeze since it's a list)\n",
        "    resized_center_x, resized_center_y = resized_center_coords\n",
        "    print(f\"Resized Center Coordinates: (x: {resized_center_x}, y: {resized_center_y})\")\n",
        "\n",
        "    # Display images in a grid with the red dot at the resized center coordinates\n",
        "    show_image_grid_with_center(images.squeeze(0), (resized_center_x, resized_center_y))  # Remove batch dimension (since batch size is 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zqfnQnoFf2S"
      },
      "source": [
        "# Height Field Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4n6u6ZoFMr_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download MiDaS model using torch.hub\n",
        "def load_midas_model(device):\n",
        "    midas_model = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Large\")\n",
        "    midas_model.to(device)\n",
        "    midas_model.eval()\n",
        "    return midas_model\n",
        "\n",
        "# Function to generate a relative height map with the center pixel set to zero\n",
        "def generate_relative_height_map(midas_model, image_tensor, device):\n",
        "    \"\"\"\n",
        "    Generates a relative height map using MiDaS, setting the center pixel to zero.\n",
        "\n",
        "    Args:\n",
        "        midas_model (nn.Module): The MiDaS model for depth estimation.\n",
        "        image_tensor (torch.Tensor): A tensor of shape (channels, height, width) representing the final image.\n",
        "\n",
        "    Returns:\n",
        "        relative_height_map (np.array): A 2D NumPy array of shape (height, width) with the center pixel at zero.\n",
        "    \"\"\"\n",
        "    # Print the original image size before any processing\n",
        "    print(\"Original image size:\", image_tensor.shape)\n",
        "\n",
        "    # Assert statement to check if image is 512x512\n",
        "    assert image_tensor.shape[1:] == (512, 512), \"Image must be 512x512 for the large MiDaS model.\"\n",
        "\n",
        "    # Ensure the input tensor has batch and channel dimensions\n",
        "    if image_tensor.dim() == 3:\n",
        "        image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Pass image through MiDaS directly without resizing\n",
        "    input_image = image_tensor.to(device)\n",
        "    with torch.no_grad():\n",
        "        depth_map = midas_model(input_image)\n",
        "\n",
        "    # Remove batch and channel dimensions to get a 2D depth map\n",
        "    depth_map_2d = depth_map.squeeze().cpu().numpy()\n",
        "\n",
        "    # Set center pixel to zero for relative height\n",
        "    center_y, center_x = depth_map_2d.shape[0] // 2, depth_map_2d.shape[1] // 2\n",
        "    center_value = depth_map_2d[center_y, center_x]\n",
        "    relative_height_map = depth_map_2d - center_value\n",
        "\n",
        "    return relative_height_map\n",
        "\n",
        "# Helper function to display 3D topographical plot of height field using Plotly\n",
        "def plotly_3d_height_field(height_field, title=\"Predicted Relative Height Field\"):\n",
        "    # Ensure height_field is 2D by selecting the first element if it has a batch dimension\n",
        "    if height_field.ndim == 3:\n",
        "        height_field = height_field[0]\n",
        "\n",
        "    # Create a grid of coordinates (x, y) for the height field\n",
        "    x = np.arange(height_field.shape[1])\n",
        "    y = np.arange(height_field.shape[0])\n",
        "    x, y = np.meshgrid(x, y)\n",
        "\n",
        "    # Create the plotly surface plot\n",
        "    fig = go.Figure(data=[go.Surface(z=height_field, x=x, y=y, colorscale='Viridis')])\n",
        "\n",
        "    # Customize layout\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        scene=dict(\n",
        "            xaxis_title='X',\n",
        "            yaxis_title='Y',\n",
        "            zaxis_title='Relative Height'\n",
        "        ),\n",
        "        margin=dict(l=0, r=0, b=0, t=30)\n",
        "    )\n",
        "\n",
        "    # Display the plot\n",
        "    fig.show()\n",
        "\n",
        "# Function to display 5 random samples with their 3D height fields\n",
        "def display_5_random_samples_with_3d_plotly(dataset, midas_model, device):\n",
        "    indices = np.random.choice(len(dataset), 5, replace=False)\n",
        "    for idx in indices:\n",
        "        image_sequence, _, _, _ = dataset[idx]\n",
        "        image_sequence = image_sequence.to(device)\n",
        "\n",
        "        # Use the last image in the sequence for MiDaS\n",
        "        last_image = image_sequence[-1]\n",
        "\n",
        "        # Generate the relative height field using MiDaS\n",
        "        relative_height_field = generate_relative_height_map(midas_model, last_image, device)\n",
        "        relative_height_field_np = relative_height_field\n",
        "\n",
        "        # Print the height field values for debugging\n",
        "        print(f\"Height field values for sample {idx + 1}:\\n\", relative_height_field_np)\n",
        "\n",
        "        # Plot the image sequence with the center dot\n",
        "        show_image_grid_with_center(image_sequence.cpu(), center_coords=(112, 112))\n",
        "\n",
        "        # Plot the relative height field in 3D using Plotly\n",
        "        plotly_3d_height_field(relative_height_field_np, title=f\"Sample {idx + 1}: Relative Height Field\")\n",
        "\n",
        "# Load MiDaS model and set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "midas_model = load_midas_model(device)\n",
        "\n",
        "# Call the function\n",
        "display_5_random_samples_with_3d_plotly(val_cloud2cloud_dataset, midas_model, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}