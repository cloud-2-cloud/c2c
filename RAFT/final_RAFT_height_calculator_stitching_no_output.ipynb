{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cloud Height Measurement System: Technical Overview\n",
        "\n",
        "## System Architecture\n",
        "\n",
        "This system combines aerial imagery with LiDAR measurements to create detailed cloud height fields using a deep learning approach. The process consists of four main stages:\n",
        "\n",
        "1. Data Collection and Preprocessing\n",
        "   - HD camera images from FEGS (Fly's Eye GLM Simulator)\n",
        "   - LiDAR measurements from Cloud Physics LiDAR (CPL)\n",
        "   - Aircraft flight metadata (altitude, speed, orientation)\n",
        "   - Image synchronization and alignment\n",
        "\n",
        "2. Motion Analysis and Height Estimation\n",
        "   - Fine-tuned RAFT (Recurrent All-Pairs Field Transforms) model for optical flow\n",
        "   - Parallax-based height calculation using motion fields\n",
        "   - Integration with aircraft metadata for scale calibration\n",
        "   - Confidence mapping for measurement reliability\n",
        "\n",
        "3. Height Field Generation and Validation\n",
        "   - Height field calculation from motion vectors\n",
        "   - LiDAR measurement integration for calibration\n",
        "   - Uncertainty estimation and quality control\n",
        "   - Individual sequence height field generation\n",
        "\n",
        "4. Height Field Stitching and Global Integration\n",
        "   - Global coordinate system creation using GPS data\n",
        "   - Weighted height field merging\n",
        "   - Multi-sequence confidence integration\n",
        "   - Large-scale cloud topology reconstruction\n",
        "\n",
        "## Key Processing Steps\n",
        "\n",
        "### 1. Data Input Preparation\n",
        "- Temporal alignment of image sequences (5-second intervals)\n",
        "- Image preprocessing and augmentation\n",
        "- Metadata normalization and feature engineering\n",
        "- Sequence packaging for batch processing\n",
        "\n",
        "### 2. Motion Field Generation\n",
        "- RAFT model application to image pairs\n",
        "- Bidirectional flow calculation\n",
        "- Motion field refinement and consistency checking\n",
        "- Confidence map generation\n",
        "\n",
        "### 3. Height Field Calculation\n",
        "- Parallax principle application\n",
        "- Aircraft motion compensation\n",
        "- Scale factor calibration using LiDAR\n",
        "- Height field uncertainty estimation\n",
        "\n",
        "### 4. Global Field Stitching\n",
        "- GPS-based sequence positioning\n",
        "- Distance-weighted height field merging\n",
        "- Confidence-based blending\n",
        "- Banding artifact removal\n",
        "- Seamless transition handling\n",
        "\n",
        "## System Advantages\n",
        "\n",
        "1. Wide Field Coverage\n",
        "   - 90-degree field of view compared to single-point LiDAR\n",
        "   - Continuous spatial coverage\n",
        "   - Higher spatial resolution\n",
        "   - Large-scale cloud field reconstruction through stitching\n",
        "\n",
        "2. Cost Efficiency\n",
        "   - Uses existing camera hardware\n",
        "   - Reduces dependency on expensive LiDAR systems\n",
        "   - Simplified instrument package\n",
        "   - Maximizes value from existing flight data\n",
        "\n",
        "3. Measurement Quality\n",
        "   - LiDAR-calibrated accuracy\n",
        "   - Uncertainty quantification\n",
        "   - Real-time quality assessment\n",
        "   - Multi-sequence validation\n",
        "   - Robust to measurement gaps\n",
        "\n",
        "4. Comprehensive Cloud Mapping\n",
        "   - Seamless integration of multiple sequences\n",
        "   - Preservation of local detail\n",
        "   - Global context maintenance\n",
        "   - Confidence-weighted merging\n",
        "   - Artifact-free reconstruction\n",
        "\n",
        "This system bridges the gap between limited single-point LiDAR measurements and the need for cloud height fields, enabling more accurate atmospheric observations and improved weather predictions. The stitching capability allows for reconstruction of large-scale cloud fields, providing coverage and detail in cloud height mapping from aerial platforms.\n"
      ],
      "metadata": {
        "id": "bou3YZf7_DsL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2DMUtlaHhrb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkDr5ozfMdva"
      },
      "outputs": [],
      "source": [
        "!pip install azure-storage-blob azure-identity --quiet\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oBDGmN7hzMj"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mWdmgSjNIUv"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import cv2\n",
        "import io\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import os\n",
        "import random\n",
        "import seaborn as sns\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from io import BytesIO\n",
        "from IPython.display import clear_output, display, HTML\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from google.colab import userdata\n",
        "from google.colab import runtime\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from matplotlib import cm, gridspec, animation\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from plotly.subplots import make_subplots\n",
        "from scipy.ndimage import gaussian_filter, gaussian_filter1d\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from timm import create_model\n",
        "from torchvision import transforms\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR, OneCycleLR, ReduceLROnPlateau\n",
        "from torch.serialization import add_safe_globals\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import PackedSequence, pack_sequence, pad_packed_sequence, pack_padded_sequence\n",
        "from torchvision.models.optical_flow import raft_large\n",
        "from torchvision.models.optical_flow import Raft_Large_Weights\n",
        "from tqdm.notebook import tqdm_notebook as tqdm\n",
        "from transformers import AutoImageProcessor, ConvNextModel, ConvNextV2Model, TimesformerModel, TimesformerConfig, SwinModel\n",
        "from typing import Tuple, Dict, Optional\n",
        "\n",
        "def show_warnings(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        with warnings.catch_warnings(record=True) as caught_warnings:\n",
        "            result = func(*args, **kwargs)\n",
        "            for warning in caught_warnings:\n",
        "                print(f\"Warning: {warning.message}\")\n",
        "                print(f\"  In file: {warning.filename}\")\n",
        "                print(f\"  Line number: {warning.lineno}\")\n",
        "                print(\"--------------------\")\n",
        "        return result\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook parameters"
      ],
      "metadata": {
        "id": "cirOvefz2S_P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5heH7jycP9WO"
      },
      "outputs": [],
      "source": [
        "# Memory\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Number of epochs for training\n",
        "num_epochs = 2\n",
        "\n",
        "# If true, train model from scratch. If false, load a previously trained model\n",
        "train_model = True\n",
        "\n",
        "# If true, use a previously generated dataset from the image files. If false, regenerate it.\n",
        "# Note that regenerating the dataset takes a bit of time.\n",
        "cache_cloud_dataset = False\n",
        "\n",
        "# Available flight data dates. Modify the train_dates variable below as needed\n",
        "# \"20170418\", \"20170422\", \"20170508\", \"20170512”, \"20170512\"\n",
        "train_dates = [\"20170418\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opTV-xLMzfIw"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DebugControl Class"
      ],
      "metadata": {
        "id": "9DGUigoFAs2V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWZp1qr5mHZ0"
      },
      "outputs": [],
      "source": [
        "class DebugControl:\n",
        "    \"\"\"\n",
        "    Controls debug output for different components of the cloud height measurement system.\n",
        "\n",
        "    This class provides granular control over debug messages for different system components,\n",
        "    allowing selective enabling/disabling of debug output for specific processing stages.\n",
        "\n",
        "    Attributes:\n",
        "        enabled (bool): Master switch for all debug output.\n",
        "        components (dict): Dictionary of component-specific debug flags:\n",
        "            - training: Training loop messages\n",
        "            - optical_flow: Optical flow computation messages\n",
        "            - loss: Loss computation messages\n",
        "            - model: Model forward pass messages\n",
        "            - shapes: Tensor shape debug messages\n",
        "            - memory: Memory usage debug messages\n",
        "            - heights: Height calculation debug messages\n",
        "            - motion: Motion field debug messages\n",
        "            - bias: Bias calculation debug messages\n",
        "            - confidence: Confidence calculation messages\n",
        "            - finetune: Fine-tuning process messages\n",
        "\n",
        "    Methods:\n",
        "        enable_all(): Enables debug output for all components.\n",
        "        disable_all(): Disables debug output for all components.\n",
        "        enable_only(*components): Enables debug output only for specified components.\n",
        "        debug_print(component, *args, **kwargs): Prints debug message if component is enabled.\n",
        "\n",
        "    Example:\n",
        "        >>> debug = DebugControl()\n",
        "        >>> debug.enable_only('memory', 'shapes')\n",
        "        >>> debug.debug_print('memory', 'Current GPU memory usage: 2GB')\n",
        "        Current GPU memory usage: 2GB\n",
        "        >>> debug.debug_print('loss', 'Loss: 0.5')  # Won't print (component disabled)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.enabled = False\n",
        "        self.components = {\n",
        "            'training': False,      # Training loop messages\n",
        "            'optical_flow': False,  # Optical flow computation messages\n",
        "            'loss': False,         # Loss computation messages\n",
        "            'model': False,        # Model forward pass messages\n",
        "            'shapes': False,        # Tensor shape debug messages\n",
        "            'memory': False,        # Memory usage debug messages\n",
        "            'heights': False,       # height debug messages\n",
        "            'motion': False,        # motion debug messages\n",
        "            'bias': False,          # bias debug messages\n",
        "            'confidence': False,     # confidence debug messages\n",
        "            'finetune': False       # finetune debug messages\n",
        "        }\n",
        "\n",
        "    def enable_all(self):\n",
        "        self.enabled = True\n",
        "        for key in self.components:\n",
        "            self.components[key] = True\n",
        "\n",
        "    def disable_all(self):\n",
        "        self.enabled = False\n",
        "        for key in self.components:\n",
        "            self.components[key] = False\n",
        "\n",
        "    def enable_only(self, *components):\n",
        "        self.enabled = True\n",
        "        for key in self.components:\n",
        "            self.components[key] = key in components\n",
        "\n",
        "    def debug_print(self, component, *args, **kwargs):\n",
        "        if self.enabled and self.components.get(component, False):\n",
        "            print(*args, **kwargs)\n",
        "\n",
        "# Create global debug controller\n",
        "debug = DebugControl()\n",
        "\n",
        "# Disable all debug output\n",
        "debug.disable_all()\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# Enable all debug output\n",
        "# debug.enable_all()\n",
        "\n",
        "# Enable only specific components\n",
        "# debug.enable_only('memory', 'shapes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5mKVG9m9tu3"
      },
      "outputs": [],
      "source": [
        "def embed_matplotlib_jpeg(fig, quality=50, dpi=100, width=\"100%\"):\n",
        "    \"\"\"\n",
        "    Save a Matplotlib figure as a compressed JPEG and embed it in the notebook.\n",
        "    Since this notebook is heavily image-based, this is to reduce the overall size.\n",
        "    Parameters:\n",
        "        fig (matplotlib.figure.Figure): The Matplotlib figure to embed.\n",
        "        quality (int): Compression quality for the JPEG (1-100, higher is better quality).\n",
        "        dpi (int): Dots per inch (resolution) for the figure.\n",
        "        width (str): Width of the image in the notebook (e.g., \"100%\", \"600px\").\n",
        "    Returns:\n",
        "        HTML object: Embedded image in HTML format.\n",
        "    \"\"\"\n",
        "    # Save the figure to a BytesIO buffer as a raw PNG\n",
        "    buffer = io.BytesIO()\n",
        "    fig.savefig(buffer, format='png', dpi=dpi, bbox_inches='tight')\n",
        "    buffer.seek(0)\n",
        "\n",
        "    # Open the PNG image with Pillow\n",
        "    image = Image.open(buffer)\n",
        "\n",
        "    # Convert the image to RGB if it has an alpha channel (RGBA)\n",
        "    if image.mode == 'RGBA':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    # Save the image as a compressed JPEG in a new buffer\n",
        "    jpeg_buffer = io.BytesIO()\n",
        "    image.save(jpeg_buffer, format='JPEG', quality=quality)\n",
        "    jpeg_buffer.seek(0)\n",
        "\n",
        "    # Encode the compressed JPEG as base64\n",
        "    encoded = base64.b64encode(jpeg_buffer.read()).decode('utf-8')\n",
        "    jpeg_buffer.close()\n",
        "\n",
        "    # Return an HTML <img> tag with a specified width\n",
        "    return HTML(f'<img src=\"data:image/jpeg;base64,{encoded}\" style=\"width:{width};\" />')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAFT Architecture and Memory Usage\n",
        "\n",
        "In this section we create a number of helper functions to monitor and report memory usage.\n",
        "\n",
        "RAFT (Recurrent All-Pairs Field Transforms) is an optical flow model that works by building dense correlations between all pixels in a pair of images and iteratively refining flow predictions. It uses a surprising a significant amount of memory.\n",
        "\n",
        "## Core Components\n",
        "\n",
        "## Feature Extraction\n",
        "The first step in RAFT is feature extraction, where each input image is processed through a convolutional encoder network. This encoder transforms each pixel into a rich feature representation containing hundreds of channels of information. Rather than producing a single feature map, RAFT creates a feature pyramid with multiple resolution levels. This pyramid structure allows the model to capture both fine details and broader image context, but it means storing multiple feature maps of varying sizes for each image.\n",
        "\n",
        "## All-Pairs Correlation\n",
        "The correlation computation is where RAFT's memory usage truly explodes. For every single pixel in the first image, RAFT computes correlation scores with every possible pixel in the second image. This creates an enormous 4D correlation volume of size H×W×H×W, where H and W are the image height and width. To put this in perspective, for our 384×384 pixel images, the correlation volume alone requires 384⁴ elements. Even using 4-byte floats, that's approximately 55GB of memory needed just to store a single pair of images' correlation volume! This quadratic scaling with image size is the primary reason for RAFT's intense memory requirements.\n",
        "\n",
        "## Iterative Refinement\n",
        "The final major component is RAFT's iterative refinement mechanism, which uses a Gated Recurrent Unit (GRU) to progressively update and refine flow estimates. As the GRU processes each pixel's motion, it looks up relevant correlation features based on the current estimated position. This process typically runs for 12-32 iterations, maintaining hidden states throughout. While not as memory-intensive as the correlation volume, these iterations require storing both the hidden states and intermediate results for backpropagation during training.\n",
        "\n",
        "## Why Sequences Are Memory-Intensive\n",
        "\n",
        "When processing sequences, memory usage multiplies because:\n",
        "- Need correlation volumes for each consecutive pair of frames\n",
        "- Intermediate activations are stored for backpropagation\n",
        "- GRU hidden states maintained across sequence\n",
        "- Each additional frame essentially adds another full RAFT computation\n",
        "\n",
        "For a sequence of length T:\n",
        "```\n",
        "Memory ∝ (T-1) × (H²W²)\n",
        "```\n",
        "\n",
        "The quadratic scaling with image size (H²W²) makes this particularly challenging. Even with optimizations like limiting correlation range or using lower resolutions, the memory requirements grow rapidly with sequence length.\n",
        "\n",
        "This is why RAFT, despite having a relatively modest number of parameters (~5-6M), can easily consume gigabytes of GPU memory during training, especially with longer sequences. Each additional frame in the sequence means another massive correlation volume must be computed and stored for backpropagation."
      ],
      "metadata": {
        "id": "3G2dkQ7nAzBK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJyHDKnq2Hvu"
      },
      "outputs": [],
      "source": [
        "def print_gpu_memory(location=\"\"):\n",
        "    \"\"\"Print GPU memory usage with location tag\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        debug.debug_print('memory', f\"\\nMemory at {location}:\")\n",
        "        debug.debug_print('memory', f\"Allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "        debug.debug_print('memory', f\"Cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\\n\")\n",
        "\n",
        "# Example\n",
        "# >>>print_gpu_memory(\"Start of finetune_raft\")\n",
        "\n",
        "def format_memory_string(bytes_value):\n",
        "    \"\"\"Convert bytes to human readable string\"\"\"\n",
        "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
        "        if bytes_value < 1024:\n",
        "            return f\"{bytes_value:.2f} {unit}\"\n",
        "        bytes_value /= 1024\n",
        "    return f\"{bytes_value:.2f} TB\"\n",
        "\n",
        "def get_detailed_memory_info():\n",
        "    \"\"\"Get detailed GPU memory information\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return \"GPU not available\"\n",
        "\n",
        "    info = []\n",
        "    info.append(f\"Total GPU memory: {format_memory_string(torch.cuda.get_device_properties(0).total_memory)}\")\n",
        "    info.append(f\"Allocated memory: {format_memory_string(torch.cuda.memory_allocated())}\")\n",
        "    info.append(f\"Reserved memory: {format_memory_string(torch.cuda.memory_reserved())}\")\n",
        "    info.append(f\"Max allocated: {format_memory_string(torch.cuda.max_memory_allocated())}\")\n",
        "    return \"\\n\".join(info)\n",
        "\n",
        "def get_model_size(model):\n",
        "    \"\"\"Calculate total parameters and their memory footprint\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
        "\n",
        "    return {\n",
        "        'total_params': total_params,\n",
        "        'trainable_params': total_trainable_params,\n",
        "        'param_memory': format_memory_string(param_size),\n",
        "        'buffer_memory': format_memory_string(buffer_size),\n",
        "        'total_memory': format_memory_string(param_size + buffer_size)\n",
        "    }\n",
        "\n",
        "def print_oom_report(batch_idx, images, sequence_lengths, model):\n",
        "    \"\"\"Print detailed OOM error report\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"OUT OF MEMORY ERROR - Batch {batch_idx}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"\\nBATCH INFORMATION:\")\n",
        "    print(f\"Batch size: {images.shape[0]}\")\n",
        "    print(f\"Max sequence length: {images.shape[1]}\")\n",
        "    print(f\"Image dimensions: {images.shape[2:]} (C,H,W)\")\n",
        "    print(f\"Actual sequence lengths: {sequence_lengths.tolist()}\")\n",
        "    print(f\"Total elements in batch: {images.numel():,}\")\n",
        "    print(f\"Batch memory: {format_memory_string(images.numel() * images.element_size())}\")\n",
        "\n",
        "    print(\"\\nMEMORY STATE:\")\n",
        "    print(get_detailed_memory_info())\n",
        "\n",
        "    print(\"\\nMODEL INFORMATION:\")\n",
        "    model_info = get_model_size(model)\n",
        "    print(f\"Total parameters: {model_info['total_params']:,}\")\n",
        "    print(f\"Trainable parameters: {model_info['trainable_params']:,}\")\n",
        "    print(f\"Parameter memory: {model_info['param_memory']}\")\n",
        "    print(f\"Buffer memory: {model_info['buffer_memory']}\")\n",
        "    print(f\"Total model memory: {model_info['total_memory']}\")\n",
        "\n",
        "    print(\"\\nRECOMMENDATIONS:\")\n",
        "    if images.shape[0] > 1:\n",
        "        print(\"- Consider reducing batch size\")\n",
        "    if max(sequence_lengths) > 5:\n",
        "        print(\"- Consider truncating sequence lengths\")\n",
        "    print(\"- Try enabling gradient checkpointing\")\n",
        "    print(\"- Consider using mixed precision training\")\n",
        "    print(\"=\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67WosKEWQQ7U"
      },
      "outputs": [],
      "source": [
        "# Initialize GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbeAsW0wOfxk"
      },
      "outputs": [],
      "source": [
        "# Authentication details\n",
        "account_name = userdata.get('storage_account_name')\n",
        "account_key = userdata.get('storage_account_key')\n",
        "container_name = userdata.get('blob_container_name')\n",
        "\n",
        "# Connection string to Azure Blob Storage\n",
        "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
        "\n",
        "# Setup to load file from blob\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "container_client = blob_service_client.get_container_client(container_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Available Aircraft Metadata\n",
        "\n",
        "## Temporal and Position Data\n",
        "- **DateTime_UTC**: ISO-8601 formatted date and time in UTC (yyyy-mm-ddThh:mm:ss)\n",
        "- **Lat**: Platform Latitude (degrees N, -90 to 90)\n",
        "- **Lon**: Platform Longitude (degrees E, -180 to 179.9999)\n",
        "\n",
        "## Altitude Measurements\n",
        "- **GPS_MSL_Alt**: GPS Altitude above Mean Sea Level (meters)\n",
        "- **WGS_84_Alt**: WGS 84 Geoid Altitude (meters)\n",
        "- **Press_Alt**: Pressure Altitude (feet)\n",
        "\n",
        "## Velocity Parameters\n",
        "- **Grnd_Spd**: Ground Speed (m/s)\n",
        "- **True_Airspeed**: True Airspeed (m/s)\n",
        "- **Mach_Number**: Aircraft Mach Number (dimensionless)\n",
        "- **Vert_Velocity**: Aircraft Vertical Velocity (m/s, negative=downward, positive=upward)\n",
        "\n",
        "## Aircraft Orientation\n",
        "- **True_Hdg**: True Heading (degrees true, 0 to 359.9999)\n",
        "- **Track**: Track Angle (degrees true, 0 to 359.9999)\n",
        "- **Drift**: Drift Angle (degrees)\n",
        "- **Pitch**: Pitch (degrees, -90 to 90, negative=nose down, positive=nose up)\n",
        "- **Roll**: Roll (degrees, -90 to 90, negative=left wing down, positive=right wing down)\n",
        "\n",
        "## Environmental Conditions\n",
        "- **Ambient_Temp**: Ambient Temperature (°C)\n",
        "- **Total_Temp**: Total Temperature (°C)\n",
        "- **Static_Press**: Static Pressure (millibars)\n",
        "- **Dynamic_Press**: Dynamic Pressure - total minus static (millibars)\n",
        "- **Cabin_Pressure**: Cabin Pressure/Altitude (millibars)\n",
        "\n",
        "## Wind Data\n",
        "- **Wind_Speed**: Wind Speed (m/s, ≥0)\n",
        "- **Wind_Dir**: Wind Direction (degrees true, 0 to 359.9999)\n",
        "\n",
        "## Solar Position\n",
        "- **Solar_Zenith**: Solar Zenith Angle (degrees)\n",
        "- **Sun_Elev_AC**: Sun Elevation from Aircraft (degrees)\n",
        "- **Sun_Az_Grd**: Sun Azimuth from Ground (degrees true, 0 to 359.9999)\n",
        "- **Sun_Az_AC**: Sun Azimuth from Aircraft (degrees true, 0 to 359.9999)\n",
        "\n"
      ],
      "metadata": {
        "id": "-mRzLLWx17qv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhxE_YnrNIKK"
      },
      "outputs": [],
      "source": [
        "aircraft_metadata_params = [\n",
        "    'DateTime_UTC', 'Lat', 'Lon', 'GPS_MSL_Alt', 'WGS_84_Alt', 'Press_Alt',\n",
        "    'Grnd_Spd', 'True_Airspeed', 'Mach_Number', 'Vert_Velocity', 'True_Hdg',\n",
        "    'Track', 'Drift', 'Pitch', 'Roll', 'Ambient_Temp', 'Total_Temp',\n",
        "    'Static_Press', 'Dynamic_Press', 'Cabin_Pressure', 'Wind_Speed',\n",
        "    'Wind_Dir', 'Solar_Zenith', 'Sun_Elev_AC', 'Sun_Az_Grd', 'Sun_Az_AC'\n",
        "]\n",
        "\n",
        "CTH_col = 'top_height'\n",
        "\n",
        "# Aircraft Metadata\n",
        "def load_metadata(blob_name):\n",
        "    blob_client = container_client.get_blob_client(blob_name)\n",
        "    streamdownloader = blob_client.download_blob()\n",
        "    metadata_df = pd.read_csv(io.BytesIO(streamdownloader.readall()))\n",
        "    return metadata_df\n",
        "\n",
        "# LiDAR Validation Heights\n",
        "def load_validation_heights(blob_name):\n",
        "    blob_client = container_client.get_blob_client(blob_name)\n",
        "    streamdownloader = blob_client.download_blob()\n",
        "    validation_df = pd.read_csv(io.BytesIO(streamdownloader.readall()))\n",
        "    return validation_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc5l4RkHOy-W"
      },
      "source": [
        "# CloudDataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R71xTUXTOZg7"
      },
      "outputs": [],
      "source": [
        "# CloudDataset classes integrating all 3 data sources: FEGS Images, Aircraft Metadata and LiDAR Validation Heights with temporal alignment\n",
        "class CloudDataset(Dataset):\n",
        "    def __init__(self, date_folders, transform=None, drop_after_last_validation=True):\n",
        "        self.date_folders = date_folders\n",
        "        self.transform = transform\n",
        "        self.drop_after_last_validation = drop_after_last_validation\n",
        "        self.data_df = self._prepare_dataframe()\n",
        "\n",
        "    def _prepare_dataframe(self):\n",
        "        \"\"\"\n",
        "        Iterates over the date folders in azure blob storage and loads:\n",
        "          1. .jpg Images from each sub-directory in the folder with '_crop_corrected_aligned' in the name.\n",
        "          2. Aircraft Metadata with 1-1 time alignment with the images.\n",
        "          3. LiDAR Validation Heights, mapped using timestamp, if not available filled with NaN.\n",
        "        Creates a df with following columns:\n",
        "            timestamp, image_path, [...aircraft_metadata_params...], validation_height\n",
        "        \"\"\"\n",
        "        image_paths, timestamps, metadata_rows, validation_heights = [], [], [], []\n",
        "\n",
        "        for folder in self.date_folders:\n",
        "            print(f\"Processing folder: {folder}\")\n",
        "            folder_image_paths, folder_timestamps, folder_metadata_rows, folder_validation_heights = [], [], [], []\n",
        "\n",
        "            blob_list = container_client.list_blobs(name_starts_with=folder)\n",
        "            metadata_path, validation_path = None, None\n",
        "            for blob in blob_list:\n",
        "                # extract image paths of all .jpg images in cropped folders\n",
        "                if blob.name.endswith(\".jpg\") and \"_crop_corrected_aligned\" in blob.name:\n",
        "                    folder_image_paths.append(blob.name)\n",
        "                    folder_timestamps.append(self._extract_timestamp_from_filename(blob.name))\n",
        "                # extract the aircraft metadata file path\n",
        "                if blob.name.startswith(f\"{folder}/IWG1.\") and \"processed\" in blob.name:\n",
        "                    metadata_path = blob.name\n",
        "                # extract the LiDAR validation file path\n",
        "                if blob.name.startswith(f\"{folder}/goesrplt_CPL_layers_\") and blob.name.endswith(\"_processed.txt\"):\n",
        "                    validation_path = blob.name\n",
        "\n",
        "            # load aircraft metadata and LiDAR validation data\n",
        "            if metadata_path:\n",
        "                metadata_df = load_metadata(metadata_path)\n",
        "            if validation_path:\n",
        "                validation_df = load_validation_heights(validation_path)\n",
        "\n",
        "            # prepare LiDAR validation data\n",
        "            validation_df['datetime_combined'] = validation_df['date'] + ' ' + validation_df['timestamp']\n",
        "            validation_df['datetime_combined'] = validation_df['datetime_combined'].str.split('.').str[0]\n",
        "            validation_df['datetime_combined'] = pd.to_datetime(validation_df['datetime_combined'], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "            # prepare aircraft metadata\n",
        "            metadata_df = metadata_df[aircraft_metadata_params]\n",
        "            metadata_df['DateTime_UTC'] = metadata_df['DateTime_UTC'].str.split('.').str[0]\n",
        "            metadata_df = self._extract_time_features(metadata_df)  # Add hour_of_day and day_of_year\n",
        "\n",
        "            metadata_timestamps = pd.to_datetime(metadata_df['DateTime_UTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "            metadata_df = metadata_df.set_index(metadata_timestamps)\n",
        "            aligned_metadata = pd.DataFrame(index=pd.to_datetime(folder_timestamps, format=\"%H:%M:%S\"))\n",
        "            aligned_metadata = aligned_metadata.join(metadata_df, how='left')\n",
        "            aligned_metadata = aligned_metadata[aircraft_metadata_params + ['hour_of_day', 'day_of_year']]\n",
        "\n",
        "            folder_metadata_rows.extend(aligned_metadata.values.tolist())\n",
        "\n",
        "            # extract LiDAR validation height exactly matching the timestamp where available, else NaN\n",
        "            for ts in folder_timestamps:\n",
        "                cth = self._map_timestamp_to_lidar(ts, validation_df)\n",
        "                folder_validation_heights.append(cth)\n",
        "\n",
        "            # Create a folder-level DataFrame\n",
        "            folder_data = {\n",
        "                'timestamp': folder_timestamps,\n",
        "                'image_path': folder_image_paths,\n",
        "                **{param: [row[i] for row in folder_metadata_rows] for i, param in enumerate(aircraft_metadata_params + ['hour_of_day', 'day_of_year'])},\n",
        "                'validation_height': folder_validation_heights\n",
        "            }\n",
        "            folder_df = pd.DataFrame(folder_data)\n",
        "\n",
        "            # Conditionally remove rows after the last valid validation_height in this folder\n",
        "            if self.drop_after_last_validation:\n",
        "                last_valid_index = folder_df['validation_height'].last_valid_index()\n",
        "                if last_valid_index is not None:\n",
        "                    folder_df_cleaned = folder_df.loc[:last_valid_index].copy()  # Use .copy() to ensure independence\n",
        "                else:\n",
        "                    folder_df_cleaned = folder_df.copy()  # In case there are no valid entries\n",
        "            else:\n",
        "                folder_df_cleaned = folder_df  # Keep all rows if dropping is disabled\n",
        "\n",
        "            # Extend to the global lists\n",
        "            image_paths.extend(folder_df_cleaned['image_path'].tolist())\n",
        "            timestamps.extend(folder_df_cleaned['timestamp'].tolist())\n",
        "            metadata_rows.extend(folder_df_cleaned[aircraft_metadata_params + ['hour_of_day', 'day_of_year']].values.tolist())\n",
        "            validation_heights.extend(folder_df_cleaned['validation_height'].tolist())\n",
        "\n",
        "            # Print the lengths for the current folder\n",
        "            print(f\"Folder {folder}:\")\n",
        "            print(f\"  Number of images: {len(folder_df_cleaned['image_path'])}\")\n",
        "            print(f\"  Number of timestamps: {len(folder_df_cleaned['timestamp'])}\")\n",
        "            print(f\"  Number of metadata rows: {len(folder_df_cleaned)}\")\n",
        "            print(f\"  Number of validation heights: {len(folder_df_cleaned['validation_height'])}\")\n",
        "\n",
        "        # Print the final lengths after processing all folders\n",
        "        print(\"After processing all folders combined:\")\n",
        "        print(f\"  Total number of images: {len(image_paths)}\")\n",
        "        print(f\"  Total number of timestamps: {len(timestamps)}\")\n",
        "        print(f\"  Total number of metadata rows: {len(metadata_rows)}\")\n",
        "        print(f\"  Total number of validation heights: {len(validation_heights)}\")\n",
        "\n",
        "        # Check for any mismatches\n",
        "        if not (len(image_paths) == len(timestamps) == len(metadata_rows) == len(validation_heights)):\n",
        "            print(\"Error: Length mismatch detected!\")\n",
        "            print(f\"  Images: {len(image_paths)}\")\n",
        "            print(f\"  Timestamps: {len(timestamps)}\")\n",
        "            print(f\"  Metadata rows: {len(metadata_rows)}\")\n",
        "            print(f\"  Validation heights: {len(validation_heights)}\")\n",
        "            return None\n",
        "\n",
        "        # combine all aligned data in a df\n",
        "        data = {\n",
        "            'timestamp': timestamps,\n",
        "            'image_path': image_paths,\n",
        "            **{param: [row[i] for row in metadata_rows] for i, param in enumerate(aircraft_metadata_params + ['hour_of_day', 'day_of_year'])},\n",
        "            'validation_height': validation_heights\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "        df = df.drop(columns=['DateTime_UTC'])\n",
        "\n",
        "        # Print columns with NaN values before interpolation\n",
        "        self._print_columns_with_nan(df)\n",
        "\n",
        "        # Interpolate missing values, excluding 'validation_height'\n",
        "        df = self._interpolate_missing_values(df)\n",
        "\n",
        "        # Add sequence length information for RNN\n",
        "        self._add_sequence_length_column(df)\n",
        "        return df\n",
        "\n",
        "    def _extract_time_features(self, df):\n",
        "        \"\"\"\n",
        "        Extracts hour of day and day of year from the DateTime_UTC column.\n",
        "        Adds 'hour_of_day' (with fractional hour) and 'day_of_year' as new columns in the DataFrame.\n",
        "        \"\"\"\n",
        "        df['DateTime_UTC'] = pd.to_datetime(df['DateTime_UTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "        df['hour_of_day'] = df['DateTime_UTC'].dt.hour + df['DateTime_UTC'].dt.minute / 60\n",
        "        df['day_of_year'] = df['DateTime_UTC'].dt.dayofyear\n",
        "        return df\n",
        "\n",
        "    def _extract_timestamp_from_filename(self, filename):\n",
        "        \"\"\"\n",
        "        Extracts the timestamp from the image filename on the blob.\n",
        "        path/to/blob/YYYYMMDD_HHMMSS_frame_n_cropped.jpg -> %Y%m%d%H%M%S\n",
        "        \"\"\"\n",
        "        filename = os.path.basename(filename)\n",
        "        date_str = filename.split(\"_\")[0]\n",
        "        time_str = filename.split(\"_\")[1]\n",
        "        timestamp = datetime.strptime(date_str + time_str, \"%Y%m%d%H%M%S\")\n",
        "        return timestamp\n",
        "\n",
        "\n",
        "    def _map_timestamp_to_lidar(self, timestamp, validation_df):\n",
        "        \"\"\"\n",
        "        extract LiDAR validation height exactly matching the timestamp where available, else NaN\n",
        "        \"\"\"\n",
        "        validation_df['datetime_combined'] = pd.to_datetime(validation_df['datetime_combined'], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "        timestamp_dt = pd.to_datetime(timestamp, format=\"%Y-%m-%d %H:%M:%S\")\n",
        "        exact_match = validation_df[validation_df['datetime_combined'] == timestamp_dt]\n",
        "        return exact_match[CTH_col].values[0] if not exact_match.empty else np.nan\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve a data record from the dataset for a given index.\n",
        "        Returns loaded and transformed image, metadata and validation height associated with that image if available.\n",
        "        \"\"\"\n",
        "        row = self.data_df.iloc[idx]\n",
        "\n",
        "        image_path = row['image_path']\n",
        "        blob_client = container_client.get_blob_client(image_path)\n",
        "        streamdownloader = blob_client.download_blob()\n",
        "        img_data = streamdownloader.readall()\n",
        "        img = Image.open(io.BytesIO(img_data)).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        metadata = torch.tensor([row[param] for param in aircraft_metadata_params], dtype=torch.float32)\n",
        "        validation_height = torch.tensor([row['validation_height']], dtype=torch.float32)\n",
        "\n",
        "        return img, metadata, validation_height\n",
        "\n",
        "    def _add_sequence_length_column(self, df):\n",
        "        # Initialize a new column to NaN\n",
        "        df['sequence_length'] = np.nan\n",
        "\n",
        "        # Track the start of each sequence\n",
        "        sequence_start = 0\n",
        "\n",
        "        # Iterate through the dataframe to detect when a validation_height exists\n",
        "        for i in range(len(df)):\n",
        "            if not pd.isna(df.loc[i, 'validation_height']):\n",
        "                # We found the end of a sequence, so mark the previous sequence images\n",
        "                sequence_length = i - sequence_start\n",
        "                df.loc[sequence_start:i, 'sequence_length'] = sequence_length + 1  # Using count (1-based index)\n",
        "                sequence_start = i + 1  # Move the start to the next sequence\n",
        "\n",
        "        # Ensure all sequence_length values are integers (if any were missed)\n",
        "        df['sequence_length'] = df['sequence_length'].astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _print_columns_with_nan(self, df):\n",
        "        \"\"\"\n",
        "        Prints the names of columns in the DataFrame that contain NaN values.\n",
        "        \"\"\"\n",
        "        columns_with_nan = df.columns[df.isna().any()].tolist()\n",
        "        if columns_with_nan:\n",
        "            print(\"Columns with NaN values:\")\n",
        "            for col in columns_with_nan:\n",
        "                print(col)\n",
        "        else:\n",
        "            print(\"No columns with NaN values.\")\n",
        "\n",
        "    def _interpolate_missing_values(self, df, exclude_columns=['validation_height', 'timestamp', 'image_path']):\n",
        "        \"\"\"\n",
        "        Interpolates missing values in the DataFrame for all columns except specified ones.\n",
        "        \"\"\"\n",
        "        # Create a copy to avoid modifying the original DataFrame\n",
        "        df = df.copy()\n",
        "\n",
        "        # Store excluded columns\n",
        "        excluded_data = {col: df[col].copy() for col in exclude_columns if col in df.columns}\n",
        "\n",
        "        # Convert all object-type columns in df to numeric where possible\n",
        "        df = df.infer_objects()\n",
        "\n",
        "        # Get columns for interpolation (excluding specified columns)\n",
        "        columns_to_interpolate = [col for col in df.columns if col not in exclude_columns]\n",
        "\n",
        "        # Convert columns to numeric where possible\n",
        "        for col in columns_to_interpolate:\n",
        "            try:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            except (ValueError, TypeError):\n",
        "                continue\n",
        "\n",
        "        # Perform interpolation only on numeric columns\n",
        "        numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "        if not numeric_columns.empty:\n",
        "            df[numeric_columns] = df[numeric_columns].interpolate(\n",
        "                method='linear',\n",
        "                axis=0,\n",
        "                limit_direction='both'\n",
        "            )\n",
        "\n",
        "        # Restore excluded columns\n",
        "        for col, data in excluded_data.items():\n",
        "            df[col] = data\n",
        "\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7L4LeWvWfCX"
      },
      "source": [
        "# Create the CloudDataset or retrieve it from cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbA91uVtO-YY"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "augmentations = transforms.Compose([\n",
        "])\n",
        "\n",
        "# Change the cache_cloud_dataset variable at the start to make this either\n",
        "# generate the CloudDataset (takes some time) or use a cached version on Google Drive\n",
        "if cache_cloud_dataset:\n",
        "    full_dataframe = pd.read_csv('/content/drive/My Drive/datasets/train_dataset.csv')\n",
        "else:\n",
        "    # Gather Data Dynamically from Azure\n",
        "    # Create the full CloudDataset without splitting initially\n",
        "    full_dataset = CloudDataset(train_dates, transform=None)\n",
        "    # Extract the full dataframe from CloudDataset\n",
        "    full_dataframe = full_dataset.data_df\n",
        "    # Ensure folder exists\n",
        "    os.makedirs('/content/drive/My Drive/datasets', exist_ok=True)\n",
        "    # Save the dataframe directly to Google Drive\n",
        "    full_dataframe.to_csv('/content/drive/My Drive/datasets/train_dataset.csv', index=False, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zW827VtQBV_"
      },
      "outputs": [],
      "source": [
        "full_dataframe.to_csv('train_dataset.csv', index=False, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVX3NDr-PB_z"
      },
      "outputs": [],
      "source": [
        "full_dataframe.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qB2uNPhqnJqw"
      },
      "outputs": [],
      "source": [
        "full_dataframe.tail(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Scaling and Cropping Strategy\n",
        "\n",
        "## Motivation\n",
        "The goal behind this data augmentation strategy is to help the model become more robust at estimating cloud heights when the LiDAR measurement point isn't always in the center of the image. This is important because:\n",
        "\n",
        "1. **Original Data Limitation**\n",
        "   - LiDAR measurements are always taken at the exact center of each image\n",
        "   - This could cause the model to overfit to center-focused features\n",
        "   - Real-world applications may need height estimates from different parts of the image\n",
        "\n",
        "2. **Spatial Understanding**\n",
        "   - Want the model to understand cloud height features regardless of their position\n",
        "   - Need to learn relationships between cloud features across the entire image\n",
        "   - Important for generalizing to different viewing angles and positions\n",
        "\n",
        "## Implementation Strategy\n",
        "\n",
        "### 1. Random Cropping (random_crop_sequence)\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\textbf{Algorithm: Random Crop Sequence} \\\\\n",
        "& \\textbf{Input: } \\text{image_sequence, min_crop_size = 384} \\\\\n",
        "& \\textbf{Output: } \\text{cropped_sequence, new_center_coords} \\\\\n",
        "& \\hline \\\\\n",
        "& \\text{orig_height, orig_width} \\gets \\text{image_size} \\\\\n",
        "& \\text{crop_size} \\gets \\text{random_int(min_crop_size, min(orig_height, orig_width))} \\\\\n",
        "& \\text{top} \\gets \\text{random_int(0, orig_height - crop_size)} \\\\\n",
        "& \\text{left} \\gets \\text{random_int(0, orig_width - crop_size)} \\\\\n",
        "& \\text{orig_center} \\gets \\text{(orig_width}/2\\text{, orig_height}/2\\text{)} \\\\\n",
        "& \\text{new_center} \\gets \\text{(orig_center.x - left, orig_center.y - top)} \\\\\n",
        "& \\text{cropped_sequence} \\gets \\text{crop_images(sequence, top, left, crop_size)}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Key Features:\n",
        "- Random crop size between minimum (384) and image dimension\n",
        "- Random position for the crop window\n",
        "- Maintains aspect ratio (square crop)\n",
        "- Original center point is transformed to new coordinates\n",
        "\n",
        "### 2. Resizing (resize_sequence_and_adjust_center)\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\textbf{Algorithm: Resize and Adjust Center} \\\\\n",
        "& \\textbf{Input: } \\text{cropped_sequence, center_coords, target_size = 384} \\\\\n",
        "& \\textbf{Output: } \\text{resized_sequence, adjusted_center_coords} \\\\\n",
        "& \\hline \\\\\n",
        "& \\text{scale_factor} \\gets \\text{target_size}/\\text{crop_size} \\\\\n",
        "& \\text{new_center.x} \\gets \\text{center_coords.x} \\times \\text{scale_factor} \\\\\n",
        "& \\text{new_center.y} \\gets \\text{center_coords.y} \\times \\text{scale_factor} \\\\\n",
        "& \\text{resized_sequence} \\gets \\text{resize_images(cropped_sequence, target_size)}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "## Benefits of This Approach\n",
        "\n",
        "1. **Data Augmentation**\n",
        "   - Effectively multiplies training data\n",
        "   - Creates variations in LiDAR measurement position\n",
        "   - Introduces scale variation while preserving aspect ratios\n",
        "\n",
        "2. **Model Robustness**\n",
        "   - Forces model to look at cloud features everywhere in image\n",
        "   - Prevents overfitting to center-specific patterns\n",
        "   - Better generalization to different viewing conditions\n",
        "\n",
        "3. **Training Stability**\n",
        "   - Consistent final image size (384x384)\n",
        "   - Maintains original aspect ratios\n",
        "   - Preserves relative spatial relationships\n",
        "\n",
        "4. **Validation Strategy**\n",
        "   - During validation, no random cropping is applied\n",
        "   - Center point remains fixed for validation\n",
        "   - Allows fair comparison with ground truth\n",
        "\n",
        "# Random Scaling and Cropping Example\n",
        "\n",
        "Original Image:\n",
        "- Size: 800 x 800\n",
        "- LiDAR measurement point: (400, 400) [exact center]\n",
        "\n",
        "Let's say our random crop generates:\n",
        "- Crop size: 500 x 500 (randomly chosen between 384 and 800)\n",
        "- Top-left position: (220, 150) [randomly chosen to fit the 500x500 crop]\n",
        "\n",
        "Step 1: Crop Transformation\n",
        "```\n",
        "Original center: (400, 400)\n",
        "Crop offset: (220, 150)\n",
        "New center = Original center - Crop offset\n",
        "New center = (400-220, 400-150) = (180, 250)\n",
        "```\n",
        "So in our 500x500 cropped image, the LiDAR point is now at (180, 250) - notably off-center\n",
        "\n",
        "Step 2: Resize to 384x384\n",
        "```\n",
        "Scale factor = 384/500 = 0.768\n",
        "Final center = (180 * 0.768, 250 * 0.768) = (138, 192)\n",
        "```\n",
        "\n",
        "Final Result:\n",
        "- Image size: 384 x 384\n",
        "- LiDAR point: (138, 192)\n",
        "- Original center point has moved significantly off-center\n",
        "  - Horizontally: shifted from center by 54 pixels (384/2 - 138 = 54)\n",
        "  - Vertically: shifted from center by 0 pixels (384/2 - 192 = 0)\n",
        "\n",
        "This asymmetric transformation better illustrates how the cropping strategy creates variations in the LiDAR measurement position, forcing the model to learn to estimate cloud heights from features across the entire image rather than just the center region.\n",
        "\n",
        "Here's a visual example of the process:"
      ],
      "metadata": {
        "id": "iBHaATqvDsb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<svg width=\"1000\" height=\"400\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 1000 400\">\n",
        "    <!-- Original 800x800 Image -->\n",
        "    <rect x=\"20\" y=\"20\" width=\"320\" height=\"320\" fill=\"none\" stroke=\"#666666\" stroke-width=\"2\"/>\n",
        "    <text x=\"20\" y=\"15\" font-family=\"Arial\" font-size=\"14\">Original Image (800 x 800)</text>\n",
        "    <circle cx=\"180\" cy=\"180\" r=\"4\" fill=\"red\"/>\n",
        "    <text x=\"140\" y=\"150\" font-family=\"Arial\" font-size=\"12\" fill=\"red\">LiDAR Point (400, 400)</text>\n",
        "\n",
        "    <!-- Crop Box -->\n",
        "    <rect x=\"108\" y=\"80\" width=\"200\" height=\"200\" fill=\"rgba(0, 102, 204, 0.1)\" stroke=\"#0066cc\" stroke-width=\"2\"/>\n",
        "    <text x=\"108\" y=\"75\" font-family=\"Arial\" font-size=\"14\" fill=\"#0066cc\">Crop Region (500 x 500)</text>\n",
        "    <text x=\"108\" y=\"300\" font-family=\"Arial\" font-size=\"12\" fill=\"#0066cc\">Starting at (220, 150)</text>\n",
        "\n",
        "    <!-- Arrow -->\n",
        "    <path d=\"M 370 180 L 430 180\" stroke=\"black\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
        "\n",
        "    <!-- Cropped Image -->\n",
        "    <rect x=\"460\" y=\"20\" width=\"200\" height=\"200\" fill=\"none\" stroke=\"#0066cc\" stroke-width=\"2\"/>\n",
        "    <text x=\"460\" y=\"15\" font-family=\"Arial\" font-size=\"14\">Cropped Image (500 x 500)</text>\n",
        "    <circle cx=\"532\" cy=\"130\" r=\"4\" fill=\"red\"/>\n",
        "    <text x=\"492\" y=\"100\" font-family=\"Arial\" font-size=\"12\" fill=\"red\">LiDAR Point (180, 250)</text>\n",
        "\n",
        "    <!-- Arrow -->\n",
        "    <path d=\"M 690 180 L 750 180\" stroke=\"black\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
        "\n",
        "    <!-- Final Resized Image -->\n",
        "    <rect x=\"780\" y=\"58\" width=\"154\" height=\"154\" fill=\"none\" stroke=\"#009933\" stroke-width=\"2\"/>\n",
        "    <text x=\"780\" y=\"53\" font-family=\"Arial\" font-size=\"14\">Resized (384 x 384)</text>\n",
        "    <circle cx=\"836\" cy=\"147\" r=\"4\" fill=\"red\"/>\n",
        "    <text x=\"796\" y=\"117\" font-family=\"Arial\" font-size=\"12\" fill=\"red\">LiDAR Point (138, 192)</text>\n",
        "\n",
        "    <!-- Arrowhead definition -->\n",
        "    <defs>\n",
        "        <marker id=\"arrowhead\" markerWidth=\"10\" markerHeight=\"7\" refX=\"9\" refY=\"3.5\" orient=\"auto\">\n",
        "            <polygon points=\"0 0, 10 3.5, 0 7\" fill=\"black\"/>\n",
        "        </marker>\n",
        "    </defs>\n",
        "\n",
        "    <!-- Grid overlay on middle image to show scale -->\n",
        "    <g stroke=\"rgba(128,128,128,0.2)\">\n",
        "        <line x1=\"460\" y1=\"70\" x2=\"660\" y2=\"70\"/>\n",
        "        <line x1=\"460\" y1=\"120\" x2=\"660\" y2=\"120\"/>\n",
        "        <line x1=\"460\" y1=\"170\" x2=\"660\" y2=\"170\"/>\n",
        "\n",
        "        <line x1=\"510\" y1=\"20\" x2=\"510\" y2=\"220\"/>\n",
        "        <line x1=\"560\" y1=\"20\" x2=\"560\" y2=\"220\"/>\n",
        "        <line x1=\"610\" y1=\"20\" x2=\"610\" y2=\"220\"/>\n",
        "    </g>\n",
        "</svg>"
      ],
      "metadata": {
        "id": "A9uMBvf5Ds8j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "9a61f08e-48ba-4dfc-bfb9-e99f4a01f098"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<svg width=\"1000\" height=\"400\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 1000 400\">\n",
              "    <!-- Original 800x800 Image -->\n",
              "    <rect x=\"20\" y=\"20\" width=\"320\" height=\"320\" fill=\"none\" stroke=\"#666666\" stroke-width=\"2\"/>\n",
              "    <text x=\"20\" y=\"15\" font-family=\"Arial\" font-size=\"14\">Original Image (800 x 800)</text>\n",
              "    <circle cx=\"180\" cy=\"180\" r=\"4\" fill=\"red\"/>\n",
              "    <text x=\"140\" y=\"150\" font-family=\"Arial\" font-size=\"12\" fill=\"red\">LiDAR Point (400, 400)</text>\n",
              "\n",
              "    <!-- Crop Box -->\n",
              "    <rect x=\"108\" y=\"80\" width=\"200\" height=\"200\" fill=\"rgba(0, 102, 204, 0.1)\" stroke=\"#0066cc\" stroke-width=\"2\"/>\n",
              "    <text x=\"108\" y=\"75\" font-family=\"Arial\" font-size=\"14\" fill=\"#0066cc\">Crop Region (500 x 500)</text>\n",
              "    <text x=\"108\" y=\"300\" font-family=\"Arial\" font-size=\"12\" fill=\"#0066cc\">Starting at (220, 150)</text>\n",
              "\n",
              "    <!-- Arrow -->\n",
              "    <path d=\"M 370 180 L 430 180\" stroke=\"black\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
              "\n",
              "    <!-- Cropped Image -->\n",
              "    <rect x=\"460\" y=\"20\" width=\"200\" height=\"200\" fill=\"none\" stroke=\"#0066cc\" stroke-width=\"2\"/>\n",
              "    <text x=\"460\" y=\"15\" font-family=\"Arial\" font-size=\"14\">Cropped Image (500 x 500)</text>\n",
              "    <circle cx=\"532\" cy=\"130\" r=\"4\" fill=\"red\"/>\n",
              "    <text x=\"492\" y=\"100\" font-family=\"Arial\" font-size=\"12\" fill=\"red\">LiDAR Point (180, 250)</text>\n",
              "\n",
              "    <!-- Arrow -->\n",
              "    <path d=\"M 690 180 L 750 180\" stroke=\"black\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
              "\n",
              "    <!-- Final Resized Image -->\n",
              "    <rect x=\"780\" y=\"58\" width=\"154\" height=\"154\" fill=\"none\" stroke=\"#009933\" stroke-width=\"2\"/>\n",
              "    <text x=\"780\" y=\"53\" font-family=\"Arial\" font-size=\"14\">Resized (384 x 384)</text>\n",
              "    <circle cx=\"836\" cy=\"147\" r=\"4\" fill=\"red\"/>\n",
              "    <text x=\"796\" y=\"117\" font-family=\"Arial\" font-size=\"12\" fill=\"red\">LiDAR Point (138, 192)</text>\n",
              "\n",
              "    <!-- Arrowhead definition -->\n",
              "    <defs>\n",
              "        <marker id=\"arrowhead\" markerWidth=\"10\" markerHeight=\"7\" refX=\"9\" refY=\"3.5\" orient=\"auto\">\n",
              "            <polygon points=\"0 0, 10 3.5, 0 7\" fill=\"black\"/>\n",
              "        </marker>\n",
              "    </defs>\n",
              "\n",
              "    <!-- Grid overlay on middle image to show scale -->\n",
              "    <g stroke=\"rgba(128,128,128,0.2)\">\n",
              "        <line x1=\"460\" y1=\"70\" x2=\"660\" y2=\"70\"/>\n",
              "        <line x1=\"460\" y1=\"120\" x2=\"660\" y2=\"120\"/>\n",
              "        <line x1=\"460\" y1=\"170\" x2=\"660\" y2=\"170\"/>\n",
              "\n",
              "        <line x1=\"510\" y1=\"20\" x2=\"510\" y2=\"220\"/>\n",
              "        <line x1=\"560\" y1=\"20\" x2=\"560\" y2=\"220\"/>\n",
              "        <line x1=\"610\" y1=\"20\" x2=\"610\" y2=\"220\"/>\n",
              "    </g>\n",
              "</svg>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n_NEMQvS9Vj"
      },
      "source": [
        "# Image loading and augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIDG3YP6PEFm"
      },
      "outputs": [],
      "source": [
        "def load_image_from_blob_cv(blob_img):\n",
        "    \"\"\"\n",
        "    Loads the image from the Azure Blob Storage using OpenCV and returns it as a numpy array.\n",
        "    Args:\n",
        "        blob_img (str): name of the blob image in the container\n",
        "    Returns:\n",
        "        (numpy.ndarray): loaded image in greyscale\n",
        "    \"\"\"\n",
        "    blob_client = container_client.get_blob_client(blob_img)\n",
        "    streamdownloader = blob_client.download_blob()\n",
        "    blob_data = streamdownloader.readall()\n",
        "    image_array = np.asarray(bytearray(blob_data), dtype=np.uint8)\n",
        "    img_bgr = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    return img_rgb\n",
        "\n",
        "def augment_color_image(img, contrast_factor=1.2, brightness_beta=10, kernel_size=(5, 5), blur=True):\n",
        "    \"\"\"\n",
        "    Augments the color image to enhance cloud features. Includes:\n",
        "      1. Contrast and Brightness Adjustment\n",
        "      2. Gaussian Blur (optional)\n",
        "      3. CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
        "      4. Sharpening (optional)\n",
        "\n",
        "    Args:\n",
        "        img (numpy.ndarray): color image in RGB format\n",
        "        contrast_factor (float): contrast factor\n",
        "        brightness_beta (int): brightness factor\n",
        "        kernel_size (tuple): kernel size for Gaussian Blur\n",
        "        blur (bool): whether to apply Gaussian Blur\n",
        "\n",
        "    Returns:\n",
        "        (numpy.ndarray): augmented image\n",
        "    \"\"\"\n",
        "    # Step 1: Adjust contrast and brightness\n",
        "    img_enhanced = cv2.convertScaleAbs(img, alpha=contrast_factor, beta=brightness_beta)\n",
        "\n",
        "    # Step 2: Optionally apply Gaussian Blur\n",
        "    if blur:\n",
        "        img_enhanced = cv2.GaussianBlur(img_enhanced, kernel_size, 0)\n",
        "\n",
        "    # Step 3: Convert to LAB color space to apply CLAHE on the L channel\n",
        "    img_lab = cv2.cvtColor(img_enhanced, cv2.COLOR_RGB2LAB)\n",
        "    l, a, b = cv2.split(img_lab)\n",
        "\n",
        "    # Step 4: Apply CLAHE (Clip Limit Adaptive Histogram Equalization)\n",
        "    clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(4, 4))\n",
        "    cl = clahe.apply(l)\n",
        "\n",
        "    # Step 5: Merge CLAHE-enhanced L channel back with A and B channels\n",
        "    img_clahe = cv2.merge((cl, a, b))\n",
        "\n",
        "    # Step 6: Convert back to RGB color space\n",
        "    img_clahe_rgb = cv2.cvtColor(img_clahe, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "    # Step 7: Optionally apply sharpening\n",
        "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
        "    img_sharpened = cv2.filter2D(img_clahe_rgb, -1, kernel)\n",
        "\n",
        "    return img_sharpened\n",
        "\n",
        "# Modify augment_color_image to use the original CLAHE-only method\n",
        "def augment_color_image_clahe_only(img, clip_limit=1.5, tile_grid_size=(4, 4)):\n",
        "    \"\"\"\n",
        "    Augments the color image by applying CLAHE (Contrast Limited Adaptive Histogram Equalization) only.\n",
        "\n",
        "    Args:\n",
        "        img (numpy.ndarray): color image in RGB format\n",
        "        clip_limit (float): clip limit for CLAHE (higher values give more contrast)\n",
        "        tile_grid_size (tuple): size of the grid for applying CLAHE\n",
        "\n",
        "    Returns:\n",
        "        (numpy.ndarray): augmented image with CLAHE applied to the L channel in LAB color space\n",
        "    \"\"\"\n",
        "    # Convert to LAB color space to apply CLAHE on the L channel (lightness)\n",
        "    img_lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
        "    l, a, b = cv2.split(img_lab)\n",
        "\n",
        "    # Apply CLAHE to the L channel\n",
        "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
        "    cl = clahe.apply(l)\n",
        "\n",
        "    # Merge CLAHE-enhanced L channel back with A and B channels\n",
        "    img_clahe = cv2.merge((cl, a, b))\n",
        "\n",
        "    # Convert back to RGB color space\n",
        "    img_clahe_rgb = cv2.cvtColor(img_clahe, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "    return img_clahe_rgb\n",
        "\n",
        "def augment_greyscale_image(img, contrast_factor=1.5, brightness_beta=30, kernel_size=(5, 5), blur=True):\n",
        "    \"\"\"\n",
        "    Augments the Greyscale image to enhance the feature of the cloud. Includes:\n",
        "      1. Contrast and Brightness\n",
        "      2. Gaussian Blur\n",
        "      3. Histogram Equalization\n",
        "      4. Sharpening\n",
        "    Args:\n",
        "        img (numpy.ndarray): greyscale image\n",
        "        contrast_factor (float): contrast factor\n",
        "        brightness_beta (int): brightness factor\n",
        "        kernel_size (tuple): kernel size for Gaussian Blur\n",
        "        blur (bool): whether to apply Gaussian Blur\n",
        "    Returns:\n",
        "        (numpy.ndarray): augmented image\n",
        "    \"\"\"\n",
        "    img_enhanced = cv2.convertScaleAbs(img, alpha=contrast_factor, beta=brightness_beta)\n",
        "    if blur:\n",
        "        img_enhanced = cv2.GaussianBlur(img_enhanced, kernel_size, 0)\n",
        "    img_enhanced = cv2.equalizeHist(img_enhanced)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    img_enhanced = clahe.apply(img_enhanced)\n",
        "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
        "    return cv2.filter2D(img_enhanced, -1, kernel)\n",
        "\n",
        "def visualize_augmentations_cv(image_list, enhance=True):\n",
        "    \"\"\"\n",
        "    Visualizes original and augmented greyscale images using OpenCV.\n",
        "    Args:\n",
        "        image_list (list): List of image paths in Azure Blob Storage\n",
        "        enhance (bool): Whether to apply image augmentation\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(len(image_list), 3, figsize=(15, 5 * len(image_list)))\n",
        "\n",
        "    for idx, blob_img in enumerate(image_list):\n",
        "        original_image = load_image_from_blob_cv(blob_img)\n",
        "        greyscale_image = cv2.cvtColor(original_image, cv2.COLOR_RGB2GRAY)\n",
        "        augmented_image = greyscale_image.copy()\n",
        "\n",
        "        if enhance:\n",
        "            augmented_image = augment_greyscale_image(augmented_image, contrast_factor=1.5, brightness_beta=30, kernel_size=(5, 5))\n",
        "\n",
        "        axes[idx, 0].imshow(original_image)\n",
        "        axes[idx, 0].set_title(\"Original Image (RGB)\")\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        axes[idx, 1].imshow(greyscale_image, cmap='gray')\n",
        "        axes[idx, 1].set_title(\"Greyscale Image\")\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        axes[idx, 2].imshow(augmented_image, cmap='gray')\n",
        "        axes[idx, 2].set_title(\"Augmented Image\")\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def random_crop_sequence(image_sequence, crop_size_minimum=224):\n",
        "    \"\"\"\n",
        "    Apply identical random crop to a sequence of images and ensure randomness in cropping location.\n",
        "    The crop size is randomly selected between the minimum size (crop_size_minimum) and the full image size.\n",
        "\n",
        "    Args:\n",
        "        image_sequence: A list or tensor of images in the sequence (each image should be the same size).\n",
        "        crop_size_minimum: The minimum crop size (default is 224).\n",
        "\n",
        "    Returns:\n",
        "        cropped_sequence: The cropped sequence of images.\n",
        "        new_center_coords: The new coordinates of the center pixel after cropping.\n",
        "    \"\"\"\n",
        "    # Get the height and width of the original images\n",
        "    orig_height, orig_width = image_sequence[0].shape[-2:]\n",
        "\n",
        "    # Randomly select the crop size between the minimum and full image size\n",
        "    max_crop_size = min(orig_height, orig_width)\n",
        "    crop_h = random.randint(crop_size_minimum, max_crop_size)\n",
        "    crop_w = crop_h  # Keeping the crop square for simplicity\n",
        "\n",
        "    # Ensure that the crop size is smaller than the original image\n",
        "    assert crop_h <= orig_height and crop_w <= orig_width, \"Crop size must be smaller than the original image size.\"\n",
        "\n",
        "    # Original center coordinates of the image\n",
        "    orig_center_x = orig_width // 2\n",
        "    orig_center_y = orig_height // 2\n",
        "\n",
        "    # Randomly select the top-left corner for the crop\n",
        "    top = random.randint(0, orig_height - crop_h)\n",
        "    left = random.randint(0, orig_width - crop_w)\n",
        "\n",
        "    # Apply the same crop to each image in the sequence\n",
        "    cropped_sequence = [TF.crop(img, top, left, crop_h, crop_w) for img in image_sequence]\n",
        "\n",
        "    # Calculate the new center coordinates based on the original center relative to the cropped region\n",
        "    new_center_x = orig_center_x - left  # Adjust the original center x based on the crop left offset\n",
        "    new_center_y = orig_center_y - top   # Adjust the original center y based on the crop top offset\n",
        "\n",
        "    # Ensure the new center coordinates are still within the cropped image bounds\n",
        "    new_center_x = max(0, min(new_center_x, crop_w - 1))\n",
        "    new_center_y = max(0, min(new_center_y, crop_h - 1))\n",
        "\n",
        "    return cropped_sequence, (new_center_x, new_center_y)\n",
        "\n",
        "def resize_sequence_and_adjust_center(cropped_sequence, new_center_coords, target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Resize the cropped sequence to a target size and adjust the center coordinates accordingly.\n",
        "\n",
        "    Args:\n",
        "        cropped_sequence: The list of cropped images.\n",
        "        new_center_coords: The (x, y) coordinates of the center in the cropped image.\n",
        "        target_size: The desired output size (height, width), default is (224, 224).\n",
        "\n",
        "    Returns:\n",
        "        resized_sequence: The resized sequence of images.\n",
        "        resized_center_coords: The adjusted (x, y) coordinates of the center in the resized images.\n",
        "    \"\"\"\n",
        "    crop_h, crop_w = cropped_sequence[0].shape[-2:]  # Get height and width of the cropped image\n",
        "    target_h, target_w = target_size  # Desired target size (e.g., 224x224)\n",
        "\n",
        "    # Calculate the scaling factors for height and width\n",
        "    scale_x = target_w / crop_w\n",
        "    scale_y = target_h / crop_h\n",
        "\n",
        "    # Resize each image in the sequence to the target size\n",
        "    resized_sequence = [TF.resize(img, size=target_size) for img in cropped_sequence]\n",
        "\n",
        "    # Adjust the center coordinates according to the scaling factor\n",
        "    new_center_x, new_center_y = new_center_coords\n",
        "    resized_center_x = int(new_center_x * scale_x)\n",
        "    resized_center_y = int(new_center_y * scale_y)\n",
        "\n",
        "    return resized_sequence, (resized_center_x, resized_center_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlDGEu4RUMv4"
      },
      "outputs": [],
      "source": [
        "images = [\n",
        "    \"20170418/170418_175706_183328_frames/20170418_175706_frame_0.jpg\",\n",
        "    \"20170418/170418_175706_183328_frames/20170418_175707_frame_60.jpg\",\n",
        "    \"20170418/170418_175706_183328_frames_tight_crop/20170418_175708_frame_120_cropped.jpg\",\n",
        "    \"20170418/170418_175706_183328_frames_tight_crop/20170418_175709_frame_180_cropped.jpg\",\n",
        "    \"20170418/170418_175706_183328_frames_tight_crop/20170418_175710_frame_240_cropped.jpg\",\n",
        "    \"20170418/170418_175706_183328_frames_crop_corrected_aligned/20170418_175708_frame_120.jpg\",\n",
        "    \"20170418/170418_175706_183328_frames_crop_corrected_aligned/20170418_175709_frame_180.jpg\",\n",
        "    \"20170418/170418_175706_183328_frames_crop_corrected_aligned/20170418_175710_frame_240.jpg\",\n",
        "]\n",
        "\n",
        "visualize_augmentations_cv(images, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl_5IDXZ3L49"
      },
      "source": [
        "# FlightMetadataManager Design\n",
        "\n",
        "The FlightMetadataManager is designed to handle the complex array of sensor and flight data collected during NASA's ER-2 missions. Here's why different types of metadata are handled in specific ways:\n",
        "\n",
        "## 1. Metadata Categorization\n",
        "\n",
        "The metadata is categorized into distinct groups based on their physical meaning and units:\n",
        "\n",
        "### Geographic Parameters\n",
        "- `Lat`, `Lon`: Geographic coordinates\n",
        "- `GPS_MSL_Alt`, `WGS_84_Alt`, `Press_Alt`: Different altitude measurements\n",
        "- **Motivation**: These parameters require consistent handling as they're used for spatial positioning and often need to be used together for accurate location determination.\n",
        "\n",
        "### Aircraft Motion Parameters\n",
        "- `Grnd_Spd`, `True_Airspeed`, `Mach_Number`: Speed measurements\n",
        "- `Vert_Velocity`: Vertical movement\n",
        "- `True_Hdg`, `Track`, `Drift`, `Pitch`, `Roll`: Aircraft orientation\n",
        "- **Motivation**: These parameters describe the aircraft's motion and orientation, which affect image capture conditions and need to be considered when analyzing cloud heights.\n",
        "\n",
        "### Environmental Parameters\n",
        "- `Ambient_Temp`, `Total_Temp`: Temperature readings\n",
        "- `Static_Press`, `Dynamic_Press`, `Cabin_Pressure`: Pressure measurements\n",
        "- `Wind_Speed`, `Wind_Dir`: Wind conditions\n",
        "- **Motivation**: Environmental conditions can affect both the aircraft's performance and the cloud formation being studied.\n",
        "\n",
        "### Solar Parameters\n",
        "- `Solar_Zenith`, `Sun_Elev_AC`, `Sun_Az_Grd`, `Sun_Az_AC`: Sun position relative to aircraft\n",
        "- **Motivation**: Helps in understanding lighting conditions which affect image quality and interpretation.\n",
        "\n",
        "## 2. Special Handling Features\n",
        "\n",
        "### Temporal Feature Engineering\n",
        "The system employs cyclic encoding for temporal features through trigonometric transformations:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\text{hour}_{\\sin} = \\sin(2\\pi \\cdot \\text{hour}/24) \\\\\n",
        "& \\text{hour}_{\\cos} = \\cos(2\\pi \\cdot \\text{hour}/24) \\\\\n",
        "& \\text{day}_{\\sin} = \\sin(2\\pi \\cdot \\text{day}/365) \\\\\n",
        "& \\text{day}_{\\cos} = \\cos(2\\pi \\cdot \\text{day}/365)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "**Motivation**:\n",
        "- Preserves cyclic nature of time (e.g., hour 23 is close to hour 0)\n",
        "- Natural range of [-1, 1] eliminates need for additional normalization\n",
        "- Smooth transitions between time periods\n",
        "- Better representation for neural network processing\n",
        "\n",
        "### Normalization Strategy\n",
        "\n",
        "The system applies different normalization strategies based on parameter type:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\textbf{For standard parameters:} \\\\\\\\\n",
        "& x_{\\text{normalized}} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}} \\\\\\\\\n",
        "& \\textbf{For angular parameters:} \\\\\\\\\n",
        "& \\theta_{\\text{normalized}} = \\begin{cases}\n",
        "\\sin(\\theta) & \\text{for sine component} \\\\\\\\\n",
        "\\cos(\\theta) & \\text{for cosine component}\n",
        "\\end{cases} \\\\\\\\\n",
        "& \\textbf{For pre-normalized parameters:} \\\\\\\\\n",
        "& x_{\\text{final}} = x_{\\text{original}}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "## 3. Parameter Organization\n",
        "\n",
        "### Feature Type Grouping\n",
        "\n",
        "The system organizes parameters into hierarchical categories:\n",
        "\n",
        "1. Primary Features\n",
        "   $$\\mathbf{F}_{\\text{primary}} = \\{\\mathbf{F}_{\\text{flight}}, \\mathbf{F}_{\\text{temporal}}, \\mathbf{F}_{\\text{target}}\\}$$\n",
        "\n",
        "2. Flight Data Subset\n",
        "   $$\\mathbf{F}_{\\text{flight}} = \\{\\mathbf{P}_{\\text{geographic}}, \\mathbf{P}_{\\text{motion}}, \\mathbf{P}_{\\text{environmental}}, \\mathbf{P}_{\\text{solar}}\\}$$\n",
        "\n",
        "3. Parameter Vectors\n",
        "   $$\\mathbf{P}_{\\text{geographic}} = [{\\text{Lat}, \\text{Lon}, \\text{Alt}_{\\text{GPS}}, \\text{Alt}_{\\text{WGS}}, \\text{Alt}_{\\text{Press}}}]$$\n",
        "   $$\\mathbf{P}_{\\text{motion}} = [\\text{Speed}_{\\text{ground}}, \\text{Speed}_{\\text{air}}, \\text{Mach}, \\text{Velocity}_{\\text{vert}}, ...]$$\n",
        "\n",
        "### Index Management\n",
        "\n",
        "The system maintains a strict ordering system:\n",
        "\n",
        "1. Base Index Structure:\n",
        "   $$I(p) : \\mathbf{F}_{\\text{all}} \\rightarrow \\mathbb{N}_0$$\n",
        "   \n",
        "2. Parameter Access:\n",
        "   $$p_i = I^{-1}(i) \\text{ where } i \\in [0, |\\mathbf{F}_{\\text{all}}| - 1]$$\n",
        "\n",
        "This organization ensures:\n",
        "- Consistent parameter ordering across batches\n",
        "- O(1) parameter lookup time\n",
        "- Maintainable structure for feature additions\n",
        "- Clear separation of concerns for different parameter types\n",
        "\n",
        "## 4. Key Benefits\n",
        "\n",
        "1. **Data Integrity**\n",
        "   - Consistent handling of related parameters\n",
        "   - Proper normalization based on parameter type\n",
        "   - Preservation of meaningful relationships between parameters\n",
        "\n",
        "2. **Model Performance**\n",
        "   - Well-organized features for neural network input\n",
        "   - Properly scaled values for better training\n",
        "   - Efficient batch processing\n",
        "\n",
        "3. **Maintenance and Extensibility**\n",
        "   - Clear structure for adding new parameters\n",
        "   - Easy modification of normalization strategies\n",
        "   - Simple parameter grouping updates\n",
        "\n",
        "4. **Error Prevention**\n",
        "   - Type checking for parameters\n",
        "   - Validation of normalization requirements\n",
        "   - Consistent parameter ordering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz0WScEm3LVs"
      },
      "outputs": [],
      "source": [
        "class FlightMetadataManager:\n",
        "    def __init__(self):\n",
        "        # Define all metadata parameters with their properties\n",
        "        self.metadata_config = {\n",
        "            # Temporal parameters (non-normalized)\n",
        "            'timestamp': {'normalize': False, 'type': 'temporal'},\n",
        "\n",
        "            # Height validation (normalized)\n",
        "            'validation_height': {'normalize': True, 'type': 'height'},\n",
        "\n",
        "            # Geographic coordinates (normalized)\n",
        "            'Lat': {'normalize': True, 'type': 'coordinate'},\n",
        "            'Lon': {'normalize': True, 'type': 'coordinate'},\n",
        "            'GPS_MSL_Alt': {'normalize': True, 'type': 'altitude'},\n",
        "            'WGS_84_Alt': {'normalize': True, 'type': 'altitude'},\n",
        "            'Press_Alt': {'normalize': True, 'type': 'altitude'},\n",
        "\n",
        "            # Speed parameters (normalized)\n",
        "            'Grnd_Spd': {'normalize': True, 'type': 'speed'},\n",
        "            'True_Airspeed': {'normalize': True, 'type': 'speed'},\n",
        "            'Mach_Number': {'normalize': True, 'type': 'dimensionless'},\n",
        "            'Vert_Velocity': {'normalize': True, 'type': 'speed'},\n",
        "\n",
        "            # Angular parameters (normalized)\n",
        "            'True_Hdg': {'normalize': True, 'type': 'angle'},\n",
        "            'Track': {'normalize': True, 'type': 'angle'},\n",
        "            'Drift': {'normalize': True, 'type': 'angle'},\n",
        "            'Pitch': {'normalize': True, 'type': 'angle'},\n",
        "            'Roll': {'normalize': True, 'type': 'angle'},\n",
        "\n",
        "            # Temperature parameters (normalized)\n",
        "            'Ambient_Temp': {'normalize': True, 'type': 'temperature'},\n",
        "            'Total_Temp': {'normalize': True, 'type': 'temperature'},\n",
        "\n",
        "            # Pressure parameters (normalized)\n",
        "            'Static_Press': {'normalize': True, 'type': 'pressure'},\n",
        "            'Dynamic_Press': {'normalize': True, 'type': 'pressure'},\n",
        "            'Cabin_Pressure': {'normalize': True, 'type': 'pressure'},\n",
        "\n",
        "            # Wind parameters (normalized)\n",
        "            'Wind_Speed': {'normalize': True, 'type': 'speed'},\n",
        "            'Wind_Dir': {'normalize': True, 'type': 'angle'},\n",
        "\n",
        "            # Solar parameters (normalized)\n",
        "            'Solar_Zenith': {'normalize': True, 'type': 'angle'},\n",
        "            'Sun_Elev_AC': {'normalize': True, 'type': 'angle'},\n",
        "            'Sun_Az_Grd': {'normalize': True, 'type': 'angle'},\n",
        "            'Sun_Az_AC': {'normalize': True, 'type': 'angle'},\n",
        "\n",
        "            # Encoded temporal features (pre-normalized)\n",
        "            'hour_sin': {'normalize': False, 'type': 'encoded_temporal'},\n",
        "            'hour_cos': {'normalize': False, 'type': 'encoded_temporal'},\n",
        "            'day_sin': {'normalize': False, 'type': 'encoded_temporal'},\n",
        "            'day_cos': {'normalize': False, 'type': 'encoded_temporal'}\n",
        "        }\n",
        "\n",
        "        # Create ordered lists of parameters for different purposes\n",
        "        self.flight_data = [param for param, config in self.metadata_config.items()\n",
        "                                if config['type'] != 'temporal' and param != 'validation_height']\n",
        "\n",
        "        self.temporal_features = [param for param, config in self.metadata_config.items()\n",
        "                                if config['type'] == 'temporal']\n",
        "\n",
        "        # All input features (flight data + temporal)\n",
        "        self.all_features = self.flight_data + self.temporal_features\n",
        "\n",
        "        # Create indices mapping for quick lookup\n",
        "        self.param_indices = {param: idx for idx, param in enumerate(self.all_features)}\n",
        "\n",
        "        # All columns that need normalization (including validation_height)\n",
        "        self.columns_to_normalize = [param for param, config in self.metadata_config.items()\n",
        "                                        if config['normalize']]\n",
        "\n",
        "    def get_index(self, param_name):\n",
        "        \"\"\"Get the index of a parameter in the normalized feature list.\"\"\"\n",
        "        return self.param_indices[param_name]\n",
        "\n",
        "    def get_feature_types(self):\n",
        "        \"\"\"Get flight data and temporal features as separate lists.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (flight_data, temporal_features) where\n",
        "                flight_data: List of normalized flight parameters\n",
        "                temporal_features: List of temporal features\n",
        "        \"\"\"\n",
        "        return self.flight_data, self.temporal_features\n",
        "\n",
        "    def print_feature_indices(self):\n",
        "      \"\"\"Print all features and their corresponding indices in a formatted table.\"\"\"\n",
        "      print(\"\\nFlight Data Features:\")\n",
        "      print(\"-\" * 50)\n",
        "      for param in self.flight_data:\n",
        "          print(f\"{self.param_indices[param]:2d}: {param}\")\n",
        "\n",
        "      print(\"\\nTemporal Features:\")\n",
        "      print(\"-\" * 50)\n",
        "      for param in self.temporal_features:\n",
        "          print(f\"{self.param_indices[param]:2d}: {param}\")\n",
        "\n",
        "metadata_manager = FlightMetadataManager()\n",
        "metadata_manager.print_feature_indices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_3aakY2yLth"
      },
      "source": [
        "# Cloud2CloudDataset Design\n",
        "\n",
        "## Purpose and Core Functionality\n",
        "\n",
        "The Cloud2CloudDataset class implements a custom PyTorch Dataset that manages three synchronized data streams:\n",
        "1. FEGS HD camera images\n",
        "2. Aircraft metadata\n",
        "3. LiDAR validation height measurements\n",
        "\n",
        "## Dataset Initialization\n",
        "\n",
        "### Input Parameters\n",
        "- `date_folders`: List of target flight dates\n",
        "- `normalization_params`: Optional pre-computed normalization parameters\n",
        "- `transform`: Image transformations pipeline\n",
        "- `augmentations`: Optional additional image augmentations\n",
        "- `apply_normalization`: Boolean flag for data normalization\n",
        "- `apply_crop_and_scale`: Boolean flag for random cropping\n",
        "- `resize_size`: Target image size (default 384)\n",
        "\n",
        "### Key Components\n",
        "\n",
        "1. **Temporal Feature Engineering**\n",
        "   \n",
        "   Cyclic encoding of time features:\n",
        "  $$\n",
        "  \\begin{align*}\n",
        "  & \\text{hour_sin} = \\sin(2\\pi \\cdot \\text{hour}/24) \\\\\n",
        "  & \\text{hour_cos} = \\cos(2\\pi \\cdot \\text{hour}/24) \\\\\n",
        "  & \\text{day_sin} = \\sin(2\\pi \\cdot \\text{day}/365) \\\\\n",
        "  & \\text{day_cos} = \\cos(2\\pi \\cdot \\text{day}/365)\n",
        "  \\end{align*}\n",
        "  $$\n",
        "\n",
        "2. **Data Normalization**\n",
        "\n",
        "   For each numerical column:\n",
        "  $$\n",
        "  x_{\\text{normalized}} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n",
        "  $$\n",
        "\n",
        "   Special handling for validation heights:\n",
        "  $$\n",
        "  \\begin{align*}\n",
        "  \\text{height}{\\text{normalized}} = \\begin{cases}\n",
        "  \\frac{\\text{height} - \\text{height}{\\min}}{\\text{height}{\\max} - \\text{height}{\\min}} & \\text{if height is valid} \\\\\n",
        "  \\text{NaN} & \\text{if height is missing}\n",
        "  \\end{cases}\n",
        "  \\end{align*}\n",
        "  $$\n",
        "\n",
        "3. **Sequence Management**\n",
        "   \n",
        "   For a sequence $S$ of length $n$:\n",
        "  $$\n",
        "  \\begin{align*}\n",
        "  & S = {f_1, f_2, ..., f_n} \\text{ where } f_i \\text{ is a frame} \\\\\n",
        "  & \\text{length}(S) = \\min(n, \\text{max_sequence_length}) \\\\\n",
        "  & \\text{valid}(S) \\iff \\text{length}(S) \\geq \\text{min_sequence_length}\n",
        "  \\end{align*}\n",
        "  $$\n",
        "\n",
        "## Data Processing Pipeline\n",
        "\n",
        "### 1. Image Processing\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\textbf{Input Image } I_{800 \\times 800} \\\\\n",
        "& \\downarrow \\text{ Convert to Grayscale} \\\\\n",
        "& I_{\\text{gray}} = \\text{RGB2GRAY}(I_{800 \\times 800}) \\\\\n",
        "& \\downarrow \\text{ Apply Augmentations} \\\\\n",
        "& I_{\\text{aug}} = \\text{Augment}(I_{\\text{gray}}) \\text{ where:} \\\\\n",
        "& \\quad \\bullet \\text{ Contrast Enhancement} \\\\\n",
        "& \\quad \\bullet \\text{ Histogram Equalization} \\\\\n",
        "& \\quad \\bullet \\text{ CLAHE} \\\\\n",
        "& \\downarrow \\text{ Random Crop and Resize} \\\\\n",
        "& I_{\\text{final}} = \\text{Resize}(\\text{RandomCrop}(I_{\\text{aug}}), 384 \\times 384)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "\n",
        "### 2. Sequence Construction\n",
        "\n",
        "For each valid sequence:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\textbf{Sequence } S_i = {(I_j, M_j, h_j)}_{j=1}^n \\text{ where:} \\\\\n",
        "& \\quad I_j = \\text{processed image} \\\\\n",
        "& \\quad M_j = \\text{metadata vector} \\\\\n",
        "& \\quad h_j = \\text{validation height (if available)} \\\\\n",
        "& \\textbf{Constraint: } \\text{timestamp}(j+1) - \\text{timestamp}(j) \\approx 5 \\text{ seconds}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "### 3. Batch Formation\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\textbf{Batch } B = {S_1, S_2, ..., S_b} \\text{ where:} \\\\\n",
        "& \\quad b = \\text{batch size} \\\\\n",
        "& \\quad \\text{max_len} = \\max(\\text{length}(S_i)) \\\\\n",
        "& \\quad \\text{Pad shorter sequences to max_len}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "## Access Methods\n",
        "\n",
        "1. **Length Function**:\n",
        "   $$|D| = \\text{number of valid sequences}$$\n",
        "\n",
        "2. **Item Access**:\n",
        "  $$\n",
        "  \\begin{align*}\n",
        "  & D[i] \\rightarrow (I_i, M_i, h_i, c_i, p_i, t_i) \\text{ where:} \\\\\n",
        "  & \\quad I_i = \\text{image sequence tensor} \\\\\n",
        "  & \\quad M_i = \\text{metadata tensor} \\\\\n",
        "  & \\quad h_i = \\text{validation height} \\\\\n",
        "  & \\quad c_i = \\text{center coordinates} \\\\\n",
        "  & \\quad p_i = \\text{image paths} \\\\\n",
        "  & \\quad t_i = \\text{timestamps}\n",
        "  \\end{align*}\n",
        "  $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7y52OAldzLI"
      },
      "outputs": [],
      "source": [
        "class Cloud2CloudDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                dataframe,\n",
        "                normalization_params=None,\n",
        "                transform=None, augmentations=None,\n",
        "                apply_normalization=True,\n",
        "                apply_crop_and_scale=True,\n",
        "                resize_size=224):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe (pd.DataFrame): dataframe containing image paths, flight data, and ground truth.\n",
        "            normalization_params (dict, optional): Dictionary containing min and max values for all the fields.\n",
        "            transform (callable, optional): optional transform to apply to each image.\n",
        "            augmentations (callable, optional): optional augmentations to apply to each image.\n",
        "            apply_normalization (bool): whether to apply normalization to the dataframe.\n",
        "            resize_size (int): The target size to resize the images (square dimensions).\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe.copy()\n",
        "        self.transform = transform\n",
        "        self.augmentations = augmentations\n",
        "        self.apply_normalization = apply_normalization\n",
        "        self.apply_crop_and_scale = apply_crop_and_scale\n",
        "        self.resize_size = resize_size\n",
        "\n",
        "        # Initialize metadata manager\n",
        "        self.metadata_manager = FlightMetadataManager()\n",
        "\n",
        "        # Add temporal encoding columns\n",
        "        hour = self.dataframe['hour_of_day']\n",
        "        day = self.dataframe['day_of_year']\n",
        "        self.dataframe['hour_sin'] = np.sin(2 * np.pi * hour / 24.0)\n",
        "        self.dataframe['hour_cos'] = np.cos(2 * np.pi * hour / 24.0)\n",
        "        self.dataframe['day_sin'] = np.sin(2 * np.pi * day / 365.0)\n",
        "        self.dataframe['day_cos'] = np.cos(2 * np.pi * day / 365.0)\n",
        "\n",
        "        if self.apply_normalization:\n",
        "            # Calculate or use provided normalization parameters\n",
        "            self.normalization_params = normalization_params or self._calculate_normalization_params(\n",
        "                self.dataframe, self.metadata_manager.columns_to_normalize\n",
        "            )\n",
        "\n",
        "            for col in self.metadata_manager.columns_to_normalize:\n",
        "                print(f\"\\nNormalizing {col}:\")\n",
        "                print(f\"Min: {self.normalization_params[col]['min']}\")\n",
        "                print(f\"Max: {self.normalization_params[col]['max']}\")\n",
        "\n",
        "                col_min = self.normalization_params[col]['min']\n",
        "                col_max = self.normalization_params[col]['max']\n",
        "\n",
        "                if col == 'validation_height':\n",
        "                    # Only normalize non-NaN values for validation_height\n",
        "                    # (we use NaNs to indicate frames with no LiDAR data)\n",
        "                    mask = ~self.dataframe[col].isna()\n",
        "                    self.dataframe.loc[mask, col] = (self.dataframe.loc[mask, col] - col_min) / (col_max - col_min)\n",
        "                else:\n",
        "                    # Normalize all values for other columns\n",
        "                    self.dataframe[col] = (self.dataframe[col] - col_min) / (col_max - col_min)\n",
        "                    if self.dataframe[col].isna().any():\n",
        "                        print(\"WARNING: NaN values detected after normalization!\")\n",
        "\n",
        "        # Track the indices where sequences start\n",
        "        self.sequence_indices = self._generate_sequence_indices()\n",
        "\n",
        "    def _calculate_normalization_params(self, dataframe, columns_to_normalize):\n",
        "        \"\"\"\n",
        "        Manually calculate min and max values for the columns and store them for consistency across datasets.\n",
        "        \"\"\"\n",
        "        params = {}\n",
        "        for col in columns_to_normalize:\n",
        "            params[col] = {\n",
        "                'min': dataframe[col].min(),\n",
        "                'max': dataframe[col].max()\n",
        "            }\n",
        "        return params\n",
        "\n",
        "    def denormalize_validation_height(self, normalized_height):\n",
        "        \"\"\"\n",
        "        Denormalize the validation height using the stored min and max values for validation_height.\n",
        "\n",
        "        Args:\n",
        "            normalized_height (float or np.array): The normalized validation height(s) to denormalize.\n",
        "\n",
        "        Returns:\n",
        "            float or np.array: The denormalized validation height(s).\n",
        "        \"\"\"\n",
        "        # Check if normalization_params exists; if not, return the input as-is\n",
        "        if not hasattr(self, 'normalization_params') or 'validation_height' not in self.normalization_params:\n",
        "            return normalized_height\n",
        "\n",
        "        # Get min and max for validation_height\n",
        "        col_min = self.normalization_params['validation_height']['min']\n",
        "        col_max = self.normalization_params['validation_height']['max']\n",
        "\n",
        "        print(f\"  Max: {col_max} Min: {col_min}\")\n",
        "\n",
        "        # Denormalize using the stored min and max values\n",
        "        original_height = normalized_height * (col_max - col_min) + col_min\n",
        "        return original_height\n",
        "\n",
        "    def denormalize_flight_data(self, normalized_flight_data):\n",
        "        \"\"\"\n",
        "        Denormalize flight data using the stored min and max values for each column.\n",
        "\n",
        "        Args:\n",
        "            normalized_flight_data (np.array): Normalized flight data array to denormalize.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Denormalized flight data.\n",
        "        \"\"\"\n",
        "        # Check if normalization_params exists; if not, return the input as-is\n",
        "        if not hasattr(self, 'normalization_params'):\n",
        "            return normalized_flight_data\n",
        "\n",
        "        denormalized_flight_data = []\n",
        "        # Use flight_data which does not contain validation_height\n",
        "        for i, col in enumerate(self.metadata_manager.flight_data):\n",
        "            if col not in self.normalization_params:\n",
        "                # If normalization parameters for a column are missing, return the normalized value as-is\n",
        "                denormalized_flight_data.append(normalized_flight_data[i])\n",
        "            else:\n",
        "                col_min = self.normalization_params[col]['min']\n",
        "                col_max = self.normalization_params[col]['max']\n",
        "                denorm_value = normalized_flight_data[i] * (col_max - col_min) + col_min\n",
        "                denormalized_flight_data.append(denorm_value)\n",
        "\n",
        "        return np.array(denormalized_flight_data)\n",
        "\n",
        "    def _generate_sequence_indices(self):\n",
        "        \"\"\"\n",
        "        Generate a list of indices where each sequence starts.\n",
        "        If a NaN is encountered in the sequence_length, the process stops.\n",
        "        \"\"\"\n",
        "        sequence_indices = []\n",
        "        idx = 0\n",
        "\n",
        "        while idx < len(self.dataframe):\n",
        "            sequence_length = self.dataframe.iloc[idx]['sequence_length']\n",
        "\n",
        "            if pd.isna(sequence_length):\n",
        "                # Stop if sequence_length is NaN (no more sequences)\n",
        "                break\n",
        "\n",
        "            # Convert sequence_length to an integer\n",
        "            sequence_length = int(sequence_length)\n",
        "            sequence_indices.append(idx)\n",
        "            idx += sequence_length  # Move to the start of the next sequence\n",
        "\n",
        "        return sequence_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequence_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the starting index of the sequence\n",
        "        start_idx = self.sequence_indices[idx]\n",
        "\n",
        "        # Get the sequence length for this specific starting point\n",
        "        sequence_length = int(self.dataframe.iloc[start_idx]['sequence_length'])\n",
        "\n",
        "        # Get image paths for the sequence\n",
        "        image_paths = self.dataframe.iloc[start_idx:start_idx + sequence_length]['image_path'].tolist()\n",
        "\n",
        "        # Initialize sequences\n",
        "        image_sequence = []\n",
        "        flight_data_sequence = []\n",
        "        timestamp_sequence = []  # New sequence for timestamps\n",
        "\n",
        "        # Get feature types from metadata manager\n",
        "        numeric_features, temporal_features = self.metadata_manager.get_feature_types()\n",
        "\n",
        "        # Single loop to process both images and flight data\n",
        "        for i in range(sequence_length):\n",
        "            # Process image\n",
        "            image_path = self.dataframe.iloc[start_idx + i]['image_path']\n",
        "\n",
        "            # Load image using OpenCV\n",
        "            img_rgb = load_image_from_blob_cv(image_path)\n",
        "\n",
        "            # Get the height and width of the loaded image\n",
        "            h, w = img_rgb.shape[:2]\n",
        "\n",
        "            # # [Optional] Convert RGB to grayscale\n",
        "            # img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "            # # Apply augmentations to grayscale image\n",
        "            # img_gray = augment_greyscale_image(img_gray)\n",
        "\n",
        "            # # Convert grayscale image to RGB format\n",
        "            # img_rgb_converted = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "            # Apply the color augmentation instead of grayscale\n",
        "            img_rgb_converted = augment_color_image_clahe_only(img_rgb, clip_limit=3.0, tile_grid_size=(8, 8))\n",
        "\n",
        "            # Apply additional augmentations to the RGB image if any\n",
        "            if self.augmentations:\n",
        "                img_rgb_converted = self.augmentations(img_rgb_converted)\n",
        "\n",
        "            # Convert back to tensor and append to the sequence\n",
        "            img_tensor = TF.to_tensor(img_rgb_converted)\n",
        "            image_sequence.append(img_tensor)\n",
        "\n",
        "            # Process flight data\n",
        "            row = self.dataframe.iloc[start_idx + i]\n",
        "\n",
        "            # Extract flight data and ensure numeric values\n",
        "            flight_data = []\n",
        "            for feature in numeric_features:  # Changed from motion_features to numeric_features\n",
        "                value = row[feature]\n",
        "                # Convert to float and handle any non-numeric values\n",
        "                try:\n",
        "                    flight_data.append(float(value))\n",
        "                except (ValueError, TypeError):\n",
        "                    print(f\"Error converting feature {feature} with value {value}\")\n",
        "                    flight_data.append(0.0)  # or some other default value\n",
        "\n",
        "            flight_data_sequence.append(flight_data)\n",
        "\n",
        "            # Process timestamps separately\n",
        "            for feature in temporal_features:\n",
        "                timestamp_sequence.append(row[feature])\n",
        "\n",
        "        # Apply random cropping and resizing only if `apply_crop_and_scale` is True\n",
        "        if self.apply_crop_and_scale:\n",
        "            # Apply random cropping to the entire sequence\n",
        "            cropped_sequence, new_center_coords = random_crop_sequence(image_sequence, crop_size_minimum=self.resize_size)\n",
        "            # Resize the cropped sequence and adjust the center coordinates\n",
        "            resized_sequence, resized_center_coords = resize_sequence_and_adjust_center(cropped_sequence, new_center_coords, target_size=(self.resize_size, self.resize_size))\n",
        "            # Stack images into a tensor\n",
        "            image_sequence = torch.stack(resized_sequence)\n",
        "        else:\n",
        "            # Resize original images to resize_size x resize_size\n",
        "            resized_sequence = [TF.resize(img, size=(self.resize_size, self.resize_size)) for img in image_sequence]\n",
        "            # Stack the resized images\n",
        "            image_sequence = torch.stack(resized_sequence)\n",
        "            # Calculate center of the resized square image\n",
        "            resized_center_coords = (self.resize_size // 2, self.resize_size // 2)\n",
        "\n",
        "        # print(f\"Image sequence shape before model: {image_sequence.shape}\")\n",
        "        if image_sequence.shape[-1] != 384 or image_sequence.shape[-2] != 384:\n",
        "            raise ValueError(f\"Incorrect image dimensions: {image_sequence.shape}. Expected 384x384\")\n",
        "\n",
        "        # Convert flight data to tensor\n",
        "        flight_data = torch.tensor(flight_data_sequence, dtype=torch.float32)\n",
        "\n",
        "        # Fetch the validation height (target)\n",
        "        validation_height = self.dataframe.iloc[start_idx + sequence_length - 1]['validation_height']\n",
        "        validation_height = torch.tensor(validation_height, dtype=torch.float32)\n",
        "\n",
        "        # Return with timestamp_sequence added\n",
        "        return image_sequence, flight_data, validation_height, resized_center_coords, image_paths, timestamp_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P0lT-b__LHg"
      },
      "source": [
        "# Train/Validation Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO7G_vVYybgT"
      },
      "source": [
        "# Training and Validation Split Design\n",
        "\n",
        "## 1. Initial Dataset Creation\n",
        "\n",
        "First, we create a full dataset without normalization:\n",
        "\n",
        "$$D_{\\text{full}} = \\{S_1, S_2, ..., S_n\\}$$\n",
        "\n",
        "where each $S_i$ is a sequence of frames.\n",
        "\n",
        "## 2. Sequence-Based Splitting\n",
        "\n",
        "Rather than splitting individual frames randomly, we split entire sequences:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "& I_{\\text{sequences}} = {1, 2, ..., n} \\text{ where } n = |D_{\\text{full}}| \\\\\n",
        "& \\text{Split } I_{\\text{sequences}} \\text{ into:} \\\\\n",
        "& \\quad I_{\\text{train}} \\text{ (80% of sequences)} \\\\\n",
        "& \\quad I_{\\text{val}} \\text{ (20% of sequences)}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "### Motivation for Sequence-Based Splitting:\n",
        "- Preserves temporal continuity within sequences\n",
        "- Prevents data leakage between train/validation\n",
        "- Maintains sequence integrity\n",
        "\n",
        "## 3. Row Extraction Process\n",
        "\n",
        "For each set (train and validation):\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\text{For each sequence index } i: \\\\\n",
        "& \\quad L_i = \\text{sequence_length}(i) \\\\\n",
        "& \\quad R_i = {i, i+1, ..., i+L_i-1} \\\\\n",
        "& R_{\\text{train}} = \\bigcup_{i \\in I_{\\text{train}}} R_i \\\\\n",
        "& R_{\\text{val}} = \\bigcup_{i \\in I_{\\text{val}}} R_i\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "## 4. Dataset Creation\n",
        "\n",
        "### Training Dataset\n",
        "$$\n",
        "\\begin{align*}\n",
        "& D_{\\text{train}} = \\text{Cloud2CloudDataset}(R_{\\text{train}}) \\text{ with:} \\\\\n",
        "& \\quad \\bullet \\text{ apply_normalization = True} \\\\\n",
        "& \\quad \\bullet \\text{ apply_crop_and_scale = True} \\\\\n",
        "& \\quad \\bullet \\text{ Calculate normalization parameters:} \\\\\n",
        "& \\quad\\quad \\text{For each feature } f: \\\\\n",
        "& \\quad\\quad\\quad \\mu_f = \\text{mean}(f) \\\\\n",
        "& \\quad\\quad\\quad \\sigma_f = \\text{std}(f)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "### Validation Dataset\n",
        "$$\n",
        "\\begin{align*}\n",
        "& D_{\\text{val}} = \\text{Cloud2CloudDataset}(R_{\\text{val}}) \\text{ with:} \\\\\n",
        "& \\quad \\bullet \\text{ apply_normalization = True} \\\\\n",
        "& \\quad \\bullet \\text{ apply_crop_and_scale = False} \\\\\n",
        "& \\quad \\bullet \\text{ Use training normalization parameters:} \\\\\n",
        "& \\quad\\quad f_{\\text{normalized}} = \\frac{f - \\mu_f}{\\sigma_f}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "## 5. Key Differences Between Train and Validation\n",
        "\n",
        "### Training Set Features:\n",
        "- Random cropping and scaling\n",
        "- Data augmentation\n",
        "- Shuffled sequences\n",
        "- Calculates normalization parameters\n",
        "\n",
        "### Validation Set Features:\n",
        "- No random cropping (center crop only)\n",
        "- Limited augmentation\n",
        "- Sequential processing\n",
        "- Uses training set's normalization parameters\n",
        "\n",
        "### **Note: We use training-only normalization parameters**\n",
        "\n",
        "This is an important machine learning practice for several reasons:\n",
        "\n",
        "a) **Preventing Data Leakage**\n",
        "   - If we calculated normalization parameters using all data (including validation), we'd be letting information from the validation set influence our training data\n",
        "   - This would create a subtle form of data leakage where our model gets to \"peek\" at validation data statistics\n",
        "\n",
        "b) **Real-world Scenario Simulation**\n",
        "   - In production, we'll need to normalize new, unseen data\n",
        "   - We won't be able to calculate new normalization parameters for this data\n",
        "   - Using training-only parameters better simulates this real-world scenario\n",
        "\n",
        "c) **Validation Integrity**\n",
        "   - Validation should measure how well our model generalizes\n",
        "   - Using training-derived normalization parameters tests if our normalization approach itself generalizes\n",
        "   - If validation performance is good, it suggests our normalization is robust\n",
        "\n",
        "Mathematically, for any feature $f$:\n",
        "```\n",
        "Training:\n",
        "μ_train = mean(f_train)\n",
        "σ_train = std(f_train)\n",
        "f_train_normalized = (f_train - μ_train) / σ_train\n",
        "\n",
        "Validation:\n",
        "f_val_normalized = (f_val - μ_train) / σ_train  # Using training parameters\n",
        "```\n",
        "\n",
        "This ensures that our validation metrics genuinely reflect how well our model will perform on unseen data.\n",
        "\n",
        "## 6. DataLoader Configuration\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\text{Loader}{\\text{train}} = \\text{DataLoader}(D{\\text{train}}) \\text{ with:} \\\\\n",
        "& \\quad \\bullet \\text{ batch_size = 1} \\\\\n",
        "& \\quad \\bullet \\text{ shuffle = True} \\\\\n",
        "& \\quad \\bullet \\text{ custom collate function} \\\\\n",
        "& \\text{Loader}{\\text{val}} = \\text{DataLoader}(D{\\text{val}}) \\text{ with:} \\\\\n",
        "& \\quad \\bullet \\text{ batch_size = 1} \\\\\n",
        "& \\quad \\bullet \\text{ shuffle = False} \\\\\n",
        "& \\quad \\bullet \\text{ same collate function}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "At this stage, the dataloader using only a batch size of 1 is useful for:\n",
        "\n",
        "- Visualizing individual sequences\n",
        "- Debugging data transformations\n",
        "- Checking center point calculations\n",
        "- Verifying normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o6CBp7ZK9nV"
      },
      "outputs": [],
      "source": [
        "# Create the full Cloud2CloudDataset (no normalization yet)\n",
        "full_cloud2cloud_dataset = Cloud2CloudDataset(dataframe=full_dataframe, transform=transform, augmentations=augmentations, apply_normalization=False, resize_size=384)\n",
        "\n",
        "# Split sequence indices into training and validation sets\n",
        "train_sequence_indices, val_sequence_indices = train_test_split(full_cloud2cloud_dataset.sequence_indices, test_size=0.2, random_state=42)\n",
        "\n",
        "# Extract rows for training dataset based on sequence indices\n",
        "train_rows = []\n",
        "for seq_start in train_sequence_indices:\n",
        "    sequence_length = int(full_cloud2cloud_dataset.dataframe.iloc[seq_start]['sequence_length'])\n",
        "    train_rows.extend(range(seq_start, seq_start + sequence_length))\n",
        "\n",
        "train_dataframe = full_dataframe.iloc[train_rows].reset_index(drop=True)\n",
        "\n",
        "# Create the training dataset and calculate normalization parameters\n",
        "train_cloud2cloud_dataset = Cloud2CloudDataset(dataframe=train_dataframe, transform=transform, augmentations=augmentations, apply_normalization=False, apply_crop_and_scale=True, resize_size=384)\n",
        "\n",
        "# Get normalization parameters from the training dataset if it exists\n",
        "if hasattr(train_cloud2cloud_dataset, 'normalization_params'):\n",
        "    normalization_params = train_cloud2cloud_dataset.normalization_params\n",
        "else:\n",
        "    normalization_params = None\n",
        "\n",
        "# Extract rows for validation dataset based on sequence indices\n",
        "val_rows = []\n",
        "for seq_start in val_sequence_indices:\n",
        "    sequence_length = int(full_cloud2cloud_dataset.dataframe.iloc[seq_start]['sequence_length'])\n",
        "    val_rows.extend(range(seq_start, seq_start + sequence_length))\n",
        "\n",
        "val_dataframe = full_dataframe.iloc[val_rows].reset_index(drop=True)\n",
        "\n",
        "# Create the validation dataset using the same normalization parameters\n",
        "val_cloud2cloud_dataset = Cloud2CloudDataset(dataframe=val_dataframe, normalization_params=normalization_params, transform=transform, augmentations=augmentations, apply_normalization=False, apply_crop_and_scale=False, resize_size=384)\n",
        "\n",
        "# Create DataLoaders for training and validation sets\n",
        "train_dataloader_single_batch = DataLoader(train_cloud2cloud_dataset, batch_size=1, shuffle=True)\n",
        "val_dataloader_single_batch = DataLoader(val_cloud2cloud_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zqfnQnoFf2S"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "KbcEC7v1yd8N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYt2qzVDjwOG"
      },
      "outputs": [],
      "source": [
        "def print_num_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in a PyTorch model.\n",
        "    \"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total trainable parameters: {total_params}\")\n",
        "\n",
        "def get_denormalized_metadata(dataset, data, sample_idx=0):\n",
        "    \"\"\"Get denormalized metadata values for height calculation.\"\"\"\n",
        "    # Get normalized metadata for the last frame\n",
        "    metadata_tensor = data['metadata'][sample_idx, -1]\n",
        "\n",
        "    # Get the denormalized values using the dataset's parameters\n",
        "    denorm_data = dataset.denormalize_flight_data(metadata_tensor)\n",
        "\n",
        "    # Create metadata dictionary using metadata manager indices\n",
        "    metadata = {\n",
        "        'GPS_MSL_Alt': float(denorm_data[dataset.metadata_manager.get_index('GPS_MSL_Alt')]),\n",
        "        'True_Airspeed': float(denorm_data[dataset.metadata_manager.get_index('True_Airspeed')]),\n",
        "        'Track': float(denorm_data[dataset.metadata_manager.get_index('Track')]),\n",
        "        'Pitch': float(denorm_data[dataset.metadata_manager.get_index('Pitch')]),\n",
        "        'Roll': float(denorm_data[dataset.metadata_manager.get_index('Roll')]),\n",
        "        'Wind_Speed': float(denorm_data[dataset.metadata_manager.get_index('Wind_Speed')]),\n",
        "        'Wind_Dir': float(denorm_data[dataset.metadata_manager.get_index('Wind_Dir')])\n",
        "    }\n",
        "\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMXAWoadzpMA"
      },
      "source": [
        "## CloudMotionModel Class\n",
        "\n",
        "The `CloudMotionModel` is a dual-RAFT architecture designed for cloud motion estimation that combines a trainable RAFT model with a frozen reference model.\n",
        "\n",
        "## Model Components\n",
        "\n",
        "### 1. Dual RAFT Models\n",
        "The architecture employs two RAFT instances:\n",
        "- **Trainable RAFT**: Fine-tuned for cloud-specific motion\n",
        "- **Frozen Reference RAFT**: Maintains baseline motion estimates\n",
        "\n",
        "```python\n",
        "self.raft = raft_large(weights=Raft_Large_Weights.DEFAULT if use_pretrained else None)\n",
        "self.reference_raft = raft_large(weights=Raft_Large_Weights.DEFAULT)  # Frozen\n",
        "```\n",
        "\n",
        "### 2. Bidirectional Anomaly Detection and Preservation\n",
        "\n",
        "The preserve_bidirectional_anomalies method addresses a key challenge in cloud motion sequences: temporal inconsistency in feature detection. Cloud features, particularly at high altitudes, may be detected strongly in some frame pairs but weakly or not at all in others.\n",
        "\n",
        "The model identifies motion anomalies using statistical thresholds:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{strong_anomalies} &= \\text{motion} > (\\mu + \\sigma \\cdot \\text{threshold}) \\\\\n",
        "\\text{weak_anomalies} &= \\text{motion} < (\\mu - \\sigma \\cdot \\text{threshold})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\mu$ is the mean motion across the sequence (calculated per pixel)\n",
        "- $\\sigma$ is the standard deviation of motion\n",
        "- threshold is the anomaly detection parameter (default 2.0)\n",
        "\n",
        "For each pixel location $(x,y)$, the final motion is determined by:\n",
        "\n",
        "$$\n",
        "\\text{final_motion}(x,y) = \\begin{cases}\n",
        "\\max(\\text{motion}(x,y)) & \\text{if strong anomaly detected at any time} \\\\\n",
        "\\min(\\text{motion}(x,y)) & \\text{if weak anomaly detected at any time} \\\\\n",
        "\\mu(\\text{motion}(x,y)) & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "This preservation mechanism is helpful because cloud motion features in a sequence $\\{I_1, I_2, ..., I_T\\}$ may only be distinctly visible in specific frame pairs. For example, a high-altitude cloud feature might show significant motion between frames $I_1$ and $I_2$, but become less distinct in subsequent pairs due to:\n",
        "- Changes in viewing angle\n",
        "- Varying illumination conditions\n",
        "- Temporary occlusions by other cloud layers\n",
        "- Varying contrast against the background\n",
        "\n",
        "By preserving both strong and weak anomalies across the entire sequence, we ensure that important motion features are retained even if they're only detected in a subset of frame pairs. This is particularly important for high-altitude clouds where parallax effects might be subtle and inconsistent across the sequence.\n",
        "\n",
        "This approach significantly improves the robustness of height estimation by:\n",
        "- Preserving rare but significant motion features\n",
        "- Reducing the impact of temporary feature occlusions\n",
        "- Maintaining consistency in height estimates for high-altitude clouds\n",
        "- Combining evidence of cloud motion across multiple temporal samples\n",
        "\n",
        "### 3. Confidence Estimation\n",
        "\n",
        "Confidence scores are computed using motion magnitude:\n",
        "\n",
        "$$\n",
        "\\text{confidence} = \\sigma(\\|\\text{refined_motion}\\|_2)\n",
        "$$\n",
        "\n",
        "where $\\sigma$ is the sigmoid function to normalize confidence to [0,1].\n",
        "\n",
        "## Forward Pass Flow\n",
        "\n",
        "1. **Sequence Processing**:\n",
        "   - Input: Sequence of images $\\{I_1, I_2, ..., I_T\\}$\n",
        "   - For each consecutive pair $(I_t, I_{t+1})$:\n",
        "     - Scale images to [0, 255] range\n",
        "     - Process through both RAFT models\n",
        "     - Store motion fields\n",
        "\n",
        "2. **Motion Stack Creation**:\n",
        "   - Stack motion fields temporally\n",
        "   - Handle packed sequences if present\n",
        "   - Motion dimensions: $(B, T-1, 2, H, W)$\n",
        "   where B=batch size, T=sequence length, H=height, W=width\n",
        "\n",
        "3. **Motion Refinement**:\n",
        "   - Apply bidirectional anomaly preservation\n",
        "   - Generate confidence maps\n",
        "   - Handle packed sequence conversions\n",
        "\n",
        "4. **Output Generation**:\n",
        "```python\n",
        "return {\n",
        "    'motion_fields': Original motion sequence\n",
        "    'refined_motion': Anomaly-preserved motion\n",
        "    'reference_motion_fields': Reference RAFT sequence\n",
        "    'reference_motion': Reference refined motion\n",
        "    'confidence': Confidence maps\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pg7HOGH0i5qv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8e712a63-5130-436c-cf2e-b40002fc8527"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bf92085fda22>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCloudMotionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mNeural\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcombines\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreference\u001b[0m \u001b[0mRAFT\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcloud\u001b[0m \u001b[0mmotion\u001b[0m \u001b[0mestimation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0muses\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0mRAFT\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mRecurrent\u001b[0m \u001b[0mAll\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mPairs\u001b[0m \u001b[0mField\u001b[0m \u001b[0mTransforms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "class CloudMotionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network model that combines trainable and reference RAFT models for cloud motion estimation.\n",
        "\n",
        "    This model uses two RAFT (Recurrent All-Pairs Field Transforms) instances:\n",
        "    - A trainable model that can be fine-tuned for cloud-specific motion\n",
        "    - A frozen reference model that maintains baseline motion estimates\n",
        "\n",
        "    The model processes sequences of images to estimate motion fields, preserving motion anomalies\n",
        "    that may indicate important cloud features that are only visible in specific frame pairs.\n",
        "\n",
        "    Attributes:\n",
        "        raft (RAFT): Trainable RAFT model for motion estimation\n",
        "        reference_raft (RAFT): Frozen RAFT model providing baseline estimates\n",
        "        anomaly_threshold (float): Threshold for detecting motion anomalies (default: 2.0)\n",
        "\n",
        "    Example:\n",
        "        >>> model = CloudMotionModel(use_pretrained=True)\n",
        "        >>> image_sequence = torch.randn(1, 5, 3, 384, 384)  # Batch x Frames x Channels x Height x Width\n",
        "        >>> outputs = model(image_sequence)\n",
        "        >>> motion_fields = outputs['motion_fields']  # Sequence of motion fields\n",
        "        >>> refined_motion = outputs['refined_motion']  # Final motion with preserved anomalies\n",
        "    \"\"\"\n",
        "    def __init__(self, use_pretrained=True, anomaly_threshold=2.0):\n",
        "        super().__init__()\n",
        "        # Initialize trainable RAFT\n",
        "        self.raft = raft_large(\n",
        "            weights=Raft_Large_Weights.DEFAULT if use_pretrained else None\n",
        "        )\n",
        "\n",
        "        # Initialize frozen reference RAFT\n",
        "        self.reference_raft = raft_large(weights=Raft_Large_Weights.DEFAULT)\n",
        "        # Freeze all parameters of reference model\n",
        "        for param in self.reference_raft.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.anomaly_threshold = anomaly_threshold\n",
        "\n",
        "    def preserve_bidirectional_anomalies(self, motion_stack):\n",
        "        \"\"\"\n",
        "        Preserves significant motion features detected across a sequence of frame pairs.\n",
        "\n",
        "        Important cloud features may only be visible in specific frame pairs due to viewing angles\n",
        "        or illumination. This method identifies and preserves these features by detecting motion\n",
        "        anomalies (both strong and weak) across the sequence.\n",
        "\n",
        "        Args:\n",
        "            motion_stack (torch.Tensor or PackedSequence): Stack of motion fields from sequence\n",
        "                Shape: Batch x Sequence x 2 x Height x Width\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Final motion field with preserved anomalies\n",
        "        \"\"\"\n",
        "        # If packed sequence, unpack first\n",
        "        if isinstance(motion_stack, PackedSequence):\n",
        "            motion_data, lengths = pad_packed_sequence(motion_stack, batch_first=True)\n",
        "        else:\n",
        "            motion_data = motion_stack\n",
        "\n",
        "        mean_motion = torch.mean(motion_data, dim=1)\n",
        "        std_motion = torch.std(motion_data, dim=1)\n",
        "\n",
        "        # Identify both strong and weak anomalies\n",
        "        strong_anomalies = motion_data > (mean_motion.unsqueeze(1) + self.anomaly_threshold * std_motion.unsqueeze(1))\n",
        "        weak_anomalies = motion_data < (mean_motion.unsqueeze(1) - self.anomaly_threshold * std_motion.unsqueeze(1))\n",
        "\n",
        "        # Combine anomalies\n",
        "        anomalies = strong_anomalies | weak_anomalies\n",
        "\n",
        "        # For non-anomalous pixels, use the mean motion\n",
        "        final_motion = mean_motion.clone()\n",
        "\n",
        "        # For anomalous pixels, use the max or min value depending on whether it's a strong or weak anomaly\n",
        "        max_motion, _ = torch.max(motion_data, dim=1)\n",
        "        min_motion, _ = torch.min(motion_data, dim=1)\n",
        "\n",
        "        final_motion[torch.any(strong_anomalies, dim=1)] = max_motion[torch.any(strong_anomalies, dim=1)]\n",
        "        final_motion[torch.any(weak_anomalies, dim=1)] = min_motion[torch.any(weak_anomalies, dim=1)]\n",
        "\n",
        "        return final_motion\n",
        "\n",
        "    def forward(self, image_sequence):\n",
        "        \"\"\"\n",
        "        Processes an image sequence through both RAFT models to estimate motion.\n",
        "\n",
        "        Args:\n",
        "            image_sequence (torch.Tensor or PackedSequence): Sequence of images\n",
        "                Shape: Batch x Sequence x 3 x Height x Width\n",
        "                Values should be in [0,1] range\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing:\n",
        "                - motion_fields: Original motion sequence from trainable RAFT\n",
        "                - refined_motion: Motion with preserved anomalies from trainable RAFT\n",
        "                - reference_motion_fields: Original motion sequence from reference RAFT\n",
        "                - reference_motion: Motion with preserved anomalies from reference RAFT\n",
        "                - confidence: Confidence maps based on motion magnitude\n",
        "        \"\"\"\n",
        "        # If packed sequence, unpack first\n",
        "        if isinstance(image_sequence, PackedSequence):\n",
        "            images_unpacked, lengths = pad_packed_sequence(image_sequence, batch_first=True)\n",
        "        else:\n",
        "            images_unpacked = image_sequence\n",
        "\n",
        "        motion_fields = []\n",
        "        reference_motion_fields = []\n",
        "\n",
        "        # Process all frames in the batch together\n",
        "        for t in range(images_unpacked.shape[1] - 1):\n",
        "            img1 = images_unpacked[:, t] * 255.0\n",
        "            img2 = images_unpacked[:, t + 1] * 255.0\n",
        "\n",
        "            # Get predictions from trainable RAFT\n",
        "            flow_predictions = self.raft(img1, img2)\n",
        "            motion_fields.append(flow_predictions[-1])\n",
        "\n",
        "            # Get predictions from reference RAFT (no gradients)\n",
        "            with torch.no_grad():\n",
        "                reference_predictions = self.reference_raft(img1, img2)\n",
        "                reference_motion_fields.append(reference_predictions[-1])\n",
        "\n",
        "        motion_stack = torch.stack(motion_fields, dim=1)\n",
        "        reference_stack = torch.stack(reference_motion_fields, dim=1)\n",
        "\n",
        "        # Pack sequences if input was packed\n",
        "        if isinstance(image_sequence, PackedSequence):\n",
        "            motion_stack = pack_padded_sequence(motion_stack, [l-1 for l in lengths], batch_first=True)\n",
        "            reference_stack = pack_padded_sequence(reference_stack, [l-1 for l in lengths], batch_first=True)\n",
        "\n",
        "        # Apply bidirectional anomaly preservation\n",
        "        refined_motion = self.preserve_bidirectional_anomalies(motion_stack)\n",
        "        reference_motion = self.preserve_bidirectional_anomalies(reference_stack)\n",
        "\n",
        "        # Calculate confidence based on motion magnitude\n",
        "        if isinstance(refined_motion, PackedSequence):\n",
        "            refined_unpacked, _ = pad_packed_sequence(refined_motion, batch_first=True)\n",
        "            confidence = torch.norm(refined_unpacked, dim=1, keepdim=True)\n",
        "        else:\n",
        "            confidence = torch.norm(refined_motion, dim=1, keepdim=True)\n",
        "        confidence = torch.sigmoid(confidence)\n",
        "\n",
        "        return {\n",
        "            'motion_fields': motion_stack,\n",
        "            'refined_motion': refined_motion,\n",
        "            'reference_motion_fields': reference_stack,\n",
        "            'reference_motion': reference_motion,\n",
        "            'confidence': confidence,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGRDlorvtOBZ"
      },
      "source": [
        "## ParallaxHeightCalculator Class\n",
        "\n",
        "The ParallaxHeightCalculator implements the parallax principle to convert motion fields into cloud heights using a combination of LiDAR calibration and statistical methods.\n",
        "\n",
        "## Core Components\n",
        "\n",
        "### 1. Scale Management\n",
        "\n",
        "The calculator maintains three critical scales that transform raw pixel motion into meaningful physical cloud heights. Each scale solves a specific challenge in the height estimation process:\n",
        "\n",
        "**Motion Scale**: Maps pixel motion to physical distance\n",
        "- Raw optical flow gives motion in pixels\n",
        "- Need to convert to real-world distances (meters)\n",
        "- LiDAR provides ground truth for calibration\n",
        "- Higher altitude clouds show less pixel motion\n",
        "- Bias towards larger values helps detect high clouds\n",
        "\n",
        "**Height Scale**: Converts relative motion to absolute height\n",
        "- Parallax effect creates relative motion proportional to height ratio\n",
        "- Converts $(h_{\\text{cloud}}/h_{\\text{aircraft}})$ ratio to absolute height\n",
        "- Calibrated using LiDAR measurements\n",
        "- Updates slowly to maintain stability\n",
        "\n",
        "**Vertical Extent Scale**: Controls the vertical range of cloud features\n",
        "- Raw height calculations can compress or stretch vertical structure\n",
        "- Scales cloud thickness to physically reasonable ranges\n",
        "- Limited to 30% of cloud-top height or 3000m\n",
        "- Preserves realistic cloud vertical development\n",
        "\n",
        "Each scale uses asymmetric smoothing with bias towards larger values:\n",
        "\n",
        "$$\n",
        "\\text{scale}_{\\text{new}} = (1-\\alpha)\\text{scale}_{\\text{old}} + \\alpha(\\text{scale}_{\\text{update}})\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is:\n",
        "```\n",
        "0.05 if scale_update > scale_old\n",
        "0.01 if scale_update ≤ scale_old\n",
        "```\n",
        "\n",
        "This asymmetric smoothing is particularly important for detecting high-altitude clouds which might only be visible in certain frames or conditions while maintaining stability in the height calculations over time.\n",
        "\n",
        "### 2. Scale Calculation\n",
        "\n",
        "For each frame with LiDAR data:\n",
        "\n",
        "1. **Height Ratio**:\n",
        "   $$\\text{height_ratio} = \\frac{\\text{lidar_height}}{\\text{aircraft_altitude}}$$\n",
        "\n",
        "2. **Expected Motion**:\n",
        "   $$\\text{expected_motion} = \\text{aircraft_speed} \\times \\text{height_ratio}$$\n",
        "\n",
        "3. **Motion Scale**:\n",
        "   $$\\text{motion_scale} = \\frac{\\text{expected_motion}}{\\text{center_motion}}$$\n",
        "\n",
        "4. **Vertical Extent Scale**:\n",
        "   $$\\text{vertical_extent} = \\min(\\frac{0.3 \\times \\text{lidar_height}}{{\\text{current_range}}}, 3000)$$\n",
        "\n",
        "### 3. Height Calculation\n",
        "\n",
        "For each pixel $(x,y)$ with valid motion:\n",
        "\n",
        "1. **Relative Motion**:\n",
        "   $$\\text{relative_motion}_{x,y} = \\frac{\\text{motion_magnitude}_{x,y} \\times \\text{motion_scale}}{\\text{aircraft_speed}}$$\n",
        "\n",
        "2. **Raw Height**:\n",
        "   $$\\text{height}_{x,y} = \\text{aircraft_altitude} \\times \\text{relative_motion}_{x,y} \\times \\text{height_scale}$$\n",
        "\n",
        "3. **Confidence-weighted Smoothing**:\n",
        "   $$\\text{height}_{x,y} = c \\times \\text{height}_{x,y} + (1-c) \\times \\text{smooth}(\\text{height}_{x,y})$$\n",
        "   where $c$ is the confidence value.\n",
        "\n",
        "### 4. Height Field Adjustment\n",
        "\n",
        "When LiDAR calibration is used:\n",
        "1. Center height subtraction: $h' = h - h_{\\text{center}}$\n",
        "2. Vertical extent scaling: $h'' = h' \\times \\text{vertical_extent_scale}$\n",
        "3. LiDAR height addition: $h_{\\text{final}} = h'' + h_{\\text{lidar}}$\n",
        "\n",
        "### 5. Confidence Calculation\n",
        "\n",
        "Combines motion magnitude and local consistency:\n",
        "1. Base confidence: $c_1 = 1 - e^{-\\text{motion}/\\text{threshold}}$\n",
        "2. Local consistency: $c_2 = 1 - |\\text{motion} - \\text{smooth}(\\text{motion})| /\\text{smooth}(\\text{motion})$\n",
        "3. Final confidence: $c = c_1 \\times c_2$\n",
        "\n",
        "### 6. Robust Scale Updates\n",
        "\n",
        "Uses Median Absolute Deviation (MAD) for outlier rejection:\n",
        "$$\\text{MAD} = \\text{median}(|\\text{scales} - \\text{median}(\\text{scales})|)$$\n",
        "\n",
        "Inliers are defined as:\n",
        "$$|\\text{scale} - \\text{median}(\\text{scales})| < 5.0 \\times \\text{MAD}$$\n",
        "\n",
        "This ensures stable height estimation even with noisy motion fields and varying aircraft conditions, while the asymmetric smoothing preserves sensitivity to high-altitude cloud features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lZY2a9DoxaZ"
      },
      "outputs": [],
      "source": [
        "class ParallaxHeightCalculator:\n",
        "    \"\"\"\n",
        "    Calculates cloud heights from motion fields using the parallax principle.\n",
        "\n",
        "    This calculator uses a combination of motion fields, aircraft metadata, and LiDAR\n",
        "    measurements to estimate cloud heights. It maintains running scales that convert\n",
        "    pixel motion into physical heights and updates these scales using statistical methods.\n",
        "\n",
        "    Three key scales are maintained:\n",
        "    - Motion scale: Converts pixel motion to physical distance\n",
        "    - Height scale: Converts relative motion to absolute height\n",
        "    - Vertical extent scale: Controls reasonable vertical range of clouds\n",
        "\n",
        "    Attributes:\n",
        "        smoothing_factor (float): Base factor for scale smoothing\n",
        "        history_size (int): Maximum number of historical values to maintain\n",
        "        use_lidar_to_calculate_heights (bool): Whether to use LiDAR for direct calibration\n",
        "        running_motion_scale (float): Current motion scale value\n",
        "        running_height_scale (float): Current height scale value\n",
        "        running_vertical_extent_scale (float): Current vertical extent scale value\n",
        "    \"\"\"\n",
        "    def __init__(self, smoothing_factor=0.1, history_size=2000, use_lidar_to_calculate_heights=False):\n",
        "        self.smoothing_factor = smoothing_factor\n",
        "        self.running_motion_scale = None\n",
        "        self.running_height_scale = None\n",
        "        self.running_vertical_extent_scale = 1.0  # Initialize with 1.0\n",
        "        self.use_lidar_to_calculate_heights = use_lidar_to_calculate_heights\n",
        "\n",
        "        self.motion_scale_history = []\n",
        "        self.height_scale_history = []\n",
        "        self.vertical_extent_scale_history = []  # New history list\n",
        "        self.timestamp_history = []\n",
        "        self.altitude_history = []\n",
        "        self.aircraft_speed_history = []\n",
        "\n",
        "    def calculate_scales(self, motion_magnitude, metadata, lidar_height):\n",
        "        \"\"\"\n",
        "        Calculates scaling factors using LiDAR measurements for calibration.\n",
        "\n",
        "        Args:\n",
        "            motion_magnitude (np.ndarray): Magnitude of motion field\n",
        "            metadata (dict): Aircraft metadata including altitude and speed\n",
        "            lidar_height (float): LiDAR-measured cloud height for calibration\n",
        "\n",
        "        Returns:\n",
        "            tuple: (motion_scale, height_scale, vertical_extent_scale) or (None, None)\n",
        "                if calculation fails\n",
        "        \"\"\"\n",
        "        MIN_MOTION_THRESHOLD = 0.1\n",
        "\n",
        "        aircraft_altitude = float(metadata['GPS_MSL_Alt'])\n",
        "        aircraft_speed = float(metadata['True_Airspeed'])\n",
        "\n",
        "        debug.debug_print('motion', \"\\nScale Calculation Debug:\")\n",
        "        debug.debug_print('motion', f\"Aircraft altitude: {aircraft_altitude:.1f} m\")\n",
        "        debug.debug_print('motion', f\"LiDAR height: {lidar_height:.1f} m\")\n",
        "        debug.debug_print('motion', f\"Aircraft speed: {aircraft_speed:.1f} m/s\")\n",
        "\n",
        "        if aircraft_altitude <= 0 or aircraft_speed <= 0:\n",
        "            debug.debug_print('motion', \"Warning: Invalid altitude or speed\")\n",
        "            return None, None\n",
        "\n",
        "        # Get center motion with local averaging\n",
        "        center_y, center_x = motion_magnitude.shape[0] // 2, motion_magnitude.shape[1] // 2\n",
        "        kernel_size = 5\n",
        "        motion_smooth = cv2.GaussianBlur(motion_magnitude, (kernel_size, kernel_size), 0)\n",
        "        center_motion = motion_smooth[center_y, center_x]\n",
        "        debug.debug_print('motion', f\"Raw center motion (pixels): {center_motion:.3f}\")\n",
        "\n",
        "        if center_motion < MIN_MOTION_THRESHOLD:\n",
        "            debug.debug_print('motion', f\"Warning: Center motion ({center_motion:.3f} px) below threshold ({MIN_MOTION_THRESHOLD} px)\")\n",
        "            return None, None\n",
        "\n",
        "        # Calculate height ratio\n",
        "        height_ratio = lidar_height / aircraft_altitude\n",
        "\n",
        "        # Calculate expected center motion\n",
        "        expected_center_motion = aircraft_speed * height_ratio\n",
        "\n",
        "        debug.debug_print('motion', \"\\nHeight Ratio Calculation:\")\n",
        "        debug.debug_print('motion', f\"LiDAR height: {lidar_height}\")\n",
        "        debug.debug_print('motion', f\"Aircraft altitude: {aircraft_altitude}\")\n",
        "        debug.debug_print('motion', f\"Resulting Height ratio: {height_ratio:.3f}\")\n",
        "        debug.debug_print('motion', \"\\nMotion Scale Calculation:\")\n",
        "        debug.debug_print('motion', f\"Aircraft speed: {aircraft_speed}\")\n",
        "        debug.debug_print('motion', f\"Height ratio: {height_ratio}\")\n",
        "        debug.debug_print('motion', f\"Resulting Expected center motion (px/s): {expected_center_motion:.3f}\")\n",
        "\n",
        "        # Calculate expected vs actual relative motion\n",
        "        actual_relative_motion = center_motion / aircraft_speed\n",
        "        expected_relative_motion = lidar_height / aircraft_altitude\n",
        "\n",
        "        debug.debug_print('motion', \"\\nRelative Motion Analysis:\")\n",
        "        debug.debug_print('motion', f\"Expected relative motion at center: {expected_relative_motion:.3f}\")\n",
        "        debug.debug_print('motion', f\"Actual relative motion at center: {actual_relative_motion:.3f}\")\n",
        "        debug.debug_print('motion', f\"Relative motion difference: {(expected_relative_motion - actual_relative_motion):.3f}\")\n",
        "        debug.debug_print('motion', f\"Relative motion ratio: {(expected_relative_motion / actual_relative_motion):.3f}\")\n",
        "\n",
        "        # Calculate scales\n",
        "        motion_scale = expected_center_motion / (center_motion + 1e-6)\n",
        "        height_scale = height_ratio\n",
        "\n",
        "        # Calculate vertical extent scale\n",
        "        current_range = np.ptp(motion_magnitude * motion_scale * aircraft_speed)\n",
        "        target_range = min(3000, lidar_height * 0.3)  # 30% of LiDAR height, minimum of 3000m\n",
        "        vertical_extent_scale = max(target_range / (current_range + 1e-6), 0.5)  # Never scale below 50%\n",
        "\n",
        "        # Update running vertical extent scale\n",
        "        if self.running_vertical_extent_scale is None:\n",
        "            self.running_vertical_extent_scale = vertical_extent_scale\n",
        "        else:\n",
        "            self.running_vertical_extent_scale = (\n",
        "                self.running_vertical_extent_scale * 0.9 + vertical_extent_scale * 0.1\n",
        "            )\n",
        "\n",
        "        debug.debug_print('motion', f\"Calculated motion scale: {motion_scale:.3f}\")\n",
        "        debug.debug_print('motion', f\"Calculated height scale: {height_scale:.3f}\")\n",
        "        debug.debug_print('motion', f\"Calculated vertical extent scale: {self.running_vertical_extent_scale:.3f}\")\n",
        "        debug.debug_print('motion', f\"Current running motion scale: {self.running_motion_scale}\")\n",
        "        debug.debug_print('motion', f\"Current running height scale: {self.running_height_scale}\")\n",
        "\n",
        "        return motion_scale, height_scale, self.running_vertical_extent_scale\n",
        "\n",
        "    def adjust_heights(self, heights, valid_mask, lidar_height, center_y, center_x, vertical_extent_scale):\n",
        "        \"\"\"\n",
        "        Adjusts height field using LiDAR measurement as reference.\n",
        "\n",
        "        The adjustment process:\n",
        "        1. Centers the height field on the LiDAR measurement\n",
        "        2. Scales the vertical extent to reasonable ranges\n",
        "        3. Maintains the LiDAR height at the center point\n",
        "\n",
        "        Args:\n",
        "            heights (np.ndarray): Raw height field\n",
        "            valid_mask (np.ndarray): Boolean mask of valid height values\n",
        "            lidar_height (float): LiDAR-measured height for calibration\n",
        "            center_y, center_x (int): Coordinates of center point\n",
        "            vertical_extent_scale (float): Scale for vertical extent adjustment\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Adjusted height field\n",
        "        \"\"\"\n",
        "        calculated_center_height = heights[center_y, center_x]\n",
        "        debug.debug_print('heights', f\"Initial calculated center height: {calculated_center_height:.1f} m\")\n",
        "\n",
        "        if calculated_center_height > 0:\n",
        "            # Step 1: Subtract center height\n",
        "            heights[valid_mask] -= calculated_center_height\n",
        "\n",
        "            # Step 2: Scale the cloud\n",
        "            heights[valid_mask] *= vertical_extent_scale\n",
        "\n",
        "            # Step 3: Add LiDAR height\n",
        "            heights[valid_mask] += lidar_height\n",
        "\n",
        "            debug.debug_print('heights', f\"Heights adjusted. Vertical extent scale: {vertical_extent_scale:.3f}\")\n",
        "            debug.debug_print('heights', f\"New height range: {np.ptp(heights[valid_mask]):.1f} m\")\n",
        "\n",
        "            # Verify the adjustment\n",
        "            adjusted_center_height = heights[center_y, center_x]\n",
        "            debug.debug_print('heights', f\"Adjusted center height: {adjusted_center_height:.1f} m\")\n",
        "\n",
        "            if not np.isclose(adjusted_center_height, lidar_height, rtol=1e-5):\n",
        "                debug.debug_print('heights', \"Warning: Adjusted center height does not match LiDAR height!\")\n",
        "        else:\n",
        "            debug.debug_print('heights', \"Warning: Calculated center height is zero or negative. Skipping adjustment.\")\n",
        "\n",
        "        return heights\n",
        "\n",
        "    def update_running_averages(self, new_motion_scale, new_height_scale, new_vertical_extent_scale, metadata):\n",
        "        \"\"\"\n",
        "        Updates running averages of scales using robust statistical methods.\n",
        "\n",
        "        Uses asymmetric smoothing to bias towards larger scales, which helps preserve\n",
        "        detection of high-altitude clouds. Employs MAD (Median Absolute Deviation) for\n",
        "        outlier rejection.\n",
        "\n",
        "        Args:\n",
        "            new_motion_scale (float): New calculated motion scale\n",
        "            new_height_scale (float): New calculated height scale\n",
        "            new_vertical_extent_scale (float): New calculated vertical extent scale\n",
        "            metadata (dict): Aircraft metadata for tracking conditions\n",
        "        \"\"\"\n",
        "        # Add to history\n",
        "        self.motion_scale_history.append(new_motion_scale)\n",
        "        self.height_scale_history.append(new_height_scale)\n",
        "        self.vertical_extent_scale_history.append(new_vertical_extent_scale)\n",
        "        self.timestamp_history.append(len(self.timestamp_history))\n",
        "        self.altitude_history.append(metadata['GPS_MSL_Alt'])\n",
        "        self.aircraft_speed_history.append(metadata['True_Airspeed'])\n",
        "\n",
        "        # Use robust statistics for motion scale if we have enough history\n",
        "        if len(self.motion_scale_history) >= 10:\n",
        "            # Get recent history (increased from 50 to 100 for better statistics)\n",
        "            recent_motion_scales = np.array(self.motion_scale_history[-100:])\n",
        "\n",
        "            # Calculate median and MAD\n",
        "            median_motion = np.median(recent_motion_scales)\n",
        "            mad = np.median(np.abs(recent_motion_scales - median_motion))\n",
        "\n",
        "            # More permissive threshold (increased from 3.0 to 5.0)\n",
        "            threshold = 5.0\n",
        "            inlier_mask = np.abs(recent_motion_scales - median_motion) < threshold * mad\n",
        "\n",
        "            if np.sum(inlier_mask) > 0:\n",
        "                good_scales = recent_motion_scales[inlier_mask]\n",
        "                # Use 75th percentile instead of mean to bias towards higher values\n",
        "                new_running_scale = np.percentile(good_scales, 75)\n",
        "\n",
        "                debug.debug_print('motion', f\"\\nRobust Scale Update:\")\n",
        "                debug.debug_print('motion', f\"Median motion scale: {median_motion:.3f}\")\n",
        "                debug.debug_print('motion', f\"MAD: {mad:.3f}\")\n",
        "                debug.debug_print('motion', f\"Inlier range: {np.min(good_scales):.3f} to {np.max(good_scales):.3f}\")\n",
        "                debug.debug_print('motion', f\"Number of inliers: {np.sum(inlier_mask)} out of {len(recent_motion_scales)}\")\n",
        "\n",
        "                # Much slower smoothing (reduced from 0.1 to 0.02)\n",
        "                if self.running_motion_scale is None:\n",
        "                    self.running_motion_scale = new_running_scale\n",
        "                else:\n",
        "                    # Bias towards larger scales with asymmetric smoothing\n",
        "                    alpha = 0.05 if new_running_scale > self.running_motion_scale else 0.01\n",
        "                    self.running_motion_scale = (1 - alpha) * self.running_motion_scale + alpha * new_running_scale\n",
        "        else:\n",
        "            # Not enough history yet, use simple update with bias towards larger scales\n",
        "            if self.running_motion_scale is None:\n",
        "                self.running_motion_scale = new_motion_scale\n",
        "            else:\n",
        "                alpha = 0.05 if new_motion_scale > self.running_motion_scale else 0.01\n",
        "                self.running_motion_scale = (1 - alpha) * self.running_motion_scale + alpha * new_motion_scale\n",
        "\n",
        "        # Height scale updates more simply\n",
        "        if self.running_height_scale is None:\n",
        "            self.running_height_scale = new_height_scale\n",
        "        else:\n",
        "            # Very slow update for height scale\n",
        "            alpha = 0.01\n",
        "            self.running_height_scale = (1 - alpha) * self.running_height_scale + alpha * new_height_scale\n",
        "\n",
        "        # Vertical extent scale updates\n",
        "        if self.running_vertical_extent_scale is None:\n",
        "            self.running_vertical_extent_scale = new_vertical_extent_scale\n",
        "        else:\n",
        "            # Use a slow update for vertical extent scale\n",
        "            alpha = 0.01\n",
        "            self.running_vertical_extent_scale = (1 - alpha) * self.running_vertical_extent_scale + alpha * new_vertical_extent_scale\n",
        "\n",
        "        debug.debug_print('motion', f\"Updated running motion scale: {self.running_motion_scale:.3f}\")\n",
        "        debug.debug_print('motion', f\"Updated running height scale: {self.running_height_scale:.3f}\")\n",
        "        debug.debug_print('motion', f\"Updated running vertical extent scale: {self.running_vertical_extent_scale:.3f}\")\n",
        "\n",
        "\n",
        "    def calculate_motion_confidence(self, motion_magnitude, min_motion_threshold=0.01):\n",
        "        \"\"\"\n",
        "        Calculates confidence values for motion measurements.\n",
        "\n",
        "        Combines motion magnitude and local consistency to estimate reliability\n",
        "        of motion measurements.\n",
        "\n",
        "        Args:\n",
        "            motion_magnitude (np.ndarray): Motion field magnitude\n",
        "            min_motion_threshold (float): Minimum motion for confidence calculation\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Confidence values in range [0,1]\n",
        "        \"\"\"\n",
        "        # Calculate base confidence from motion magnitude\n",
        "        confidence = 1.0 - np.exp(-motion_magnitude / min_motion_threshold)\n",
        "\n",
        "        # Look for local motion consistency\n",
        "        kernel_size = 5\n",
        "        motion_smooth = cv2.GaussianBlur(motion_magnitude, (kernel_size, kernel_size), 0)\n",
        "        local_consistency = np.abs(motion_magnitude - motion_smooth) / (motion_smooth + 1e-6)\n",
        "\n",
        "        # Combine confidences\n",
        "        confidence *= (1.0 - local_consistency)\n",
        "\n",
        "        # Normalize to [0,1]\n",
        "        confidence = np.clip(confidence, 0, 1)\n",
        "\n",
        "        return confidence\n",
        "\n",
        "    def calculate_heights(self, motion_field, metadata, lidar_height=None):\n",
        "        \"\"\"\n",
        "        Calculates cloud heights from motion field using parallax principle.\n",
        "\n",
        "        Main height calculation pipeline:\n",
        "        1. Calculate motion magnitude and confidence\n",
        "        2. Update scaling factors if LiDAR data available\n",
        "        3. Convert motion to relative heights\n",
        "        4. Apply confidence-weighted smoothing\n",
        "        5. Adjust heights using LiDAR if available\n",
        "\n",
        "        Args:\n",
        "            motion_field (np.ndarray): Motion field from optical flow\n",
        "            metadata (dict): Aircraft metadata including altitude and speed\n",
        "            lidar_height (float, optional): LiDAR measurement for calibration\n",
        "\n",
        "        Returns:\n",
        "            tuple: (heights, confidence)\n",
        "                - heights: np.ndarray of calculated cloud heights\n",
        "                - confidence: np.ndarray of confidence values\n",
        "        \"\"\"\n",
        "        MIN_MOTION_THRESHOLD = 0.1  # pixels\n",
        "\n",
        "        aircraft_altitude = float(metadata['GPS_MSL_Alt'])\n",
        "        aircraft_speed = float(metadata['True_Airspeed'])\n",
        "\n",
        "        # Calculate motion magnitude\n",
        "        dx = motion_field[0]\n",
        "        dy = motion_field[1]\n",
        "        debug.debug_print('motion', \"\\nMotion Component Debug:\")\n",
        "        debug.debug_print('motion', f\"dx range: {dx.min():.3f} to {dx.max():.3f}\")\n",
        "        debug.debug_print('motion', f\"dy range: {dy.min():.3f} to {dy.max():.3f}\")\n",
        "\n",
        "        motion_magnitude = np.sqrt(dx**2 + dy**2 + 1e-6)\n",
        "        debug.debug_print('motion', f\"motion_magnitude range: {motion_magnitude.min():.3f} to {motion_magnitude.max():.3f}\")\n",
        "\n",
        "        # Apply minimum motion threshold\n",
        "        valid_motion = motion_magnitude > MIN_MOTION_THRESHOLD\n",
        "\n",
        "        # Calculate motion confidence\n",
        "        confidence = self.calculate_motion_confidence(motion_magnitude)\n",
        "        confidence[~valid_motion] *= 0.5\n",
        "\n",
        "        # Initialize scales\n",
        "        motion_scale = self.running_motion_scale\n",
        "        height_scale = self.running_height_scale\n",
        "        vertical_extent_scale = self.running_vertical_extent_scale\n",
        "\n",
        "        # Try to calculate new scales if we have LiDAR data\n",
        "        if lidar_height is not None:\n",
        "            scales = self.calculate_scales(motion_magnitude, metadata, lidar_height)\n",
        "\n",
        "            if scales[0] is not None:  # Only update if we got valid scales\n",
        "                new_motion_scale, new_height_scale, new_vertical_extent_scale = scales\n",
        "                debug.debug_print('motion', \"\\nRunning Average Update Debug:\")\n",
        "                debug.debug_print('motion', f\"New motion scale: {new_motion_scale:.3f}\")\n",
        "                debug.debug_print('motion', f\"New height scale: {new_height_scale:.3f}\")\n",
        "                debug.debug_print('motion', f\"New vertical extent scale: {new_vertical_extent_scale:.3f}\")\n",
        "\n",
        "                # Update running averages only if center motion is valid\n",
        "                center_y, center_x = motion_magnitude.shape[0] // 2, motion_magnitude.shape[1] // 2\n",
        "                if valid_motion[center_y, center_x] and confidence[center_y, center_x] > 0.5:\n",
        "                    debug.debug_print('motion', f\"Center motion valid (>{MIN_MOTION_THRESHOLD}) and confidence high (>0.5)\")\n",
        "                    self.update_running_averages(new_motion_scale, new_height_scale, new_vertical_extent_scale, metadata)\n",
        "\n",
        "                if self.use_lidar_to_calculate_heights:\n",
        "                    motion_scale = new_motion_scale\n",
        "                    height_scale = new_height_scale\n",
        "                    vertical_extent_scale = new_vertical_extent_scale\n",
        "                else:\n",
        "                    motion_scale = self.running_motion_scale\n",
        "                    height_scale = self.running_height_scale if self.running_height_scale is not None else new_height_scale\n",
        "                    vertical_extent_scale = self.running_vertical_extent_scale\n",
        "\n",
        "        # Initialize heights array with NaN\n",
        "        heights = np.full_like(motion_magnitude, np.nan)\n",
        "\n",
        "        if motion_scale is not None and height_scale is not None and np.any(valid_motion):\n",
        "            debug.debug_print('heights', \"\\nHeight Calculation Debug:\")\n",
        "            debug.debug_print('heights', f\"Current motion_scale: {motion_scale}\")\n",
        "            debug.debug_print('heights', f\"Current height_scale: {height_scale}\")\n",
        "            debug.debug_print('heights', f\"Current vertical_extent_scale: {vertical_extent_scale}\")\n",
        "\n",
        "            relative_motion = np.zeros_like(motion_magnitude)\n",
        "            debug.debug_print('heights', f\"aircraft_speed before division: {aircraft_speed}\")\n",
        "\n",
        "            relative_motion[valid_motion] = np.abs(motion_magnitude[valid_motion] * motion_scale) / aircraft_speed\n",
        "\n",
        "            # Get center point values for relative motion analysis\n",
        "            center_y, center_x = motion_magnitude.shape[0] // 2, motion_magnitude.shape[1] // 2\n",
        "            center_relative_motion = relative_motion[center_y, center_x]\n",
        "            expected_relative_motion = lidar_height / aircraft_altitude if lidar_height is not None else None\n",
        "\n",
        "            debug.debug_print('heights', \"\\nRelative Motion Analysis in Height Calculation:\")\n",
        "            debug.debug_print('heights', f\"Center relative motion (actual): {center_relative_motion:.3f}\")\n",
        "            if expected_relative_motion is not None:\n",
        "                debug.debug_print('heights', f\"Expected relative motion: {expected_relative_motion:.3f}\")\n",
        "                debug.debug_print('heights', f\"Relative motion difference: {(expected_relative_motion - center_relative_motion):.3f}\")\n",
        "                debug.debug_print('heights', f\"Relative motion ratio: {(expected_relative_motion / (center_relative_motion + 1e-6)):.3f}\")\n",
        "\n",
        "            debug.debug_print('heights', f\"relative_motion range: {relative_motion.min():.3f} to {relative_motion.max():.3f}\")\n",
        "            debug.debug_print('heights', f\"aircraft_speed: {aircraft_speed:.3f}\")\n",
        "\n",
        "            heights[valid_motion] = aircraft_altitude * relative_motion[valid_motion] * height_scale\n",
        "            debug.debug_print('heights', f\"heights range before smoothing: {np.nanmin(heights):.3f} to {np.nanmax(heights):.3f}\")\n",
        "\n",
        "            kernel_size = 5\n",
        "            heights_smooth = cv2.GaussianBlur(heights, (kernel_size, kernel_size), 0)\n",
        "            valid_mask = ~np.isnan(heights)\n",
        "            heights[valid_mask] = confidence[valid_mask] * heights[valid_mask] + \\\n",
        "                                (1 - confidence[valid_mask]) * heights_smooth[valid_mask]\n",
        "\n",
        "            if self.use_lidar_to_calculate_heights and lidar_height is not None:\n",
        "                heights = self.adjust_heights(heights, valid_mask, lidar_height, center_y, center_x, vertical_extent_scale)\n",
        "\n",
        "        # Flip height field and confidence horizontally to match RAFT coordinate system\n",
        "        heights = heights[:, ::-1]\n",
        "        confidence = confidence[:, ::-1]\n",
        "\n",
        "\n",
        "        center_y, center_x = motion_magnitude.shape[0] // 2, motion_magnitude.shape[1] // 2\n",
        "        center_height = heights[center_y, center_x]\n",
        "        debug.debug_print('heights', f\"Final center height: {center_height:.1f} m\")\n",
        "\n",
        "        center_confidence = confidence[center_y, center_x]\n",
        "\n",
        "        debug.debug_print('heights', \"\\nFinal Height Calculation:\")\n",
        "        debug.debug_print('heights', f\"Valid height calculations: {np.sum(~np.isnan(heights)) / heights.size * 100:.1f}%\")\n",
        "        if np.any(~np.isnan(heights)):\n",
        "            debug.debug_print('heights', f\"Height range: {np.nanmin(heights):.1f} to {np.nanmax(heights):.1f} m\")\n",
        "        debug.debug_print('heights', f\"Center height: {center_height if not np.isnan(center_height) else 'NaN'}\")\n",
        "        debug.debug_print('heights', f\"Center confidence: {center_confidence:.3f}\")\n",
        "        if lidar_height is not None:\n",
        "            debug.debug_print('heights', f\"LiDAR height: {lidar_height:.1f} m\")\n",
        "            if not np.isnan(center_height):\n",
        "                debug.debug_print('heights', f\"Center height error: {abs(center_height - lidar_height):.1f} m\")\n",
        "\n",
        "        return heights, confidence\n",
        "\n",
        "    def get_parameters(self):\n",
        "        \"\"\"\n",
        "        Returns current calculator parameters.\n",
        "\n",
        "        Returns:\n",
        "            dict: Current scale values and history size\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'motion_scale': self.running_motion_scale if self.running_motion_scale is not None else 1.0,\n",
        "            'height_scale': self.running_height_scale if self.running_height_scale is not None else 1.0,\n",
        "            'vertical_extent_scale': self.running_vertical_extent_scale if self.running_vertical_extent_scale is not None else 1.0,\n",
        "            'history_size': len(self.motion_scale_history)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSVqqNUqtRfC"
      },
      "source": [
        "## plot_scale_histories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZiXx4eUiS7_"
      },
      "outputs": [],
      "source": [
        "def plot_scale_histories(model):\n",
        "    \"\"\"\n",
        "    Creates a comprehensive visualization of scale histories from both fine-tuned and reference RAFT height calculators.\n",
        "\n",
        "    This function generates a 6-panel figure showing:\n",
        "    1. Motion Scale History: Evolution of motion scaling factors over time\n",
        "    2. Height Scale History: Changes in height scaling factors\n",
        "    3. Vertical Extent Scale History: Adaptations in vertical range scaling\n",
        "    4. Fine-tuned RAFT Scales vs Altitude: How scales vary with aircraft altitude\n",
        "    5. Original RAFT Scales vs Altitude: Reference model's scale behavior with altitude\n",
        "    6. Fine-tuned RAFT Scales vs Aircraft Speed: Scale relationships with aircraft velocity\n",
        "\n",
        "    Each plot includes:\n",
        "    - Raw scale values over time or against metadata\n",
        "    - Running averages where applicable\n",
        "    - Clear labeling and legends\n",
        "    - Grid lines for easier reading\n",
        "\n",
        "    Args:\n",
        "        model (CombinedModel): Model containing both fine-tuned and reference\n",
        "            height calculators with their scale histories\n",
        "\n",
        "    Returns:\n",
        "        IPython.display.HTML: JPEG-compressed visualization embedded in notebook\n",
        "\n",
        "    Note:\n",
        "        The function automatically compresses the output to JPEG format to maintain\n",
        "        notebook performance with large history visualizations.\n",
        "    \"\"\"\n",
        "    ft_calc = model.finetuned_height_calculator\n",
        "    ref_calc = model.reference_height_calculator\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 20))  # Increased height to accommodate new subplot\n",
        "\n",
        "    # Motion Scale History\n",
        "    plt.subplot(321)\n",
        "    plt.plot(ft_calc.motion_scale_history, label='Fine-tuned RAFT', alpha=0.7)\n",
        "    plt.plot(ref_calc.motion_scale_history, label='Original RAFT', alpha=0.7)\n",
        "    if ft_calc.running_motion_scale is not None:\n",
        "        plt.axhline(y=ft_calc.running_motion_scale, color='r', linestyle='--',\n",
        "                  label=f'Fine-tuned Running Avg: {ft_calc.running_motion_scale:.3f}')\n",
        "    if ref_calc.running_motion_scale is not None:\n",
        "        plt.axhline(y=ref_calc.running_motion_scale, color='g', linestyle='--',\n",
        "                  label=f'Original Running Avg: {ref_calc.running_motion_scale:.3f}')\n",
        "    plt.title('Motion Scale History')\n",
        "    plt.xlabel('Sample')\n",
        "    plt.ylabel('Scale Factor')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Height Scale History\n",
        "    plt.subplot(322)\n",
        "    plt.plot(ft_calc.height_scale_history, label='Fine-tuned RAFT', alpha=0.7)\n",
        "    plt.plot(ref_calc.height_scale_history, label='Original RAFT', alpha=0.7)\n",
        "    if ft_calc.running_height_scale is not None:\n",
        "        plt.axhline(y=ft_calc.running_height_scale, color='r', linestyle='--',\n",
        "                  label=f'Fine-tuned Running Avg: {ft_calc.running_height_scale:.3f}')\n",
        "    if ref_calc.running_height_scale is not None:\n",
        "        plt.axhline(y=ref_calc.running_height_scale, color='g', linestyle='--',\n",
        "                  label=f'Original Running Avg: {ref_calc.running_height_scale:.3f}')\n",
        "    plt.title('Height Scale History')\n",
        "    plt.xlabel('Sample')\n",
        "    plt.ylabel('Scale Factor')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Vertical Extent Scale History\n",
        "    plt.subplot(323)\n",
        "    plt.plot(ft_calc.vertical_extent_scale_history, label='Fine-tuned RAFT', alpha=0.7)\n",
        "    plt.plot(ref_calc.vertical_extent_scale_history, label='Original RAFT', alpha=0.7)\n",
        "    if ft_calc.running_vertical_extent_scale is not None:\n",
        "        plt.axhline(y=ft_calc.running_vertical_extent_scale, color='r', linestyle='--',\n",
        "                  label=f'Fine-tuned Running Avg: {ft_calc.running_vertical_extent_scale:.3f}')\n",
        "    if ref_calc.running_vertical_extent_scale is not None:\n",
        "        plt.axhline(y=ref_calc.running_vertical_extent_scale, color='g', linestyle='--',\n",
        "                  label=f'Original Running Avg: {ref_calc.running_vertical_extent_scale:.3f}')\n",
        "    plt.title('Vertical Extent Scale History')\n",
        "    plt.xlabel('Sample')\n",
        "    plt.ylabel('Scale Factor')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Fine-tuned RAFT Scales vs Altitude\n",
        "    plt.subplot(324)\n",
        "    plt.scatter(ft_calc.altitude_history, ft_calc.motion_scale_history,\n",
        "              alpha=0.5, label='Motion Scale')\n",
        "    plt.scatter(ft_calc.altitude_history, ft_calc.height_scale_history,\n",
        "              alpha=0.5, label='Height Scale')\n",
        "    plt.scatter(ft_calc.altitude_history, ft_calc.vertical_extent_scale_history,\n",
        "              alpha=0.5, label='Vertical Extent Scale')\n",
        "    plt.xlabel('Aircraft Altitude (m)')\n",
        "    plt.ylabel('Scale Factor')\n",
        "    plt.title('Fine-tuned RAFT Scale Factors vs Altitude')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Original RAFT Scales vs Altitude\n",
        "    plt.subplot(325)\n",
        "    plt.scatter(ref_calc.altitude_history, ref_calc.motion_scale_history,\n",
        "              alpha=0.5, label='Motion Scale')\n",
        "    plt.scatter(ref_calc.altitude_history, ref_calc.height_scale_history,\n",
        "              alpha=0.5, label='Height Scale')\n",
        "    plt.scatter(ref_calc.altitude_history, ref_calc.vertical_extent_scale_history,\n",
        "              alpha=0.5, label='Vertical Extent Scale')\n",
        "    plt.xlabel('Aircraft Altitude (m)')\n",
        "    plt.ylabel('Scale Factor')\n",
        "    plt.title('Original RAFT Scale Factors vs Altitude')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Fine-tuned RAFT Scales vs Aircraft Speed\n",
        "    plt.subplot(326)\n",
        "    plt.scatter(ft_calc.aircraft_speed_history, ft_calc.motion_scale_history,\n",
        "              alpha=0.5, label='Motion Scale')\n",
        "    plt.scatter(ft_calc.aircraft_speed_history, ft_calc.height_scale_history,\n",
        "              alpha=0.5, label='Height Scale')\n",
        "    plt.scatter(ft_calc.aircraft_speed_history, ft_calc.vertical_extent_scale_history,\n",
        "              alpha=0.5, label='Vertical Extent Scale')\n",
        "    plt.xlabel('Aircraft Speed (m/s)')\n",
        "    plt.ylabel('Scale Factor')\n",
        "    plt.title('Fine-tuned RAFT Scale Factors vs Aircraft Speed')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Create JPEG and close the figure\n",
        "    jpeg_output = embed_matplotlib_jpeg(fig, dpi=72)\n",
        "    plt.close(fig)  # Close the figure explicitly\n",
        "    return jpeg_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXZmIE30tWTq"
      },
      "source": [
        "## CombinedModel Class\n",
        "\n",
        "The CombinedModel integrates the `CloudMotionModel` with the `ParallaxHeightCalculator` to create a complete cloud-height measurement pipeline.\n",
        "\n",
        "## Core Components\n",
        "\n",
        "### 1. Motion Estimation\n",
        "Employs dual RAFT instances from `CloudMotionModel`:\n",
        "- Trainable RAFT for cloud-specific motion\n",
        "- Frozen reference RAFT for baseline comparison\n",
        "- Both produce motion fields $M \\in \\mathbb{R}^{B \\times T \\times 2 \\times H \\times W}$\n",
        "\n",
        "### 2. Height Calculation\n",
        "Maintains two parallel height calculators:\n",
        "```python\n",
        "self.finetuned_height_calculator = ParallaxHeightCalculator(\n",
        "    smoothing_factor=0.1,\n",
        "    history_size=1000,\n",
        "    use_lidar_to_calculate_heights=True\n",
        ")\n",
        "self.reference_height_calculator = ParallaxHeightCalculator(...)\n",
        "```\n",
        "This allows direct comparison between fine-tuned and baseline performance.\n",
        "\n",
        "### 3. Forward Pass Pipeline\n",
        "\n",
        "1. **Motion Field Generation**:\n",
        "   - Input sequence: $I \\in \\mathbb{R}^{B \\times T \\times 3 \\times H \\times W}$\n",
        "   - Generate motion fields from both RAFT models\n",
        "   - Preserve bidirectional anomalies\n",
        "   - Calculate motion confidence\n",
        "\n",
        "2. **Height Field Generation**:\n",
        "   When metadata and LiDAR available:\n",
        "   $$\n",
        "   \\begin{align*}\n",
        "   h_{\\text{ft}} &= \\text{finetuned_calculator}(M_{\\text{ft}}, \\text{metadata}, h_{\\text{lidar}}) \\\\\n",
        "   h_{\\text{ref}} &= \\text{reference_calculator}(M_{\\text{ref}}, \\text{metadata}, h_{\\text{lidar}})\n",
        "   \\end{align*}\n",
        "   $$\n",
        "\n",
        "### 4. Output Structure\n",
        "```python\n",
        "{\n",
        "    'motion_fields': Sequence of motion fields\n",
        "    'refined_motion': Final motion with preserved anomalies\n",
        "    'reference_motion': Reference model motion\n",
        "    'height_field_finetuned': Height estimates from fine-tuned model\n",
        "    'height_uncertainty_finetuned': Uncertainty estimates\n",
        "    'height_field_reference': Reference height estimates\n",
        "    'height_uncertainty_reference': Reference uncertainty\n",
        "    'calibration_params_finetuned': Current scale parameters\n",
        "    'calibration_params_reference': Reference scale parameters\n",
        "}\n",
        "```\n",
        "\n",
        "### 5. Scale Management\n",
        "\n",
        "Both height calculators maintain independent running scales:\n",
        "- Motion scale: $s_m$ for pixel-to-meter conversion\n",
        "- Height scale: $s_h$ for relative-to-absolute height\n",
        "- Vertical extent scale: $s_v$ for cloud thickness\n",
        "\n",
        "These scales are updated and saved in checkpoints for model continuity.\n",
        "\n",
        "### 6. State Management\n",
        "```python\n",
        "def get_height_calculator_states(self):\n",
        "    \"\"\"Returns current states of both calculators.\"\"\"\n",
        "    return {\n",
        "        'finetuned': {\n",
        "            'running_motion_scale',\n",
        "            'running_height_scale',\n",
        "            'running_vertical_extent_scale',\n",
        "            'scale_histories'\n",
        "        },\n",
        "        'reference': {...}\n",
        "    }\n",
        "```\n",
        "\n",
        "### 7. Model Hierarchy\n",
        "```\n",
        "CombinedModel\n",
        "├── CloudMotionModel\n",
        "│   ├── Trainable RAFT\n",
        "│   └── Reference RAFT\n",
        "├── Fine-tuned Height Calculator\n",
        "│   └── Scale Management\n",
        "└── Reference Height Calculator\n",
        "    └── Scale Management\n",
        "```\n",
        "\n",
        "This architecture enables:\n",
        "- Direct comparison of fine-tuned vs baseline performance\n",
        "- Height estimation with uncertainty quantification\n",
        "- Scale preservation across training sessions\n",
        "- Comprehensive motion and height field visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuySG7TeiH2Z"
      },
      "outputs": [],
      "source": [
        "class CombinedModel(nn.Module):\n",
        "    \"\"\"\n",
        "    End-to-end model combining motion estimation and height calculation for cloud measurement.\n",
        "\n",
        "    This model integrates:\n",
        "    - Dual RAFT motion estimation (trainable and reference)\n",
        "    - Parallel height calculators for direct comparison\n",
        "    - Scale management and calibration\n",
        "    - Uncertainty estimation\n",
        "\n",
        "    The model processes image sequences to produce both motion fields and height estimates,\n",
        "    maintaining separate fine-tuned and reference pipelines for comparison.\n",
        "\n",
        "    Attributes:\n",
        "        raft (CloudMotionModel): Motion estimation model with dual RAFT instances\n",
        "        finetuned_height_calculator (ParallaxHeightCalculator): Calculator for fine-tuned model\n",
        "        reference_height_calculator (ParallaxHeightCalculator): Calculator for reference model\n",
        "        metadata_manager (FlightMetadataManager): Manages flight metadata processing\n",
        "\n",
        "    Args:\n",
        "        raft_model (CloudMotionModel): Initialized motion estimation model\n",
        "    \"\"\"\n",
        "    def __init__(self, raft_model):\n",
        "        super().__init__()\n",
        "        self.raft = raft_model\n",
        "        self.metadata_manager = FlightMetadataManager()\n",
        "\n",
        "        # Initialize two separate height calculators\n",
        "        self.finetuned_height_calculator = ParallaxHeightCalculator(\n",
        "            smoothing_factor=0.1,\n",
        "            history_size=1000,\n",
        "            use_lidar_to_calculate_heights=True\n",
        "        )\n",
        "        self.reference_height_calculator = ParallaxHeightCalculator(\n",
        "            smoothing_factor=0.1,\n",
        "            history_size=1000,\n",
        "            use_lidar_to_calculate_heights=True\n",
        "        )\n",
        "\n",
        "    def forward(self, images, metadata=None, validation_height=None, sequence_lengths=None):\n",
        "        \"\"\"\n",
        "        Processes image sequences through the complete height estimation pipeline.\n",
        "\n",
        "        Args:\n",
        "            images (torch.Tensor or PackedSequence): Image sequence\n",
        "                Shape: Batch x Time x 3 x Height x Width\n",
        "            metadata (torch.Tensor or PackedSequence, optional): Flight metadata\n",
        "            validation_height (torch.Tensor, optional): LiDAR measurements\n",
        "            sequence_lengths (torch.Tensor, optional): Valid sequence lengths\n",
        "\n",
        "        Returns:\n",
        "            dict: Results including:\n",
        "                - motion_fields: Raw motion sequences\n",
        "                - refined_motion: Motion with preserved anomalies\n",
        "                - reference_motion: Reference model motion\n",
        "                - height_field_finetuned: Fine-tuned height estimates\n",
        "                - height_uncertainty_finetuned: Fine-tuned uncertainty\n",
        "                - height_field_reference: Reference height estimates\n",
        "                - height_uncertainty_reference: Reference uncertainty\n",
        "                - calibration_params_finetuned: Current scale parameters\n",
        "                - calibration_params_reference: Reference parameters\n",
        "        \"\"\"\n",
        "        # Get device from packed sequence data\n",
        "        device = images.data.device\n",
        "\n",
        "        # Get motion fields from RAFT\n",
        "        raft_outputs = self.raft(images)\n",
        "\n",
        "        outputs = {\n",
        "            'motion_fields': raft_outputs['motion_fields'],\n",
        "            'refined_motion': raft_outputs['refined_motion'],\n",
        "            'reference_motion': raft_outputs['reference_motion'],\n",
        "            'confidence': raft_outputs['confidence']\n",
        "        }\n",
        "\n",
        "        # If metadata provided, calculate heights\n",
        "        if metadata is not None and sequence_lengths is not None:\n",
        "            # Unpack sequences if needed\n",
        "            if isinstance(metadata, PackedSequence):\n",
        "                metadata_unpacked, _ = pad_packed_sequence(metadata, batch_first=True)\n",
        "                batch_size = metadata_unpacked.shape[0]\n",
        "            else:\n",
        "                metadata_unpacked = metadata\n",
        "                batch_size = metadata.shape[0]\n",
        "\n",
        "            # Use sequence_lengths to get the last valid index for each sequence\n",
        "            last_valid_indices = sequence_lengths - 1\n",
        "\n",
        "            # Convert metadata and motion fields to numpy\n",
        "            aircraft_altitude = metadata_unpacked[torch.arange(batch_size), last_valid_indices, 2].cpu().numpy()\n",
        "\n",
        "            if isinstance(outputs['refined_motion'], PackedSequence):\n",
        "                final_motion_unpacked, _ = pad_packed_sequence(outputs['refined_motion'], batch_first=True)\n",
        "                reference_motion_unpacked, _ = pad_packed_sequence(outputs['reference_motion'], batch_first=True)\n",
        "                final_motion = final_motion_unpacked.detach().cpu().numpy()\n",
        "                reference_motion = reference_motion_unpacked.detach().cpu().numpy()\n",
        "            else:\n",
        "                final_motion = outputs['refined_motion'].detach().cpu().numpy()\n",
        "                reference_motion = outputs['reference_motion'].detach().cpu().numpy()\n",
        "\n",
        "            lidar_height = validation_height.cpu().numpy() if validation_height is not None else None\n",
        "\n",
        "            # Calculate heights for each sample in batch\n",
        "            batch_heights_ft = []\n",
        "            batch_uncertainties_ft = []\n",
        "            batch_heights_ref = []\n",
        "            batch_uncertainties_ref = []\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                last_valid_idx = last_valid_indices[i]\n",
        "                # Create metadata dict using manager's indices\n",
        "                metadata_dict = {\n",
        "                    'GPS_MSL_Alt': float(metadata_unpacked[i, last_valid_idx,\n",
        "                        self.metadata_manager.get_index('GPS_MSL_Alt')].cpu().numpy()),\n",
        "                    'True_Airspeed': float(metadata_unpacked[i, last_valid_idx,\n",
        "                        self.metadata_manager.get_index('True_Airspeed')].cpu().numpy()),\n",
        "                    'Track': float(metadata_unpacked[i, last_valid_idx,\n",
        "                        self.metadata_manager.get_index('Track')].cpu().numpy()),\n",
        "                    'Pitch': float(metadata_unpacked[i, last_valid_idx,\n",
        "                        self.metadata_manager.get_index('Pitch')].cpu().numpy()),\n",
        "                    'Roll': float(metadata_unpacked[i, last_valid_idx,\n",
        "                        self.metadata_manager.get_index('Roll')].cpu().numpy()),\n",
        "                    'Wind_Speed': float(metadata_unpacked[i, last_valid_idx,\n",
        "                        self.metadata_manager.get_index('Wind_Speed')].cpu().numpy()),\n",
        "                    'Wind_Dir': float(metadata_unpacked[i, last_valid_idx,\n",
        "                        self.metadata_manager.get_index('Wind_Dir')].cpu().numpy())\n",
        "                }\n",
        "\n",
        "                # Calculate heights for fine-tuned motion\n",
        "                heights_ft, uncertainty_ft = self.finetuned_height_calculator.calculate_heights(\n",
        "                    final_motion[i],\n",
        "                    metadata_dict,\n",
        "                    float(lidar_height[i]) if lidar_height is not None else None\n",
        "                )\n",
        "                batch_heights_ft.append(heights_ft)\n",
        "                batch_uncertainties_ft.append(uncertainty_ft)\n",
        "\n",
        "                # Calculate heights for reference motion\n",
        "                heights_ref, uncertainty_ref = self.reference_height_calculator.calculate_heights(\n",
        "                    reference_motion[i],\n",
        "                    metadata_dict,\n",
        "                    float(lidar_height[i]) if lidar_height is not None else None\n",
        "                )\n",
        "                batch_heights_ref.append(heights_ref)\n",
        "                batch_uncertainties_ref.append(uncertainty_ref)\n",
        "\n",
        "            # Convert to tensors and store outputs\n",
        "            outputs['height_field_finetuned'] = torch.from_numpy(np.stack(batch_heights_ft)).to(device)\n",
        "            outputs['height_uncertainty_finetuned'] = torch.from_numpy(np.stack(batch_uncertainties_ft)).to(device)\n",
        "            outputs['height_field_reference'] = torch.from_numpy(np.stack(batch_heights_ref)).to(device)\n",
        "            outputs['height_uncertainty_reference'] = torch.from_numpy(np.stack(batch_uncertainties_ref)).to(device)\n",
        "\n",
        "            # Store calibration parameters and histories for both calculators\n",
        "            outputs['calibration_params_finetuned'] = self.finetuned_height_calculator.get_parameters()\n",
        "            outputs['calibration_params_reference'] = self.reference_height_calculator.get_parameters()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def get_height_calculator_states(self):\n",
        "        \"\"\"\n",
        "        Returns current states of both height calculators.\n",
        "\n",
        "        Used for checkpoint saving and scale preservation across training sessions.\n",
        "\n",
        "        Returns:\n",
        "            dict: State dictionaries for both calculators including:\n",
        "                - Running scales (motion, height, vertical extent)\n",
        "                - Scale histories\n",
        "                - Metadata histories (timestamps, altitudes, speeds)\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'finetuned': {\n",
        "                'running_motion_scale': self.finetuned_height_calculator.running_motion_scale,\n",
        "                'running_height_scale': self.finetuned_height_calculator.running_height_scale,\n",
        "                'running_vertical_extent_scale': self.finetuned_height_calculator.running_vertical_extent_scale,\n",
        "                'motion_scale_history': self.finetuned_height_calculator.motion_scale_history,\n",
        "                'height_scale_history': self.finetuned_height_calculator.height_scale_history,\n",
        "                'vertical_extent_scale_history': self.finetuned_height_calculator.vertical_extent_scale_history,\n",
        "                'timestamp_history': self.finetuned_height_calculator.timestamp_history,\n",
        "                'altitude_history': self.finetuned_height_calculator.altitude_history,\n",
        "                'aircraft_speed_history': self.finetuned_height_calculator.aircraft_speed_history\n",
        "            },\n",
        "            'reference': {\n",
        "                'running_motion_scale': self.reference_height_calculator.running_motion_scale,\n",
        "                'running_height_scale': self.reference_height_calculator.running_height_scale,\n",
        "                'running_vertical_extent_scale': self.reference_height_calculator.running_vertical_extent_scale,\n",
        "                'motion_scale_history': self.reference_height_calculator.motion_scale_history,\n",
        "                'height_scale_history': self.reference_height_calculator.height_scale_history,\n",
        "                'vertical_extent_scale_history': self.reference_height_calculator.vertical_extent_scale_history,\n",
        "                'timestamp_history': self.reference_height_calculator.timestamp_history,\n",
        "                'altitude_history': self.reference_height_calculator.altitude_history,\n",
        "                'aircraft_speed_history': self.reference_height_calculator.aircraft_speed_history\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def load_height_calculator_states(self, states):\n",
        "        \"\"\"\n",
        "        Loads saved states into both height calculators.\n",
        "\n",
        "        Used when resuming training or inference to maintain scale calibration.\n",
        "\n",
        "        Args:\n",
        "            states (dict): State dictionaries for both calculators containing:\n",
        "                - Running scales\n",
        "                - Scale histories\n",
        "                - Metadata histories\n",
        "        \"\"\"\n",
        "        if 'finetuned' in states:\n",
        "            self.finetuned_height_calculator.running_motion_scale = states['finetuned']['running_motion_scale']\n",
        "            self.finetuned_height_calculator.running_height_scale = states['finetuned']['running_height_scale']\n",
        "            self.finetuned_height_calculator.running_vertical_extent_scale = states['finetuned']['running_vertical_extent_scale']\n",
        "            self.finetuned_height_calculator.motion_scale_history = states['finetuned']['motion_scale_history']\n",
        "            self.finetuned_height_calculator.height_scale_history = states['finetuned']['height_scale_history']\n",
        "            self.finetuned_height_calculator.vertical_extent_scale_history = states['finetuned']['vertical_extent_scale_history']\n",
        "            self.finetuned_height_calculator.timestamp_history = states['finetuned']['timestamp_history']\n",
        "            self.finetuned_height_calculator.altitude_history = states['finetuned']['altitude_history']\n",
        "            self.finetuned_height_calculator.aircraft_speed_history = states['finetuned']['aircraft_speed_history']\n",
        "\n",
        "        if 'reference' in states:\n",
        "            self.reference_height_calculator.running_motion_scale = states['reference']['running_motion_scale']\n",
        "            self.reference_height_calculator.running_height_scale = states['reference']['running_height_scale']\n",
        "            self.reference_height_calculator.running_vertical_extent_scale = states['reference']['running_vertical_extent_scale']\n",
        "            self.reference_height_calculator.motion_scale_history = states['reference']['motion_scale_history']\n",
        "            self.reference_height_calculator.height_scale_history = states['reference']['height_scale_history']\n",
        "            self.reference_height_calculator.vertical_extent_scale_history = states['reference']['vertical_extent_scale_history']\n",
        "            self.reference_height_calculator.timestamp_history = states['reference']['timestamp_history']\n",
        "            self.reference_height_calculator.altitude_history = states['reference']['altitude_history']\n",
        "            self.reference_height_calculator.aircraft_speed_history = states['reference']['aircraft_speed_history']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SequenceConsistencyLoss Class\n",
        "\n",
        "`SequenceConsistencyLoss` enforces temporal coherence and motion consistency in cloud height estimation through three complementary loss components:\n",
        "\n",
        "## Photometric Loss\n",
        "\n",
        "Photometric loss ensures accurate frame reconstruction by comparing how well the predicted motion fields can reconstruct subsequent frames. This enforces temporal consistency in the motion estimates.\n",
        "\n",
        "$$L_{photo} = \\sum_{t=1}^{T-1} \\|I_{t+1} - \\text{warp}(I_t, F_t)\\| \\cdot V_t$$\n",
        "\n",
        "where:\n",
        "- $I_t$ is frame t in the sequence\n",
        "- $F_t$ is the estimated motion field\n",
        "- $V_t$ is a visibility mask\n",
        "- `warp()` applies the motion field to warp frame t\n",
        "\n",
        "**Why this is important for cloud motion:**\n",
        "Clouds undergo continuous, gradual changes in appearance and shape. At high altitudes, cloud features must maintain temporal consistency to enable accurate tracking. The visibility mask component is particularly crucial as it allows the model to handle complex scenarios where clouds at different heights overlap or occlude each other. This loss helps distinguish between actual cloud movement and changes caused by varying illumination conditions or aircraft motion, which is essential for accurate height estimation.\n",
        "\n",
        "## Smoothness Loss\n",
        "\n",
        "Smoothness loss penalizes rapid spatial variations in the motion field, promoting locally coherent motion predictions. This helps avoid spurious motion estimates and encourages physically plausible cloud motion patterns.\n",
        "\n",
        "$$L_{smooth} = \\sum_{t=1}^{T-1} \\|\\nabla F_t\\|$$\n",
        "\n",
        "where $\\nabla F_t$ represents spatial gradients of the motion field\n",
        "\n",
        "**Why this is important for cloud motion:**\n",
        "Cloud formations typically exhibit fluid-like behavior with coherent motion patterns. At high altitudes, where clouds tend to move more uniformly, this loss term ensures the estimated motion fields reflect the natural, continuous flow of cloud systems. This is especially important for distinguishing between different cloud layers and preventing physically impossible motion discontinuities that could lead to errors in height estimation.\n",
        "\n",
        "## Flow Consistency Loss\n",
        "\n",
        "Flow consistency loss ensures bidirectional consistency between forward and backward motion estimates. This helps identify and penalize invalid motion predictions that are not consistent when applied in both temporal directions.\n",
        "\n",
        "$$L_{consist} = \\sum_{t=1}^{T-1} \\|F_t + \\text{warp}(F'_t, F_t)\\|$$\n",
        "\n",
        "where $F'_t$ is the backward flow\n",
        "\n",
        "**Why this is important for cloud motion:**\n",
        "True cloud movement should be temporally reversible over short time scales. This loss helps differentiate between actual cloud motion and apparent motion caused by changes in cloud shape or illumination. When dealing with high-altitude clouds viewed from an aircraft, consistency checking becomes particularly important as clouds may appear to move differently depending on the viewing angle and aircraft position. This bidirectional verification helps ensure reliable height estimates across varying viewing conditions.\n",
        "\n",
        "## Combined Loss\n",
        "\n",
        "The final loss combines these components with configurable weights:\n",
        "\n",
        "$$L_{total} = L_{photo} + \\lambda_{smooth}L_{smooth} + \\lambda_{consist}L_{consist}$$\n",
        "\n",
        "where $\\lambda_{smooth}$ and $\\lambda_{consist}$ are weighting factors.\n",
        "\n",
        "## Implementation Features\n",
        "\n",
        "- Uses PyTorch's packed sequence functionality to efficiently handle variable-length sequences\n",
        "- Implements visibility masking to exclude occluded regions from loss computation\n",
        "- Provides bidirectional consistency checking through forward and backward flow comparison\n",
        "- Applies spatial regularization through smoothness constraints\n",
        "- Handles batch processing for efficient training\n",
        "\n",
        "The combined loss terms guide the model to learn motion fields that:\n",
        "1. Accurately reconstruct subsequent frames\n",
        "2. Maintain spatial smoothness\n",
        "3. Exhibit bidirectional consistency\n",
        "4. Account for occlusions through visibility masking"
      ],
      "metadata": {
        "id": "ahCuAFO4z2CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceConsistencyLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    A multi-component loss function for training cloud motion estimation models.\n",
        "\n",
        "    This loss combines photometric reconstruction, motion smoothness, and flow\n",
        "    consistency terms to ensure physically valid cloud motion estimation. The class\n",
        "    handles variable-length sequences using PyTorch's packed sequence functionality.\n",
        "\n",
        "    The total loss is a weighted sum of three components:\n",
        "        1. Photometric Loss: Ensures accurate frame reconstruction\n",
        "        2. Smoothness Loss: Promotes coherent motion fields\n",
        "        3. Flow Consistency Loss: Enforces bidirectional consistency\n",
        "\n",
        "    Args:\n",
        "        smoothness_weight (float, optional): Weight for smoothness loss term. Default: 0.1\n",
        "        consistency_weight (float, optional): Weight for flow consistency term. Default: 0.5\n",
        "    \"\"\"\n",
        "    def __init__(self, smoothness_weight=0.1, consistency_weight=0.5):\n",
        "        super().__init__()\n",
        "        self.smoothness_weight = smoothness_weight\n",
        "        self.consistency_weight = consistency_weight\n",
        "\n",
        "    def forward_warp(self, image, flow):\n",
        "        \"\"\"\n",
        "        Warps an image according to the given flow field.\n",
        "\n",
        "        Args:\n",
        "            image (torch.Tensor or PackedSequence): Image or sequence of images to warp\n",
        "                Shape: [B, T, C, H, W] if tensor\n",
        "            flow (torch.Tensor or PackedSequence): Flow fields to apply\n",
        "                Shape: [B, T, 2, H, W] if tensor\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor or PackedSequence: Warped images with same structure as input\n",
        "        \"\"\"\n",
        "        if isinstance(image, PackedSequence):\n",
        "            image_data, image_batch_sizes = pad_packed_sequence(image, batch_first=True)\n",
        "        else:\n",
        "            image_data = image\n",
        "\n",
        "        if isinstance(flow, PackedSequence):\n",
        "            flow_data, flow_batch_sizes = pad_packed_sequence(flow, batch_first=True)\n",
        "        else:\n",
        "            flow_data = flow\n",
        "\n",
        "        B, T, C, H, W = image_data.shape\n",
        "        _, T_flow, _, _, _ = flow_data.shape\n",
        "\n",
        "        grid_y, grid_x = torch.meshgrid(\n",
        "            torch.arange(H, device=image_data.device),\n",
        "            torch.arange(W, device=image_data.device),\n",
        "            indexing='ij'\n",
        "        )\n",
        "\n",
        "        grid = torch.stack((grid_x, grid_y)).to(image_data.device).float()\n",
        "        grid = grid.unsqueeze(0).unsqueeze(0).expand(B, T_flow, -1, H, W)\n",
        "\n",
        "        flow_grid = grid + flow_data\n",
        "        flow_grid = flow_grid.permute(0, 1, 3, 4, 2)\n",
        "\n",
        "        # Normalize flow grid to [-1, 1] range\n",
        "        flow_grid[..., 0] = 2.0 * flow_grid[..., 0] / (W - 1) - 1.0\n",
        "        flow_grid[..., 1] = 2.0 * flow_grid[..., 1] / (H - 1) - 1.0\n",
        "\n",
        "        # Reshape for grid_sample\n",
        "        flow_grid = flow_grid.view(B * T_flow, H, W, 2)\n",
        "        image_to_warp = image_data[:, :T_flow].contiguous().view(B * T_flow, C, H, W)\n",
        "\n",
        "        warped = F.grid_sample(\n",
        "            image_to_warp,\n",
        "            flow_grid,\n",
        "            mode='bilinear',\n",
        "            padding_mode='border',\n",
        "            align_corners=True\n",
        "        )\n",
        "\n",
        "        # Reshape back to original dimensions\n",
        "        warped = warped.view(B, T_flow, C, H, W)\n",
        "\n",
        "        # Repack if input was packed\n",
        "        if isinstance(image, PackedSequence):\n",
        "            warped = pack_padded_sequence(warped, flow_batch_sizes, batch_first=True)\n",
        "\n",
        "        return warped\n",
        "\n",
        "    def compute_visibility_mask(self, flow_forward, flow_backward):\n",
        "        \"\"\"\n",
        "        Computes visibility masks to handle occlusions in the motion field.\n",
        "\n",
        "        Args:\n",
        "            flow_forward (torch.Tensor or PackedSequence): Forward flow fields\n",
        "                Shape: [B, T, 2, H, W] if tensor\n",
        "            flow_backward (torch.Tensor or PackedSequence): Backward flow fields\n",
        "                Shape: [B, T, 2, H, W] if tensor\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor or PackedSequence: Binary visibility mask\n",
        "                Shape: [B, T, 1, H, W] if tensor\n",
        "        \"\"\"\n",
        "        forward_warped = self.forward_warp(flow_backward, flow_forward)\n",
        "        backward_warped = self.forward_warp(flow_forward, flow_backward)\n",
        "\n",
        "        # Unpack if needed\n",
        "        if isinstance(forward_warped, PackedSequence):\n",
        "            forward_warped, batch_sizes = pad_packed_sequence(forward_warped, batch_first=True)\n",
        "            flow_backward, _ = pad_packed_sequence(flow_backward, batch_first=True)\n",
        "            backward_warped, _ = pad_packed_sequence(backward_warped, batch_first=True)\n",
        "            flow_forward, _ = pad_packed_sequence(flow_forward, batch_first=True)\n",
        "\n",
        "        forward_diff = torch.norm(forward_warped - flow_backward, dim=2, keepdim=True)\n",
        "        backward_diff = torch.norm(backward_warped - flow_forward, dim=2, keepdim=True)\n",
        "\n",
        "        forward_mask = forward_diff < 1.0\n",
        "        backward_mask = backward_diff < 1.0\n",
        "\n",
        "        return forward_mask & backward_mask\n",
        "\n",
        "    def compute_consistency_loss(self, flow_forward, flow_backward):\n",
        "        \"\"\"\n",
        "        Computes bidirectional consistency loss between forward and backward flows.\n",
        "\n",
        "        Args:\n",
        "            flow_forward (torch.Tensor or PackedSequence): Forward flow fields\n",
        "                Shape: [B, T, 2, H, W] if tensor\n",
        "            flow_backward (torch.Tensor or PackedSequence): Backward flow fields\n",
        "                Shape: [B, T, 2, H, W] if tensor\n",
        "\n",
        "        Returns:\n",
        "            tuple: (consistency_mask, consistency_error)\n",
        "                - consistency_mask (torch.Tensor): Per-pixel consistency weights\n",
        "                - consistency_error (torch.Tensor): Mean consistency error\n",
        "        \"\"\"\n",
        "        # Assuming flow_forward and flow_backward have shape (B, T, 2, H, W)\n",
        "\n",
        "        # Warp forward flow using backward flow\n",
        "        warped_forward = self.forward_warp(flow_forward, flow_backward)\n",
        "\n",
        "        # Warp backward flow using forward flow\n",
        "        warped_backward = self.forward_warp(flow_backward, flow_forward)\n",
        "\n",
        "        # Unpack if needed\n",
        "        if isinstance(warped_forward, PackedSequence):\n",
        "            warped_forward, batch_sizes = pad_packed_sequence(warped_forward, batch_first=True)\n",
        "            warped_backward, _ = pad_packed_sequence(warped_backward, batch_first=True)\n",
        "            flow_forward, _ = pad_packed_sequence(flow_forward, batch_first=True)\n",
        "            flow_backward, _ = pad_packed_sequence(flow_backward, batch_first=True)\n",
        "\n",
        "        # Compute consistency error\n",
        "        forward_consistency_error = torch.norm(warped_forward + flow_backward, dim=2)\n",
        "        backward_consistency_error = torch.norm(warped_backward + flow_forward, dim=2)\n",
        "\n",
        "        # Combine errors\n",
        "        consistency_error = (forward_consistency_error + backward_consistency_error) / 2\n",
        "\n",
        "        # Create mask based on consistency\n",
        "        consistency_mask = torch.exp(-consistency_error / 0.1)\n",
        "\n",
        "        return consistency_mask, consistency_error.mean()\n",
        "\n",
        "\n",
        "    def forward(self, flows, image_sequence, sequence_lengths):\n",
        "        \"\"\"\n",
        "        Computes the combined loss for a batch of sequences.\n",
        "\n",
        "        Args:\n",
        "            flows (PackedSequence): Predicted motion fields\n",
        "                Data shape: [B, T-1, 2, H, W] when unpacked\n",
        "            image_sequence (PackedSequence): Input image sequences\n",
        "                Data shape: [B, T, C, H, W] when unpacked\n",
        "            sequence_lengths (torch.Tensor): Length of each sequence in batch\n",
        "                Shape: [B]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Scalar loss value combining photometric, smoothness,\n",
        "                and consistency terms according to their weights\n",
        "\n",
        "        Note:\n",
        "            Sequences in the batch must be sorted by length in descending order\n",
        "            for proper packed sequence handling.\n",
        "        \"\"\"\n",
        "        # Unpack sequences\n",
        "        flows_unpacked, flows_lengths = pad_packed_sequence(flows, batch_first=True)\n",
        "        images_unpacked, _ = pad_packed_sequence(image_sequence, batch_first=True)\n",
        "\n",
        "        total_loss = 0\n",
        "        valid_count = 0\n",
        "\n",
        "        # Process each sequence based on its actual length\n",
        "        for i, length in enumerate(flows_lengths):\n",
        "            # Get sequence data\n",
        "            seq_flows = flows_unpacked[i:i+1, :length]\n",
        "            seq_images = images_unpacked[i:i+1, :length+1]  # +1 because we need one more image than flows\n",
        "\n",
        "            # Photometric loss\n",
        "            warped_images = self.forward_warp(seq_images[:, :-1], seq_flows)\n",
        "            visibility_mask = self.compute_visibility_mask(seq_flows, seq_flows)\n",
        "            photo_loss = (torch.abs(warped_images - seq_images[:, 1:]) * visibility_mask).mean()\n",
        "\n",
        "            # Smoothness loss\n",
        "            smoothness_loss = (\n",
        "                torch.abs(seq_flows[:, :, :, 1:, :] - seq_flows[:, :, :, :-1, :]).mean() +\n",
        "                torch.abs(seq_flows[:, :, :, :, 1:] - seq_flows[:, :, :, :, :-1]).mean()\n",
        "            )\n",
        "\n",
        "            # Flow consistency loss\n",
        "            consistency_mask, consistency_loss = self.compute_consistency_loss(seq_flows, -seq_flows)\n",
        "            photo_loss = (photo_loss * consistency_mask).mean()\n",
        "\n",
        "            # Combine losses\n",
        "            sequence_loss = (photo_loss +\n",
        "                           self.smoothness_weight * smoothness_loss +\n",
        "                           self.consistency_weight * consistency_loss)\n",
        "\n",
        "            if torch.isfinite(sequence_loss):\n",
        "                total_loss += sequence_loss\n",
        "                valid_count += 1\n",
        "\n",
        "        return total_loss / max(valid_count, 1)"
      ],
      "metadata": {
        "id": "OofR_Hwdz1yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRbn4HQBs9qr"
      },
      "source": [
        "## CombinedLoss Architecture\n",
        "\n",
        "The `CombinedLoss` combines motion consistency with LiDAR supervision to enable accurate height field estimation:\n",
        "\n",
        "### Motion Loss Component\n",
        "Uses `SequenceConsistencyLoss` to ensure temporal coherence in predicted motion fields:\n",
        "\n",
        "$$L_{motion} = L_{sequence}(F_{pred}, I, l)$$\n",
        "\n",
        "where:\n",
        "- $F_{pred}$ are predicted motion fields\n",
        "- $I$ are input image sequences\n",
        "- $l$ are sequence lengths\n",
        "\n",
        "### LiDAR Supervision Component\n",
        "\n",
        "While the model can predict motion patterns, converting to absolute heights requires calibration. A single LiDAR measurement at the image center provides this reference point.\n",
        "\n",
        "1. Calculate motion at image center:\n",
        "   $$m_{center} = \\|F_{pred}(x_c, y_c)\\|$$\n",
        "\n",
        "2. Convert to relative motion:\n",
        "   $$r_{actual} = \\frac{m_{center}}{v_{aircraft}}$$\n",
        "\n",
        "3. Calculate expected relative motion:\n",
        "   $$r_{expected} = \\frac{h_{lidar}}{h_{aircraft}}$$\n",
        "\n",
        "4. Determine scaling factor:\n",
        "   $$s = \\frac{r_{expected}}{r_{actual}}$$\n",
        "\n",
        "5. Apply to motion field:\n",
        "   $$F_{scaled} = F_{pred} \\cdot s$$\n",
        "\n",
        "The loss is:\n",
        "$$L_{lidar} = L_{smooth\\_l1}(F_{scaled}, F_{pred})$$\n",
        "\n",
        "### Total Loss\n",
        "$$L_{total} = L_{motion} + w_{lidar} \\cdot L_{lidar} \\cdot \\alpha$$\n",
        "\n",
        "where:\n",
        "- $w_{lidar}$ is the LiDAR weight (0.01)\n",
        "- $\\alpha = \\text{clamp}(\\frac{L_{motion}}{L_{lidar}}, 0.01, 10)$ dynamically scales the LiDAR loss\n",
        "\n",
        "### Height Error Monitoring\n",
        "\n",
        "Tracks error between predicted and LiDAR heights:\n",
        "\n",
        "$$E_{height} = \\frac{1}{N}\\sum_{i=1}^N |h_i - h_{ref,i}|$$\n",
        "\n",
        "For valid predictions where:\n",
        "- $h_i$ are predicted center heights\n",
        "- $h_{ref,i}$ are LiDAR measurements\n",
        "- $N$ is the number of valid samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzGj_Quhcs4c"
      },
      "outputs": [],
      "source": [
        "class CombinedLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Loss function combining motion consistency with LiDAR-guided motion field scaling.\n",
        "\n",
        "    Uses a single LiDAR height measurement to calculate expected motion at image center\n",
        "    and scale the entire motion field accordingly. This enables accurate absolute height\n",
        "    estimation from relative motion measurements.\n",
        "\n",
        "    Args:\n",
        "        smoothness_weight (float): Weight for motion smoothness term. Default: 0.01\n",
        "        lidar_weight (float): Weight for LiDAR supervision term. Default: 0.01\n",
        "    \"\"\"\n",
        "    def __init__(self, smoothness_weight=0.01, lidar_weight=0.01):\n",
        "        super().__init__()\n",
        "        self.smoothness_weight = smoothness_weight\n",
        "        self.lidar_weight = lidar_weight\n",
        "        self.consistency_loss = SequenceConsistencyLoss(smoothness_weight=smoothness_weight)\n",
        "        self.metadata_manager = FlightMetadataManager()\n",
        "\n",
        "    def calculate_expected_relative_motion(self, lidar_height, metadata):\n",
        "        \"\"\"\n",
        "        Calculate the expected relative motion at LiDAR measurement point.\n",
        "\n",
        "        For cloud height estimation, relative motion should be proportional to height:\n",
        "        relative_motion = cloud_height / aircraft_height\n",
        "\n",
        "        Args:\n",
        "            lidar_height (float): Cloud height measured by LiDAR\n",
        "            metadata (dict): Flight metadata containing aircraft altitude\n",
        "\n",
        "        Returns:\n",
        "            float: Expected relative motion based on height ratio\n",
        "        \"\"\"\n",
        "        aircraft_altitude = float(metadata['GPS_MSL_Alt'])\n",
        "        debug.debug_print('finetune', f\"Debug: lidar_height = {lidar_height}, aircraft_altitude = {aircraft_altitude}\")\n",
        "        expected_relative_motion = lidar_height / (aircraft_altitude + 1e-8)\n",
        "        debug.debug_print('finetune', f\"Debug: expected_relative_motion = {expected_relative_motion}\")\n",
        "        return expected_relative_motion\n",
        "\n",
        "    def forward(self, outputs, images, sequence_lengths, validation_height=None, metadata=None):\n",
        "        \"\"\"\n",
        "        Compute combined loss incorporating motion consistency and LiDAR supervision.\n",
        "\n",
        "        The loss has two main components:\n",
        "        1. Motion consistency loss using SequenceConsistencyLoss\n",
        "        2. LiDAR supervision loss that:\n",
        "            - Calculates expected motion from LiDAR height\n",
        "            - Measures actual motion at image center\n",
        "            - Scales motion field to match expected values\n",
        "            - Computes smooth L1 loss between scaled and original fields\n",
        "\n",
        "        Args:\n",
        "            outputs (dict): Model outputs containing motion fields\n",
        "            images (PackedSequence): Input image sequences\n",
        "            sequence_lengths (torch.Tensor): Length of each sequence\n",
        "            validation_height (torch.Tensor, optional): LiDAR height measurements\n",
        "            metadata (PackedSequence, optional): Flight metadata per frame\n",
        "\n",
        "        Returns:\n",
        "            tuple: (total_loss, loss_dict)\n",
        "                - total_loss: Combined weighted loss\n",
        "                - loss_dict: Individual loss components\n",
        "        \"\"\"\n",
        "        device = images.data.device\n",
        "        motion_loss = torch.tensor(0.0, device=device)\n",
        "        lidar_loss = torch.tensor(0.0, device=device)\n",
        "        height_error = torch.tensor(0.0, device=device)\n",
        "        total_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "        # Motion consistency loss\n",
        "        motion_loss = self.consistency_loss(\n",
        "            outputs['motion_fields'],\n",
        "            images,\n",
        "            sequence_lengths\n",
        "        )\n",
        "        debug.debug_print('finetune', f\"Debug: motion_loss = {motion_loss}\")\n",
        "\n",
        "        total_loss = motion_loss\n",
        "        loss_dict = {'motion_loss': motion_loss}\n",
        "\n",
        "        # Add LiDAR supervision if available\n",
        "        if validation_height is not None and metadata is not None:\n",
        "            motion_fields_unpacked, _ = pad_packed_sequence(outputs['motion_fields'], batch_first=True)\n",
        "            metadata_unpacked, _ = pad_packed_sequence(metadata, batch_first=True)\n",
        "\n",
        "            lidar_loss = torch.tensor(0.0, device=device)\n",
        "            valid_samples = 0\n",
        "\n",
        "            for i, seq_len in enumerate(sequence_lengths):\n",
        "                debug.debug_print('finetune', f\"\\nDebug: Processing batch item {i}\")\n",
        "                flow_len = seq_len - 1  # number of flow fields is one less than sequence length\n",
        "\n",
        "                if flow_len < 1:\n",
        "                    debug.debug_print('finetune', f\"Warning: Sequence {i} is too short (length {seq_len})\")\n",
        "                    continue\n",
        "\n",
        "                # Use the last valid flow field\n",
        "                motion_field = motion_fields_unpacked[i, flow_len - 1]\n",
        "                h, w = motion_field.shape[-2:]\n",
        "                center_y, center_x = h // 2, w // 2\n",
        "\n",
        "                # Calculate actual motion magnitude at center\n",
        "                center_motion = torch.norm(motion_field[:, center_y, center_x])\n",
        "                debug.debug_print('finetune', f\"Debug: center_motion = {center_motion}\")\n",
        "\n",
        "                # Convert center motion to relative motion using aircraft speed\n",
        "                aircraft_speed = metadata_unpacked[i, seq_len - 1, 7]  # True_Airspeed\n",
        "                debug.debug_print('finetune', f\"Debug: aircraft_speed = {aircraft_speed}\")\n",
        "\n",
        "                if torch.isnan(aircraft_speed) or aircraft_speed <= 0:\n",
        "                    debug.debug_print('finetune', f\"Warning: Invalid aircraft_speed for batch item {i}\")\n",
        "                    continue\n",
        "\n",
        "                center_relative_motion = center_motion / (aircraft_speed + 1e-8)\n",
        "                debug.debug_print('finetune', f\"Debug: center_relative_motion = {center_relative_motion}\")\n",
        "\n",
        "                # Calculate expected relative motion using metadata manager\n",
        "                expected_relative_motion = self.calculate_expected_relative_motion(\n",
        "                    validation_height[i],\n",
        "                    {'GPS_MSL_Alt': metadata_unpacked[i, seq_len - 1,\n",
        "                        self.metadata_manager.get_index('GPS_MSL_Alt')].item()}\n",
        "                )\n",
        "\n",
        "                if torch.isnan(expected_relative_motion):\n",
        "                    debug.debug_print('finetune', f\"Warning: Invalid expected_relative_motion for batch item {i}\")\n",
        "                    continue\n",
        "\n",
        "                # Convert to tensor and match device\n",
        "                expected_relative_motion = torch.full_like(center_relative_motion, expected_relative_motion)\n",
        "                debug.debug_print('finetune', f\"Debug: expected_relative_motion tensor = {expected_relative_motion}\")\n",
        "\n",
        "                # Calculate scaling factor\n",
        "                scale_factor = expected_relative_motion / (center_relative_motion + 1e-8)\n",
        "                debug.debug_print('finetune', f\"Debug: scale_factor = {scale_factor}\")\n",
        "\n",
        "                # Scale entire motion field and calculate loss\n",
        "                scaled_motion = motion_field * scale_factor.view(-1, 1, 1)\n",
        "                debug.debug_print('finetune', f\"Debug: scaled_motion shape = {scaled_motion.shape}, min = {scaled_motion.min()}, max = {scaled_motion.max()}\")\n",
        "                debug.debug_print('finetune', f\"Debug: motion_field shape = {motion_field.shape}, min = {motion_field.min()}, max = {motion_field.max()}\")\n",
        "\n",
        "                batch_lidar_loss = F.smooth_l1_loss(scaled_motion, motion_field)\n",
        "                debug.debug_print('finetune', f\"Debug: batch_lidar_loss = {batch_lidar_loss}\")\n",
        "\n",
        "                if torch.isfinite(batch_lidar_loss):\n",
        "                    lidar_loss += batch_lidar_loss\n",
        "                    valid_samples += 1\n",
        "\n",
        "            if valid_samples > 0:\n",
        "                lidar_loss = lidar_loss / valid_samples\n",
        "                debug.debug_print('finetune', f\"Debug: average lidar_loss = {lidar_loss}\")\n",
        "                scaled_lidar_loss = lidar_loss * torch.clamp(motion_loss.detach() / (lidar_loss + 1e-8), min=0.01, max=10)\n",
        "                total_loss = total_loss + self.lidar_weight * scaled_lidar_loss\n",
        "                loss_dict['lidar_loss'] = lidar_loss\n",
        "                loss_dict['scaled_lidar_loss'] = scaled_lidar_loss\n",
        "            else:\n",
        "                debug.debug_print('finetune', \"Warning: No valid samples for lidar loss calculation\")\n",
        "\n",
        "        # Height error monitoring\n",
        "        if validation_height is not None and 'height_field_finetuned' in outputs:\n",
        "            with torch.no_grad():\n",
        "                height_error = self._calculate_height_error(\n",
        "                    outputs['height_field_finetuned'],\n",
        "                    validation_height\n",
        "                )\n",
        "                debug.debug_print('finetune', f\"Debug: height_error = {height_error}\")\n",
        "\n",
        "        loss_dict['height_error'] = height_error\n",
        "        loss_dict['total_loss'] = total_loss\n",
        "\n",
        "        debug.debug_print('finetune', f\"Debug: total_loss = {total_loss}\")\n",
        "        debug.debug_print('finetune', f\"Debug: loss_dict = {loss_dict}\")\n",
        "\n",
        "        # Final check for NaNs\n",
        "        for key, value in loss_dict.items():\n",
        "            if torch.isnan(value) or torch.isinf(value):\n",
        "                debug.debug_print('finetune', f\"Warning: {key} is NaN or Inf\")\n",
        "\n",
        "        return total_loss, loss_dict\n",
        "\n",
        "    def _calculate_height_error(self, height_field, validation_height):\n",
        "        \"\"\"\n",
        "        Calculate mean absolute error between predicted and LiDAR heights at center point.\n",
        "\n",
        "        Args:\n",
        "            height_field (torch.Tensor): Predicted height field, shape [B, H, W]\n",
        "            validation_height (torch.Tensor): LiDAR measurements, shape [B]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Mean absolute height error for valid samples\n",
        "        \"\"\"\n",
        "        B, H, W = height_field.shape\n",
        "        center_y, center_x = H // 2, W // 2\n",
        "        center_heights = height_field[:, center_y, center_x]\n",
        "\n",
        "        if center_heights.shape != validation_height.shape:\n",
        "            center_heights = center_heights.view(-1)\n",
        "            validation_height = validation_height.view(-1)\n",
        "\n",
        "        valid_mask = torch.isfinite(center_heights) & torch.isfinite(validation_height)\n",
        "        if valid_mask.sum() > 0:\n",
        "            height_errors = torch.abs(center_heights[valid_mask] - validation_height[valid_mask])\n",
        "            # Print individual height errors\n",
        "            debug.debug_print('finetune', \"Individual height errors:\")\n",
        "            for j, error in enumerate(height_errors):\n",
        "                debug.debug_print('finetune', f\"  Sample {j}: {error.item():.2f} m\")\n",
        "            return height_errors.mean()\n",
        "\n",
        "        debug.debug_print('finetune', \"Warning: No valid samples for height error calculation\")\n",
        "        return torch.tensor(0.0, device=height_field.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S93PsUEjZB6q"
      },
      "outputs": [],
      "source": [
        "# Initialize model and move to device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = CloudMotionModel(use_pretrained=True)\n",
        "\n",
        "# Print out the size of the model for reference\n",
        "print_num_parameters(model)\n",
        "\n",
        "# Move model to device after creation\n",
        "model = model.to(device)\n",
        "\n",
        "# Create random test batch\n",
        "batch = {\n",
        "    'images': torch.randn(2, 5, 3, 384, 384).to(device)\n",
        "}\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(batch['images'])\n",
        "\n",
        "# Print shapes and devices\n",
        "for key, value in outputs.items():\n",
        "    print(f\"{key}: shape={value.shape}, device={value.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Visualizations"
      ],
      "metadata": {
        "id": "Qo55NOtey4X3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "QLvs2uxxzDYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_normalize(img):\n",
        "    \"\"\"\n",
        "    Normalize image data handling NaN values and edge cases.\n",
        "\n",
        "    Args:\n",
        "        img: Image data as numpy array or other format\n",
        "            For numpy arrays, normalizes to [0,1] range\n",
        "            For other formats, returns unchanged\n",
        "\n",
        "    Returns:\n",
        "        Normalized image with same type as input:\n",
        "        - For numpy arrays: (img - min) / (max - min) if max > min\n",
        "        - For numpy arrays with max == min: Array of zeros\n",
        "        - For non-numpy inputs: Original image unchanged\n",
        "    \"\"\"\n",
        "    if isinstance(img, np.ndarray):\n",
        "        min_val = np.nanmin(img)\n",
        "        max_val = np.nanmax(img)\n",
        "        if max_val > min_val:\n",
        "            return (img - min_val) / (max_val - min_val)\n",
        "        return np.zeros_like(img)\n",
        "    return img\n",
        "\n",
        "def create_diagnostic_text(heights, motion, metadata, calib_params, center_height, model_type=\"\"):\n",
        "    \"\"\"\n",
        "    Create formatted diagnostic text for visualization overlays.\n",
        "\n",
        "    Args:\n",
        "        heights (np.ndarray): Height field array\n",
        "        motion (np.ndarray): Motion field array\n",
        "        metadata (dict): Flight metadata containing:\n",
        "            - GPS_MSL_Alt: Aircraft altitude in meters\n",
        "            - True_Airspeed: Aircraft speed in m/s\n",
        "        calib_params (dict): Calibration parameters with:\n",
        "            - motion_scale: Motion field scaling factor\n",
        "            - height_scale: Height field scaling factor\n",
        "        center_height (float): LiDAR height measurement at center\n",
        "        model_type (str, optional): Model identifier string\n",
        "\n",
        "    Returns:\n",
        "        str: HTML-formatted diagnostic text containing:\n",
        "            - Aircraft altitude and speed\n",
        "            - LiDAR and predicted center heights\n",
        "            - Height field range and valid data percentage\n",
        "            - Motion field range\n",
        "            - Calibration scale factors\n",
        "    \"\"\"\n",
        "    center_height_value = heights[heights.shape[0]//2, heights.shape[1]//2]\n",
        "\n",
        "    return (\n",
        "        f\"{model_type} Diagnostics:<br>\"\n",
        "        f\"Aircraft Alt: {metadata['GPS_MSL_Alt']:.1f}m<br>\"\n",
        "        f\"Aircraft Speed: {metadata['True_Airspeed']:.1f} m/s<br>\"\n",
        "        f\"LiDAR height: {center_height:.1f}m<br>\"\n",
        "        f\"Center height: {center_height_value:.1f}m<br>\"\n",
        "        f\"Height range: {np.nanmin(heights):.1f}m to {np.nanmax(heights):.1f}m<br>\"\n",
        "        f\"Valid data: {np.sum(~np.isnan(heights)) / heights.size * 100:.1f}%<br>\"\n",
        "        f\"Motion range: {np.min(motion):.3f} to {np.max(motion):.3f} pixels<br>\"\n",
        "        f\"Motion scale: {calib_params['motion_scale']:.3f}<br>\"\n",
        "        f\"Height scale: {calib_params['height_scale']:.3f}<br>\"\n",
        "    )"
      ],
      "metadata": {
        "id": "HY4xGV1GzEFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vc_LkNWoQzL"
      },
      "source": [
        "## Motion Field Visualizations\n",
        "\n",
        "## visualize_motion_sequence\n",
        "\n",
        "Creates a three-panel visualization comparing motion estimates across a sequence:\n",
        "\n",
        "### Layout\n",
        "\\begin{align*}\n",
        "\\text{Panel 1} &: \\text{Original image frames} \\\\\n",
        "\\text{Panel 2} &: \\text{Frame-by-frame motion fields} \\\\\n",
        "\\text{Panel 3} &: \\text{Refined vs Original RAFT comparison}\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "Motion fields are represented using:\n",
        "1. Background image in grayscale\n",
        "2. Motion magnitude overlay (viridis colormap)\n",
        "3. Subsampled motion vectors (white arrows)\n",
        "4. Confidence overlay for refined motion (red-yellow-green)\n",
        "\n",
        "## `plot_motion_field_with_image()`\n",
        "\n",
        "Creates composite visualization of motion field over an image:\n",
        "\n",
        "$$\\text{magnitude} = \\sqrt{dx^2 + dy^2}$$\n",
        "\n",
        "Components:\n",
        "- Background image (grayscale)\n",
        "- Motion magnitude (viridis colormap, α = 0.7)\n",
        "- Motion vectors on grid (subsample=16)\n",
        "- Optional confidence overlay (RdYlGn colormap, α = 0.3)\n",
        "\n",
        "## `visualize_detailed_motion()`\n",
        "\n",
        "Four-panel detailed motion analysis:\n",
        "1. Original image frame\n",
        "2. Motion field overlay\n",
        "3. Motion magnitude map:\n",
        "   $$\\text{magnitude} = \\sqrt{dx^2 + dy^2}$$\n",
        "4. Confidence map\n",
        "\n",
        "Each visualization:\n",
        "- Uses safe normalization for correct scaling\n",
        "- Handles packed sequences for variable-length inputs\n",
        "- Returns compressed JPEG to manage notebook size\n",
        "- Includes calibrated colorbars for quantitative comparison\n",
        "\n",
        "Key features:\n",
        "- Supports both individual frames and sequences\n",
        "- Visualizes fine-tuned vs original RAFT differences\n",
        "- Displays motion confidence estimates\n",
        "- Memory-efficient cleanup of matplotlib objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBJToGP9f10T"
      },
      "outputs": [],
      "source": [
        "@show_warnings\n",
        "def visualize_motion_sequence(model_outputs, data, sample_idx=0, figsize=(25, 15)):\n",
        "    \"\"\"\n",
        "    Create multi-panel visualization comparing motion fields across a sequence.\n",
        "\n",
        "    Creates three visualization panels:\n",
        "    1. Original image sequence\n",
        "    2. Frame-by-frame motion fields with shared colorbar\n",
        "    3. Comparison of refined vs original RAFT motion estimation\n",
        "\n",
        "    Args:\n",
        "        model_outputs (dict): Model predictions containing:\n",
        "            - motion_fields: Per-frame motion estimates\n",
        "            - refined_motion: Final refined motion field\n",
        "            - reference_motion: Original RAFT motion field\n",
        "            - confidence: Motion confidence scores\n",
        "        data (dict or Tensor): Input data with:\n",
        "            - images: Image sequence\n",
        "            - sequence_lengths: Length of each sequence\n",
        "            - image_paths: Optional paths for labeling\n",
        "        sample_idx (int): Which sample in batch to visualize\n",
        "        figsize (tuple): Figure dimensions (width, height)\n",
        "\n",
        "    Returns:\n",
        "        HTML: JPEG visualization embedded in notebook\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # Handle both dictionary and tensor/packed sequence inputs\n",
        "        if isinstance(data, dict):\n",
        "            input_images = data['images']\n",
        "            sequence_lengths = data['sequence_lengths']\n",
        "            image_paths = data.get('image_paths', None)\n",
        "\n",
        "            # Unpack if needed\n",
        "            if isinstance(input_images, PackedSequence):\n",
        "                input_images, _ = pad_packed_sequence(input_images, batch_first=True)\n",
        "\n",
        "            seq_length = int(sequence_lengths[sample_idx])\n",
        "        else:\n",
        "            input_images = data\n",
        "            if isinstance(input_images, PackedSequence):\n",
        "                input_images, lengths = pad_packed_sequence(input_images, batch_first=True)\n",
        "                seq_length = int(lengths[sample_idx])\n",
        "            else:\n",
        "                seq_length = int(input_images.size(1))\n",
        "            image_paths = None\n",
        "\n",
        "        def unpack_if_needed(tensor_or_packed):\n",
        "            if isinstance(tensor_or_packed, PackedSequence):\n",
        "                unpacked, _ = pad_packed_sequence(tensor_or_packed, batch_first=True)\n",
        "                return unpacked\n",
        "            return tensor_or_packed\n",
        "\n",
        "        # Move tensors to CPU and convert to numpy\n",
        "        input_images = input_images.detach().cpu()\n",
        "\n",
        "        # Get motion fields, handling both packed and unpacked cases\n",
        "        motion_fields = unpack_if_needed(model_outputs['motion_fields'])\n",
        "        refined_motion = unpack_if_needed(model_outputs['refined_motion'])\n",
        "        reference_motion = unpack_if_needed(model_outputs['reference_motion'])\n",
        "\n",
        "        # Convert to numpy\n",
        "        motion_fields = motion_fields[sample_idx].detach().cpu().numpy()\n",
        "        refined_motion = refined_motion[sample_idx].detach().cpu().numpy()\n",
        "        reference_motion = reference_motion[sample_idx].detach().cpu().numpy()\n",
        "        confidence = safe_normalize(model_outputs['confidence'][sample_idx, 0].detach().cpu().numpy())\n",
        "\n",
        "\n",
        "        # Create figure and a single GridSpec for main content\n",
        "        fig = plt.figure(figsize=figsize)\n",
        "        gs = plt.GridSpec(3, seq_length, height_ratios=[1, 1, 1.2])\n",
        "\n",
        "        # Plot original images in top row\n",
        "        for i in range(seq_length):\n",
        "            ax = plt.subplot(gs[0, i])\n",
        "            img = input_images[sample_idx, i].numpy().transpose(1, 2, 0)\n",
        "            img = safe_normalize(img)\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            if image_paths is not None:\n",
        "                try:\n",
        "                    path = os.path.basename(str(image_paths[sample_idx][i]))\n",
        "                    plt.title(f'Frame {i+1}\\n{path}', fontsize=8)\n",
        "                except (IndexError, TypeError):\n",
        "                    plt.title(f'Frame {i+1}')\n",
        "            else:\n",
        "                plt.title(f'Frame {i+1}')\n",
        "            plt.axis('off')\n",
        "\n",
        "        # Plot individual motion fields in middle row with a shared colorbar\n",
        "        motion_overlays = []\n",
        "        for i in range(seq_length - 1):\n",
        "            ax = plt.subplot(gs[1, i])\n",
        "            overlay = plot_motion_field_with_image(\n",
        "                motion_fields[i],\n",
        "                safe_normalize(input_images[sample_idx, i+1].numpy().transpose(1, 2, 0)),\n",
        "                title=f'Motion Field {i+1}',\n",
        "                add_colorbar=False  # Don't add individual colorbars\n",
        "            )\n",
        "            motion_overlays.append(overlay)\n",
        "\n",
        "        # Plot refined motion and reference motion side by side in bottom row\n",
        "        ax = plt.subplot(gs[2, :seq_length//2])\n",
        "        plot_motion_field_with_image(\n",
        "            refined_motion,\n",
        "            safe_normalize(input_images[sample_idx, -1].numpy().transpose(1, 2, 0)),\n",
        "            title='Fine-tuned RAFT Motion',\n",
        "            confidence=confidence\n",
        "        )\n",
        "\n",
        "        ax = plt.subplot(gs[2, seq_length//2:])\n",
        "        plot_motion_field_with_image(\n",
        "            reference_motion,\n",
        "            safe_normalize(input_images[sample_idx, -1].numpy().transpose(1, 2, 0)),\n",
        "            title='Original RAFT Motion'\n",
        "        )\n",
        "\n",
        "        # Apply tight_layout first\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Then add the colorbar after tight_layout has been applied\n",
        "        # Get the position of the middle row plots to align the colorbar\n",
        "        middle_row_pos = gs[1, :].get_position(fig)\n",
        "        cax = fig.add_axes([0.92, middle_row_pos.y0, 0.02, middle_row_pos.height])\n",
        "        fig.colorbar(motion_overlays[0], cax=cax, label='Motion Magnitude')\n",
        "\n",
        "        # Create JPEG and close the figure\n",
        "        jpeg_output = embed_matplotlib_jpeg(fig, dpi=50)\n",
        "        plt.close(fig)  # Close the figure explicitly\n",
        "        return jpeg_output\n",
        "\n",
        "@show_warnings\n",
        "def plot_motion_field_with_image(motion_field, background_image, title='Motion Field',\n",
        "                              subsample=16, confidence=None, alpha=0.7, ax=None, add_colorbar=True):\n",
        "    \"\"\"\n",
        "    Create composite visualization of motion field overlaid on image.\n",
        "\n",
        "    Combines:\n",
        "    - Background image in grayscale\n",
        "    - Motion magnitude as colormap overlay\n",
        "    - Subsampled motion vectors as arrows\n",
        "    - Optional confidence scores as second overlay\n",
        "\n",
        "    Args:\n",
        "        motion_field (np.ndarray): Motion vectors [2, H, W]\n",
        "        background_image (np.ndarray): Image to overlay on [H, W, C]\n",
        "        title (str): Plot title\n",
        "        subsample (int): Spacing between motion arrows\n",
        "        confidence (np.ndarray, optional): Confidence scores [H, W]\n",
        "        alpha (float): Overlay transparency\n",
        "        ax (matplotlib.axes, optional): Axes to plot on\n",
        "        add_colorbar (bool): Whether to add colorbar\n",
        "\n",
        "    Returns:\n",
        "        matplotlib.image.AxesImage: Motion magnitude overlay for colorbar\n",
        "    \"\"\"\n",
        "    # Use provided axes or current axes\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    dx = motion_field[0]\n",
        "    dy = motion_field[1]\n",
        "    magnitude = np.sqrt(dx**2 + dy**2)\n",
        "\n",
        "    background_image = safe_normalize(background_image)\n",
        "\n",
        "    # Create composed visualization\n",
        "    ax.imshow(background_image, cmap='gray')\n",
        "\n",
        "    # Create a semi-transparent overlay of the motion magnitude\n",
        "    motion_overlay = ax.imshow(magnitude, cmap='viridis', alpha=alpha)\n",
        "    if add_colorbar:\n",
        "        plt.colorbar(motion_overlay, label='Motion Magnitude', ax=ax)\n",
        "\n",
        "    # Create grid for quiver plot\n",
        "    h, w = magnitude.shape\n",
        "    y, x = np.mgrid[0:h:subsample, 0:w:subsample]\n",
        "\n",
        "    # Subsample motion field for arrows\n",
        "    dx_sub = dx[::subsample, ::subsample]\n",
        "    dy_sub = dy[::subsample, ::subsample]\n",
        "\n",
        "    # Plot arrows\n",
        "    ax.quiver(x, y, dx_sub, dy_sub, angles='xy',\n",
        "             scale_units='xy', scale=0.5,\n",
        "             color='white', alpha=0.8)\n",
        "\n",
        "    if confidence is not None:\n",
        "        confidence_overlay = ax.imshow(confidence, cmap='RdYlGn', alpha=0.3)\n",
        "        if add_colorbar:\n",
        "            plt.colorbar(confidence_overlay, label='Confidence', ax=ax)\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_axis_off()\n",
        "\n",
        "    return motion_overlay  # Return the motion overlay for creating shared colorbar\n",
        "\n",
        "@show_warnings\n",
        "def visualize_detailed_motion(model_outputs, image_sequence, sample_idx=0,\n",
        "                           frame_idx=-1, figsize=(20, 5)):\n",
        "    \"\"\"\n",
        "    Create detailed four-panel motion analysis visualization.\n",
        "\n",
        "    Panels:\n",
        "    1. Original image\n",
        "    2. Motion field overlay\n",
        "    3. Motion magnitude map\n",
        "    4. Confidence map\n",
        "\n",
        "    Args:\n",
        "        model_outputs (dict): Model predictions with motion fields and confidence\n",
        "        image_sequence (Tensor or PackedSequence): Input image sequence\n",
        "        sample_idx (int): Which sample in batch to visualize\n",
        "        frame_idx (int): Which frame to analyze (-1 for last)\n",
        "        figsize (tuple): Figure dimensions\n",
        "\n",
        "    Returns:\n",
        "        HTML: JPEG visualization embedded in notebook\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # Unpack sequences if needed\n",
        "        if isinstance(image_sequence, PackedSequence):\n",
        "            images_unpacked, lengths = pad_packed_sequence(image_sequence, batch_first=True)\n",
        "            seq_length = lengths[sample_idx]\n",
        "            if frame_idx == -1:\n",
        "                frame_idx = seq_length - 2  # Last valid frame for motion\n",
        "        else:\n",
        "            images_unpacked = image_sequence\n",
        "\n",
        "        # Get motion fields\n",
        "        if isinstance(model_outputs['refined_motion'], PackedSequence):\n",
        "            refined_unpacked, _ = pad_packed_sequence(model_outputs['refined_motion'], batch_first=True)\n",
        "            motion_fields_unpacked, _ = pad_packed_sequence(model_outputs['motion_fields'], batch_first=True)\n",
        "\n",
        "            if frame_idx == -1:\n",
        "                motion_field = refined_unpacked[sample_idx].detach().cpu().numpy()\n",
        "            else:\n",
        "                motion_field = motion_fields_unpacked[sample_idx, frame_idx].detach().cpu().numpy()\n",
        "        else:\n",
        "            if frame_idx == -1:\n",
        "                motion_field = model_outputs['refined_motion'][sample_idx].detach().cpu().numpy()\n",
        "            else:\n",
        "                motion_field = model_outputs['motion_fields'][sample_idx, frame_idx].detach().cpu().numpy()\n",
        "\n",
        "        # Get frame to display\n",
        "        frame = images_unpacked[sample_idx, frame_idx + 1].detach().cpu().numpy()\n",
        "        frame = safe_normalize(frame)  # Normalize the frame\n",
        "\n",
        "        # Get confidence\n",
        "        if isinstance(model_outputs['confidence'], PackedSequence):\n",
        "            confidence_unpacked, _ = pad_packed_sequence(model_outputs['confidence'], batch_first=True)\n",
        "            confidence = confidence_unpacked[sample_idx, 0].detach().cpu().numpy()\n",
        "        else:\n",
        "            confidence = model_outputs['confidence'][sample_idx, 0].detach().cpu().numpy()\n",
        "\n",
        "        confidence = safe_normalize(confidence)  # Normalize the confidence\n",
        "\n",
        "        # Create figure\n",
        "        fig = plt.figure(figsize=figsize)\n",
        "\n",
        "        # Original image\n",
        "        plt.subplot(141)\n",
        "        plt.imshow(np.transpose(frame, (1, 2, 0)))\n",
        "        plt.title('Original Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Motion field overlay\n",
        "        plt.subplot(142)\n",
        "        plot_motion_field_with_image(\n",
        "            motion_field,\n",
        "            background_image=np.transpose(frame, (1, 2, 0)),\n",
        "            title='Motion Field Overlay'\n",
        "        )\n",
        "\n",
        "        # Motion magnitude\n",
        "        plt.subplot(143)\n",
        "        magnitude = np.sqrt(motion_field[0]**2 + motion_field[1]**2)\n",
        "        magnitude = safe_normalize(magnitude)  # Normalize the magnitude\n",
        "        plt.imshow(magnitude, cmap='viridis')\n",
        "        plt.colorbar(label='Magnitude')\n",
        "        plt.title('Motion Magnitude')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Confidence map\n",
        "        plt.subplot(144)\n",
        "        plt.imshow(confidence, cmap='RdYlGn')\n",
        "        plt.colorbar(label='Confidence')\n",
        "        plt.title('Confidence Map')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Convert figure to JPEG\n",
        "        jpeg_output = embed_matplotlib_jpeg(fig, dpi=50)\n",
        "        plt.close(fig)  # Explicitly close the figure\n",
        "\n",
        "        return jpeg_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zy_ouV3KJPA"
      },
      "source": [
        "## Height Field Visualizations\n",
        "\n",
        "## create_height_field_plot\n",
        "\n",
        "Creates a 3D surface plot of cloud height fields using Plotly's Surface plots.\n",
        "\n",
        "### Components\n",
        "1. Main height surface:\n",
        "   $$(x,y) = \\text{meshgrid}(\\text{range}(-w/2, w/2), \\text{range}(-h/2, h/2))$$\n",
        "\n",
        "2. Uncertainty visualization:\n",
        "  $$\n",
        "  \\text{opacity} =\n",
        "  \\begin{cases}\n",
        "  0.3 \\cdot \\text{uncertainty}, & \\text{if height valid} \\\\\n",
        "  0, & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "  $$\n",
        "\n",
        "3. LiDAR reference point:\n",
        "   - Position: $(0,0,h_{lidar})$\n",
        "   - Marker: Red diamond\n",
        "\n",
        "### Adaptive tick spacing:\n",
        "  $$\n",
        "  \\text{spacing} = \\begin{cases}\n",
        "  2000\\text{m}, & \\text{if range} > 10\\text{km} \\\\\n",
        "  1000\\text{m}, & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "  $$\n",
        "\n",
        "## visualize_calibrated_height_fields\n",
        "\n",
        "Creates side-by-side comparison of fine-tuned vs reference RAFT height fields.\n",
        "\n",
        "### Layout\n",
        "1. Left subplot: Fine-tuned RAFT\n",
        "   - Height field surface (Viridis)\n",
        "   - Uncertainty overlay (Reds)\n",
        "   - Calibration parameters\n",
        "   - Validity percentage\n",
        "\n",
        "2. Right subplot: Original RAFT\n",
        "   - Identical components\n",
        "   - Shared z-scale\n",
        "\n",
        "### Camera Settings\n",
        "\\begin{align*}\n",
        "\\text{aspect ratio} &= [1:1:0.5] \\\\\n",
        "\\text{camera position} &= (1.5, 1.5, 1)\n",
        "\\end{align*}\n",
        "\n",
        "Both functions include:\n",
        "- NaN handling for invalid heights\n",
        "- Percentage of valid data calculation\n",
        "- Diagnostic statistics\n",
        "- Consistent colorbar positioning\n",
        "- Shared z-axis scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiFDMXBxlmlm"
      },
      "outputs": [],
      "source": [
        "def create_height_field_plot(heights, uncertainty, calibration_params, center_height, title,\n",
        "                          colorbar_x=1.02, show_lidar_legend=False):\n",
        "    \"\"\"\n",
        "    Create a 3D surface plot of cloud height field with uncertainty overlay.\n",
        "\n",
        "    Creates three visualization layers:\n",
        "    1. Height field surface with viridis colormap\n",
        "    2. Uncertainty overlay with red transparency\n",
        "    3. LiDAR reference point marker\n",
        "\n",
        "    Args:\n",
        "        heights (np.ndarray): Cloud height field array [H, W]\n",
        "        uncertainty (np.ndarray): Uncertainty values [H, W]\n",
        "        calibration_params (dict): Height field calibration parameters\n",
        "        center_height (float): LiDAR height measurement at center\n",
        "        title (str): Plot title\n",
        "        colorbar_x (float): X-position of colorbar (0-1 range)\n",
        "        show_lidar_legend (bool): Whether to show LiDAR point in legend\n",
        "\n",
        "    Returns:\n",
        "        tuple: (traces, valid_percentage)\n",
        "            - traces: List of Plotly graph objects for surface plots\n",
        "            - valid_percentage: Percentage of valid height measurements\n",
        "    \"\"\"\n",
        "    h, w = heights.shape\n",
        "    x = np.arange(w) - (w // 2)\n",
        "    y = np.arange(h) - (h // 2)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    valid_mask = ~np.isnan(heights)\n",
        "    valid_percentage = np.sum(valid_mask) / heights.size * 100\n",
        "    plotted_heights = heights.copy()\n",
        "    plotted_heights[~valid_mask] = np.nanmin(heights)\n",
        "\n",
        "    # Calculate height range for tick spacing\n",
        "    height_range = np.nanmax(heights) - np.nanmin(heights)\n",
        "    tick_spacing = 1000  # 1km spacing\n",
        "    if height_range > 10000:\n",
        "        tick_spacing = 2000  # 2km spacing for large ranges\n",
        "\n",
        "    traces = []\n",
        "\n",
        "    # Main surface plot\n",
        "    traces.append(go.Surface(\n",
        "        x=X, y=Y, z=plotted_heights,\n",
        "        colorscale='Viridis',\n",
        "        colorbar=dict(\n",
        "            title='Height (m)',\n",
        "            x=colorbar_x,  # Position passed as parameter\n",
        "            len=0.75,\n",
        "            y=0.5,\n",
        "            dtick=tick_spacing,\n",
        "            thickness=20,\n",
        "            tickmode='linear'\n",
        "        ),\n",
        "        opacity=0.8,\n",
        "        showscale=True\n",
        "    ))\n",
        "\n",
        "    # Uncertainty surface if available\n",
        "    if uncertainty is not None:\n",
        "        uncertainty_colors = np.zeros((h, w, 4))\n",
        "        uncertainty_colors[..., 0] = 1\n",
        "        uncertainty_colors[..., 3] = uncertainty * 0.3\n",
        "        uncertainty_colors[~valid_mask] = [0, 0, 0, 0]\n",
        "        traces.append(go.Surface(\n",
        "            x=X, y=Y, z=plotted_heights,\n",
        "            surfacecolor=uncertainty_colors[..., 3],\n",
        "            colorscale='Reds',\n",
        "            showscale=False,\n",
        "            opacity=0.3\n",
        "        ))\n",
        "\n",
        "    # LiDAR point\n",
        "    traces.append(go.Scatter3d(\n",
        "        x=[0], y=[0], z=[center_height],\n",
        "        mode='markers',\n",
        "        marker=dict(size=5, color='red', symbol='diamond'),\n",
        "        name=f'LiDAR: {center_height:.0f}m',\n",
        "        showlegend=show_lidar_legend  # Control whether this plot shows in legend\n",
        "    ))\n",
        "\n",
        "    return traces, valid_percentage\n",
        "\n",
        "def visualize_calibrated_height_fields(refined_heights, refined_uncertainty,\n",
        "                                      reference_heights, reference_uncertainty,\n",
        "                                      ft_calib_params, ref_calib_params, center_height,\n",
        "                                      refined_stats, reference_stats):\n",
        "    \"\"\"\n",
        "    Create side-by-side comparison of fine-tuned vs original RAFT height fields.\n",
        "\n",
        "    Generates a Plotly figure with:\n",
        "    1. Left subplot: Fine-tuned RAFT height field\n",
        "    2. Right subplot: Original RAFT height field\n",
        "    Both with uncertainty overlays and diagnostic statistics\n",
        "\n",
        "    Args:\n",
        "        refined_heights (np.ndarray): Fine-tuned height field [H, W]\n",
        "        refined_uncertainty (np.ndarray): Fine-tuned uncertainty [H, W]\n",
        "        reference_heights (np.ndarray): Original RAFT height field [H, W]\n",
        "        reference_uncertainty (np.ndarray): Original uncertainty [H, W]\n",
        "        ft_calib_params (dict): Fine-tuned calibration parameters\n",
        "        ref_calib_params (dict): Original calibration parameters\n",
        "        center_height (float): LiDAR height measurement\n",
        "        refined_stats (str): Diagnostic text for fine-tuned model\n",
        "        reference_stats (str): Diagnostic text for original model\n",
        "\n",
        "    Returns:\n",
        "        plotly.graph_objects.Figure: Interactive 3D visualization with:\n",
        "            - Synchronized height scales and camera angles\n",
        "            - Uncertainty overlays\n",
        "            - Diagnostic statistics\n",
        "            - LiDAR reference points\n",
        "    \"\"\"\n",
        "    # Create both plots with different colorbar positions\n",
        "    refined_traces, refined_valid = create_height_field_plot(\n",
        "        refined_heights, refined_uncertainty, ft_calib_params,\n",
        "        center_height, \"Height Field (Fine-tuned RAFT)\",\n",
        "        colorbar_x=0.45,\n",
        "        show_lidar_legend=False  # Don't show legend for first plot\n",
        "    )\n",
        "    reference_traces, reference_valid = create_height_field_plot(\n",
        "        reference_heights, reference_uncertainty, ref_calib_params,\n",
        "        center_height, \"Height Field (Original RAFT)\",\n",
        "        colorbar_x=1.02,\n",
        "        show_lidar_legend=True  # Show legend only for second plot\n",
        "    )\n",
        "\n",
        "    # Print diagnostic information\n",
        "    print(\"\\n\" + refined_stats.replace(\"<br>\", \"\\n\"))\n",
        "    print(\"\\n\" + reference_stats.replace(\"<br>\", \"\\n\"))\n",
        "\n",
        "    # Create subplot figure\n",
        "    fig = make_subplots(\n",
        "        rows=1, cols=2,\n",
        "        specs=[[{'type': 'surface'}, {'type': 'surface'}]],\n",
        "        subplot_titles=None,\n",
        "        horizontal_spacing=0.02  # Reduce space between subplots\n",
        "    )\n",
        "\n",
        "    # Add traces to subplots\n",
        "    for trace in refined_traces:\n",
        "        fig.add_trace(trace, row=1, col=1)\n",
        "    for trace in reference_traces:\n",
        "        fig.add_trace(trace, row=1, col=2)\n",
        "\n",
        "    # Update layout with increased dimensions and adjusted margins\n",
        "    fig.update_layout(\n",
        "        height=800,\n",
        "        width=1350,  # Increased from 1200\n",
        "        title_text=\"Calibrated Height Fields Comparison\",\n",
        "        title_y=0.95,  # Move main title down slightly\n",
        "        margin=dict(t=180, b=50, l=0, r=20),  # Adjusted all margins\n",
        "        scene=dict(\n",
        "            xaxis_title='X Distance (pixels)',\n",
        "            yaxis_title='Y Distance (pixels)',\n",
        "            zaxis_title='Height (m)',\n",
        "            aspectratio=dict(x=1, y=1, z=0.5),\n",
        "            camera=dict(eye=dict(x=1.5, y=1.5, z=1)),\n",
        "            domain=dict(x=[0.0, 0.48])  # Adjust left plot position\n",
        "        ),\n",
        "        scene2=dict(\n",
        "            xaxis_title='X Distance (pixels)',\n",
        "            yaxis_title='Y Distance (pixels)',\n",
        "            zaxis_title='Height (m)',\n",
        "            aspectratio=dict(x=1, y=1, z=0.5),\n",
        "            camera=dict(eye=dict(x=1.5, y=1.5, z=1)),\n",
        "            domain=dict(x=[0.52, 1.0])\n",
        "        ),\n",
        "        annotations=[\n",
        "            dict(\n",
        "                text=f\"Height Field (Fine-tuned RAFT)<br>Valid data: {refined_valid:.1f}%\",\n",
        "                x=0.42,  # Right align for left plot\n",
        "                y=1.08,  # Adjust vertical position as needed\n",
        "                xref=\"paper\",\n",
        "                yref=\"paper\",\n",
        "                showarrow=False,\n",
        "                font=dict(size=14),\n",
        "                xanchor=\"right\"  # Right justify the text\n",
        "            ),\n",
        "            dict(\n",
        "                text=f\"Height Field (Original RAFT)<br>Valid data: {reference_valid:.1f}%\",\n",
        "                x=0.94,  # Right align for right plot\n",
        "                y=1.08,  # Adjust vertical position as needed\n",
        "                xref=\"paper\",\n",
        "                yref=\"paper\",\n",
        "                showarrow=False,\n",
        "                font=dict(size=14),\n",
        "                xanchor=\"right\"  # Right justify the text\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Add diagnostic text annotations with adjusted y-position\n",
        "    fig.add_annotation(\n",
        "        x=0.01, y=1.15,  # Moved up slightly\n",
        "        xref=\"paper\", yref=\"paper\",\n",
        "        text=refined_stats,\n",
        "        showarrow=False,\n",
        "        font=dict(size=10),\n",
        "        bgcolor=\"white\",\n",
        "        opacity=0.8,\n",
        "        align=\"left\"\n",
        "    )\n",
        "\n",
        "    fig.add_annotation(\n",
        "        x=0.70, y=1.15,\n",
        "        xref=\"paper\", yref=\"paper\",\n",
        "        text=reference_stats,\n",
        "        showarrow=False,\n",
        "        font=dict(size=10),\n",
        "        bgcolor=\"white\",\n",
        "        opacity=0.8,\n",
        "        align=\"left\"\n",
        "    )\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Progress Visualization\n",
        "\n",
        "This function creates visualizations during model training to monitor both optical flow estimation and height field generation.\n",
        "\n",
        "## Components\n",
        "\n",
        "### 1. Motion Field Visualization\n",
        "\\begin{align*}\n",
        "& \\text{Input}: F_{refined}, F_{reference} \\in \\mathbb{R}^{2 \\times H \\times W} \\\\\n",
        "& \\text{Output}: \\text{3-panel comparison showing:} \\\\\n",
        "& \\quad - \\text{Original image sequence} \\\\\n",
        "& \\quad - \\text{Frame-by-frame motion fields} \\\\\n",
        "& \\quad - \\text{Refined vs Original RAFT comparison}\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "### 2. Height Field Generation\n",
        "For valid LiDAR measurements:\n",
        "\n",
        "\\begin{align*}\n",
        "h_{center} &= \\text{denormalize}(h_{lidar}) \\\\\n",
        "h_{refined} &= \\text{calculate_heights}(F_{refined}, \\text{metadata}, h_{center}) \\\\\n",
        "h_{reference} &= \\text{calculate_heights}(F_{reference}, \\text{metadata}, h_{center})\n",
        "\\end{align*}\n",
        "\n",
        "### 3. Calibration Analysis\n",
        "\\begin{align*}\n",
        "\\text{Parameters tracked:}& \\\\\n",
        "s_{motion} &= \\text{motion scale factor} \\\\\n",
        "s_{height} &= \\text{height scale factor} \\\\\n",
        "s_{vertical} &= \\text{vertical extent scale}\n",
        "\\end{align*}\n",
        "\n",
        "### 4. Diagnostic Metrics\n",
        "For each model (refined and reference):\n",
        "- Height field range\n",
        "- Valid data percentage\n",
        "- Motion magnitude range\n",
        "- Center height accuracy\n",
        "- Calibration parameters\n",
        "\n",
        "The function handles:\n",
        "- PackedSequence unpacking\n",
        "- Tensor normalization\n",
        "- GPU memory management\n",
        "- Exception handling"
      ],
      "metadata": {
        "id": "Zlx6EyqezWa8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU3WAk0iKJv2"
      },
      "outputs": [],
      "source": [
        "def visualize_training_progress(outputs, batch_data, dataset, model, sample_idx=0):\n",
        "    \"\"\"\n",
        "    Create comprehensive visualization of model training progress.\n",
        "\n",
        "    Generates three visualizations:\n",
        "    1. Motion field comparison showing:\n",
        "        - Original image sequence\n",
        "        - Per-frame motion fields\n",
        "        - Refined vs original RAFT comparison\n",
        "\n",
        "    2. Height field reconstruction showing:\n",
        "        - Fine-tuned model heights with uncertainty\n",
        "        - Original model heights with uncertainty\n",
        "        - LiDAR reference point\n",
        "        - Diagnostic statistics\n",
        "\n",
        "    3. Calibration parameter history showing:\n",
        "        - Motion scale evolution\n",
        "        - Height scale evolution\n",
        "        - Vertical extent scale changes\n",
        "\n",
        "    Args:\n",
        "        outputs (dict): Model outputs containing:\n",
        "            - refined_motion: Fine-tuned motion fields\n",
        "            - reference_motion: Original RAFT motion fields\n",
        "        batch_data (dict): Training batch with:\n",
        "            - metadata: Flight parameters\n",
        "            - validation_height: LiDAR measurements\n",
        "        dataset: Dataset object for denormalization\n",
        "        model: Model instance with height calculators\n",
        "        sample_idx (int): Which sample in batch to visualize\n",
        "\n",
        "    Note:\n",
        "        Handles packed sequences and GPU memory management.\n",
        "        Catches and reports visualization errors.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            # Motion sequence visualization\n",
        "            if 'refined_motion' in outputs and 'reference_motion' in outputs:\n",
        "                display(visualize_motion_sequence(outputs, batch_data, sample_idx=sample_idx))\n",
        "\n",
        "            # Height field visualization if data available\n",
        "            if ('metadata' in batch_data and\n",
        "                'validation_height' in batch_data and\n",
        "                not torch.isnan(batch_data['validation_height'][sample_idx])):\n",
        "\n",
        "                # Unpack metadata if it's a packed sequence\n",
        "                if isinstance(batch_data['metadata'], PackedSequence):\n",
        "                    metadata_unpacked, _ = pad_packed_sequence(batch_data['metadata'], batch_first=True)\n",
        "                    last_idx = batch_data['sequence_lengths'][sample_idx] - 1\n",
        "                    temp_data = {'metadata': metadata_unpacked}\n",
        "                else:\n",
        "                    temp_data = batch_data\n",
        "\n",
        "                metadata = get_denormalized_metadata(dataset, temp_data, sample_idx)\n",
        "\n",
        "                center_height = float(dataset.denormalize_validation_height(\n",
        "                    batch_data['validation_height'][sample_idx]))\n",
        "\n",
        "                # Handle packed motion fields\n",
        "                if isinstance(outputs['refined_motion'], PackedSequence):\n",
        "                    refined_unpacked, _ = pad_packed_sequence(outputs['refined_motion'], batch_first=True)\n",
        "                    reference_unpacked, _ = pad_packed_sequence(outputs['reference_motion'], batch_first=True)\n",
        "                    refined_motion = refined_unpacked[sample_idx].detach().cpu().numpy()\n",
        "                    reference_motion = reference_unpacked[sample_idx].detach().cpu().numpy()\n",
        "                else:\n",
        "                    refined_motion = outputs['refined_motion'][sample_idx].detach().cpu().numpy()\n",
        "                    reference_motion = outputs['reference_motion'][sample_idx].detach().cpu().numpy()\n",
        "\n",
        "                # Calculate heights\n",
        "                refined_heights, refined_uncertainty = model.finetuned_height_calculator.calculate_heights(\n",
        "                    refined_motion, metadata, center_height)\n",
        "                reference_heights, reference_uncertainty = model.reference_height_calculator.calculate_heights(\n",
        "                    reference_motion, metadata, center_height)\n",
        "\n",
        "                # Get calibration parameters\n",
        "                ft_calib_params = model.finetuned_height_calculator.get_parameters()\n",
        "                ref_calib_params = model.reference_height_calculator.get_parameters()\n",
        "\n",
        "                # Create diagnostic text\n",
        "                refined_stats = create_diagnostic_text(\n",
        "                    refined_heights, refined_motion, metadata,\n",
        "                    ft_calib_params, center_height, \"Fine-tuned RAFT\"\n",
        "                )\n",
        "                reference_stats = create_diagnostic_text(\n",
        "                    reference_heights, reference_motion, metadata,\n",
        "                    ref_calib_params, center_height, \"Original RAFT\"\n",
        "                )\n",
        "\n",
        "                # Create visualization\n",
        "                fig = visualize_calibrated_height_fields(\n",
        "                    refined_heights, refined_uncertainty,\n",
        "                    reference_heights, reference_uncertainty,\n",
        "                    ft_calib_params, ref_calib_params, center_height,\n",
        "                    refined_stats, reference_stats\n",
        "                )\n",
        "\n",
        "                # Display visualization\n",
        "                fig.show()\n",
        "\n",
        "                # Scale histories visualization (static matplotlib - use JPEG)\n",
        "                display(plot_scale_histories(model))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Visualization error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test visualization with random data\n",
        "\n"
      ],
      "metadata": {
        "id": "qaAeVyqQgiCZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yofiYrEFf6LP"
      },
      "outputs": [],
      "source": [
        "# Get the correct metadata length\n",
        "metadata_length = len(metadata_manager.flight_data)\n",
        "\n",
        "# Create random test batch\n",
        "batch = {\n",
        "    'images': torch.randn(2, 5, 3, 384, 384).to(device),\n",
        "    'sequence_lengths': torch.tensor([5, 5]),  # Both sequences have length 5\n",
        "    'image_paths': [\n",
        "        ['path1.jpg', 'path2.jpg', 'path3.jpg', 'path4.jpg', 'path5.jpg'],\n",
        "        ['path6.jpg', 'path7.jpg', 'path8.jpg', 'path9.jpg', 'path10.jpg']\n",
        "    ],\n",
        "    'metadata': torch.randn(2, 5, metadata_length).to(device),  # Using correct metadata length\n",
        "    'validation_height': torch.randn(2).to(device)\n",
        "}\n",
        "\n",
        "# Run model inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(batch['images'])\n",
        "\n",
        "# Print shapes and devices\n",
        "for key, value in outputs.items():\n",
        "    print(f\"{key}: shape={value.shape}, device={value.device}\")\n",
        "\n",
        "# Visualize entire sequence with motion fields\n",
        "print(\"Calling visualization function with:\")\n",
        "print(f\"model_outputs keys: {outputs.keys()}\")\n",
        "print(f\"batch type: {type(batch)}\")\n",
        "if isinstance(batch, dict):\n",
        "    print(f\"batch keys: {batch.keys()}\")\n",
        "display(visualize_motion_sequence(outputs, batch))\n",
        "\n",
        "# Visualize detailed motion for a specific frame\n",
        "display(visualize_detailed_motion(outputs, batch['images']))\n",
        "\n",
        "# Visualize a specific sample and frame\n",
        "sample_idx = 0  # First sample in batch\n",
        "frame_idx = 1   # Second frame pair\n",
        "display(visualize_detailed_motion(outputs, batch['images'],\n",
        "                         sample_idx=sample_idx,\n",
        "                         frame_idx=frame_idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "KlYb5EBx0BBW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shvq0QvOuyMQ"
      },
      "source": [
        "## TrainingLogger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAnAFfEThjrS"
      },
      "outputs": [],
      "source": [
        "class TrainingLogger:\n",
        "    \"\"\"\n",
        "    Logger for tracking and visualizing training metrics and saving model checkpoints.\n",
        "\n",
        "    Args:\n",
        "        save_dir (str): Directory path for saving checkpoints\n",
        "\n",
        "    Attributes:\n",
        "        metrics (defaultdict): Stores training metrics by name\n",
        "        current_epoch (int): Current training epoch\n",
        "    \"\"\"\n",
        "    def __init__(self, save_dir='/content/drive/MyDrive/motion_maps'):\n",
        "        self.save_dir = save_dir\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        self.metrics = defaultdict(list)\n",
        "        self.current_epoch = 0\n",
        "        print(f\"Saving checkpoints to: {save_dir}\")\n",
        "\n",
        "    def log_metrics(self, metrics_dict, step_type='batch'):\n",
        "        \"\"\"\n",
        "        Log training metrics for batch or epoch.\n",
        "\n",
        "        Args:\n",
        "            metrics_dict (dict): Metrics to log {name: value}\n",
        "            step_type (str): Either 'batch' or 'epoch'\n",
        "        \"\"\"\n",
        "        for key, value in metrics_dict.items():\n",
        "            metric_name = f\"{step_type}/{key}\"\n",
        "            self.metrics[metric_name].append(value)\n",
        "            print(f\"Logged metric: {metric_name} = {value}\")\n",
        "\n",
        "    def save_checkpoint(self, model, optimizer, scheduler, model_type='motion',\n",
        "                        additional_info=None):\n",
        "        \"\"\"\n",
        "        Save model checkpoint with training state.\n",
        "\n",
        "        Args:\n",
        "            model: Model to save\n",
        "            optimizer: Optimizer state\n",
        "            scheduler: Learning rate scheduler state\n",
        "            model_type (str): Type of model being saved\n",
        "            additional_info (dict, optional): Extra info to save\n",
        "\n",
        "        Saves:\n",
        "            - Model weights\n",
        "            - Optimizer state\n",
        "            - Scheduler state\n",
        "            - Current epoch\n",
        "            - Training metrics\n",
        "            - Additional info if provided\n",
        "        \"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': self.current_epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'metrics': dict(self.metrics)\n",
        "        }\n",
        "\n",
        "        if additional_info:\n",
        "            checkpoint.update(additional_info)\n",
        "\n",
        "        save_path = os.path.join(\n",
        "            self.save_dir,\n",
        "            f'motion_model_epoch_{self.current_epoch}.pt'\n",
        "        )\n",
        "        torch.save(checkpoint, save_path)\n",
        "        print(f\"Saved checkpoint to: {save_path}\")\n",
        "\n",
        "    def plot_training_progress(self):\n",
        "        \"\"\"\n",
        "        Create visualization of training metrics.\n",
        "\n",
        "        Generates 4-panel plot showing:\n",
        "        - Height estimation error\n",
        "        - Motion prediction loss\n",
        "        - LiDAR supervision loss\n",
        "        - Total combined loss\n",
        "\n",
        "        Returns:\n",
        "            JPEG visualization for notebook display\n",
        "        \"\"\"\n",
        "        print(\"=\" * 50)\n",
        "        print(\"Available metrics:\")\n",
        "        for key, values in self.metrics.items():\n",
        "            print(f\"  {key}: {values}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # Plot Height Error\n",
        "        plt.subplot(221)\n",
        "        if 'epoch/height_error' in self.metrics:\n",
        "            plt.plot(self.metrics['epoch/height_error'], label='Height Error', color='red')\n",
        "        plt.title('Height Error')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Error')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Plot Motion Loss\n",
        "        plt.subplot(222)\n",
        "        if 'epoch/motion_loss' in self.metrics:\n",
        "            plt.plot(self.metrics['epoch/motion_loss'], label='Motion Loss', color='blue')\n",
        "        plt.title('Motion Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Plot Lidar Loss\n",
        "        plt.subplot(223)\n",
        "        if 'epoch/lidar_loss' in self.metrics:\n",
        "            plt.plot(self.metrics['epoch/lidar_loss'], label='Lidar Loss', color='green')\n",
        "        plt.title('Lidar Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Plot Total Loss\n",
        "        plt.subplot(224)\n",
        "        if 'epoch/total_loss' in self.metrics:\n",
        "            plt.plot(self.metrics['epoch/total_loss'], label='Total Loss', color='purple')\n",
        "        plt.title('Total Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Adjust x-axis for all subplots\n",
        "        for subplot in [221, 222, 223, 224]:\n",
        "            plt.subplot(subplot)\n",
        "            plt.xlim(0, len(self.metrics['epoch/total_loss']) - 1)  # Set x-axis from 0 to number of epochs - 1\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Convert to JPEG and display\n",
        "        jpeg_output = embed_matplotlib_jpeg(fig)\n",
        "        plt.close(fig)\n",
        "        display(jpeg_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MaABlkau6Af"
      },
      "source": [
        "## Training Loop Architecture\n",
        "\n",
        "The training loop operates in epochs, with each epoch processing the full dataset. Here's what happens in each epoch:\n",
        "\n",
        "1. **Batch Processing**\n",
        "   - Loads a batch of image sequences with metadata\n",
        "   - Skips invalid batches\n",
        "   - Moves data to GPU if available\n",
        "\n",
        "2. **Model Forward Pass**\n",
        "   - Processes images through RAFT model\n",
        "   - Computes motion fields and height estimates\n",
        "   - Uses automatic mixed precision for efficiency\n",
        "\n",
        "3. **Loss Calculation**\n",
        "   - Computes combined loss including:\n",
        "     - Motion consistency\n",
        "     - LiDAR supervision\n",
        "     - Height estimation error\n",
        "   - Checks for NaN/Inf values\n",
        "\n",
        "4. **Optimization Step**\n",
        "   - Computes gradients\n",
        "   - Clips gradients at 0.5 norm\n",
        "   - Updates model parameters\n",
        "   - Cleans GPU memory\n",
        "\n",
        "5. **Monitoring**\n",
        "   - Tracks multiple loss components\n",
        "   - Prints progress every 20 batches\n",
        "   - Visualizes first batch results\n",
        "   - Logs metrics and learning rate\n",
        "\n",
        "6. **End of Epoch**\n",
        "   - Computes average losses\n",
        "   - Updates learning rate based on performance\n",
        "   - Creates visualization plots\n",
        "   - Saves checkpoint if loss improved\n",
        "\n",
        "The loop includes error handling for out-of-memory issues and careful GPU memory management throughout due to RAFT's voracious memory appetite.\n",
        "\n",
        "### Model Setup\n",
        "\\begin{align*}\n",
        "\\text{Model} &= \\text{CombinedModel}(\\text{RAFT}) \\\\\n",
        "\\text{Loss} &= \\text{CombinedLoss}(\\lambda_{smooth}) \\\\\n",
        "\\text{Optimizer} &= \\text{AdamW}(lr=\\alpha) \\\\\n",
        "\\text{Scheduler} &= \\text{ReduceLROnPlateau}(\\text{factor}=0.5, \\text{patience}=5)\n",
        "\\end{align*}\n",
        "\n",
        "### Per-Epoch Training\n",
        "For each epoch $e$:\n",
        "\n",
        "1. Forward Pass:\n",
        "   \\begin{align*}\n",
        "   \\text{outputs} &= \\text{Model}(I, M, h_{lidar}, l) \\\\\n",
        "   L, L_{dict} &= \\text{Loss}(\\text{outputs}, I, l, h_{lidar}, M)\n",
        "   \\end{align*}\n",
        "\n",
        "   where:\n",
        "   - $I$: image sequence\n",
        "   - $M$: metadata\n",
        "   - $h_{lidar}$: validation height\n",
        "   - $l$: sequence lengths\n",
        "\n",
        "2. Backward Pass:\n",
        "   \\begin{align*}\n",
        "   \\nabla L &= \\text{backward}(L) \\\\\n",
        "   \\|\\nabla\\| &\\leq 0.5 \\text{ (gradient clipping)}\n",
        "   \\end{align*}\n",
        "\n",
        "3. Loss Tracking:\n",
        "   $$\\bar{L}_e = \\frac{1}{N}\\sum_{i=1}^N L_i$$\n",
        "\n",
        "4. Learning Rate Update:\n",
        "   $$\n",
        "   \\alpha_{e+1} = \\begin{cases}\n",
        "   0.5\\alpha_e & \\text{if no improvement for 5 epochs} \\\\\n",
        "   \\alpha_e & \\text{otherwise}\n",
        "   \\end{cases}\n",
        "   $$\n",
        "\n",
        "### Checkpointing\n",
        "Saves model when:\n",
        "$$L_e < \\min_{i<e} L_i \\text{ or } e \\bmod f = 0$$\n",
        "where $f$ is save frequency.\n",
        "\n",
        "### Features\n",
        "- Automatic mixed precision training\n",
        "- GPU memory management\n",
        "- Gradient anomaly detection\n",
        "- Visualization every epoch\n",
        "- Progress logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgGsGFw-a6q6"
      },
      "outputs": [],
      "source": [
        "def train_cloud_motion_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    num_epochs=10,\n",
        "    device=None,\n",
        "    learning_rate=1e-6,\n",
        "    smoothness_weight=0.01,\n",
        "    save_frequency=1,\n",
        "    max_sequence_length=5,\n",
        "    gradient_checkpointing=False):\n",
        "    \"\"\"\n",
        "    Train a cloud motion model for height estimation.\n",
        "\n",
        "    Core components:\n",
        "    1. Combined model wrapping RAFT\n",
        "    2. Combined loss with motion and LiDAR supervision\n",
        "    3. AdamW optimizer with LR scheduling\n",
        "    4. Training visualizations and checkpointing\n",
        "\n",
        "    Args:\n",
        "        model: Base RAFT model to train\n",
        "        train_loader: DataLoader with cloud image sequences\n",
        "        num_epochs (int): Number of training epochs\n",
        "        device: Torch device for training\n",
        "        learning_rate (float): Initial learning rate\n",
        "        smoothness_weight (float): Weight for motion smoothness loss\n",
        "        save_frequency (int): Save checkpoint every N epochs\n",
        "        max_sequence_length (int): Maximum sequence length\n",
        "        gradient_checkpointing (bool): Enable gradient checkpointing\n",
        "\n",
        "    Returns:\n",
        "        tuple: (trained_model, logger)\n",
        "            - trained_model: Trained CombinedModel instance\n",
        "            - logger: TrainingLogger with metrics history\n",
        "\n",
        "    Features:\n",
        "        - Mixed precision training\n",
        "        - GPU memory management\n",
        "        - Gradient anomaly detection\n",
        "        - Progress visualization\n",
        "        - Checkpoint saving\n",
        "        - Error handling for OOM\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Training on device: {device}\")\n",
        "    print_gpu_memory(\"Before moving model to device\")\n",
        "\n",
        "    combined_model = CombinedModel(model)\n",
        "\n",
        "    if gradient_checkpointing and hasattr(combined_model.raft, 'gradient_checkpointing_enable'):\n",
        "        combined_model.raft.gradient_checkpointing_enable()\n",
        "        print(\"Enabled gradient checkpointing for RAFT\")\n",
        "\n",
        "    combined_model = combined_model.to(device)\n",
        "    print_gpu_memory(\"After moving model to device\")\n",
        "\n",
        "    combined_loss = CombinedLoss(smoothness_weight=smoothness_weight).to(device)\n",
        "    logger = TrainingLogger()\n",
        "\n",
        "    optimizer = AdamW([\n",
        "        {'params': combined_model.raft.parameters(), 'lr': learning_rate}\n",
        "    ])\n",
        "\n",
        "    scheduler = ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        verbose=True,\n",
        "        min_lr=1e-8\n",
        "    )\n",
        "\n",
        "    print_gpu_memory(\"Before starting training loop\")\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    try:\n",
        "        for epoch in range(num_epochs):\n",
        "            logger.current_epoch = epoch\n",
        "            combined_model.train()\n",
        "            epoch_losses = defaultdict(float)\n",
        "            avg_loss_this_epoch = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "            for batch_idx, batch_data in enumerate(train_loader):\n",
        "                # Skip invalid batches\n",
        "                if batch_data is None:\n",
        "                    continue\n",
        "\n",
        "                print_gpu_memory(f\"Start of batch {batch_idx}\")\n",
        "\n",
        "                images = batch_data['images'].to(device)\n",
        "                metadata = batch_data['metadata'].to(device)\n",
        "                sequence_lengths = batch_data['sequence_lengths'].to(device)\n",
        "                validation_height = batch_data['validation_height'].to(device)\n",
        "\n",
        "                try:\n",
        "                    with autocast(device.type):\n",
        "                        outputs = combined_model(\n",
        "                            images=images,\n",
        "                            metadata=metadata,\n",
        "                            validation_height=validation_height,\n",
        "                            sequence_lengths=sequence_lengths\n",
        "                        )\n",
        "                        loss, loss_dict = combined_loss(\n",
        "                            outputs,\n",
        "                            images,\n",
        "                            sequence_lengths,\n",
        "                            validation_height=validation_height,\n",
        "                            metadata=metadata\n",
        "                        )\n",
        "\n",
        "                    for loss_type, loss_value in loss_dict.items():\n",
        "                        if isinstance(loss_value, torch.Tensor):\n",
        "                            if torch.isnan(loss_value) or torch.isinf(loss_value):\n",
        "                                print(f\"Warning: {loss_type} is {loss_value}\")\n",
        "\n",
        "                    optimizer.zero_grad(set_to_none=True)\n",
        "                    loss.backward()\n",
        "\n",
        "                    for name, param in combined_model.named_parameters():\n",
        "                        if param.grad is not None:\n",
        "                            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
        "                                print(f\"Warning: NaN or Inf gradients in {name}\")\n",
        "\n",
        "                    torch.nn.utils.clip_grad_norm_(\n",
        "                        combined_model.raft.parameters(),\n",
        "                        max_norm=0.5\n",
        "                    )\n",
        "\n",
        "                    optimizer.step()\n",
        "\n",
        "                    for k, v in loss_dict.items():\n",
        "                        if isinstance(v, torch.Tensor):\n",
        "                            epoch_losses[k] += v.item()\n",
        "                        else:\n",
        "                            epoch_losses[k] += float(v)\n",
        "\n",
        "                    num_batches += 1\n",
        "                    avg_loss_this_epoch = sum(epoch_losses.values()) / num_batches\n",
        "\n",
        "                    current_lr = optimizer.param_groups[0]['lr']\n",
        "                    avg_losses = {k: v / (batch_idx + 1) for k, v in epoch_losses.items()}\n",
        "                    if batch_idx % 20 == 0:\n",
        "                        print(f\"Batch {batch_idx+1}/{len(train_loader)}, Epoch {epoch+1}/{num_epochs}, Loss: {avg_losses['total_loss']:.4f}, LR: {current_lr:.2e}\")\n",
        "\n",
        "                    if batch_idx == 0:\n",
        "                        print(f\"\\nVisualizing batch {batch_idx+1}/{len(train_loader)} of epoch {epoch+1}/{num_epochs}\")\n",
        "                        with torch.no_grad():\n",
        "                            visualize_training_progress(\n",
        "                                outputs,\n",
        "                                batch_data,\n",
        "                                train_cloud2cloud_dataset,\n",
        "                                model=combined_model,\n",
        "                                sample_idx=0\n",
        "                            )\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    if \"out of memory\" in str(e):\n",
        "                        print_oom_report(batch_idx, images, sequence_lengths, combined_model)\n",
        "                        torch.cuda.empty_cache()\n",
        "                    else:\n",
        "                        raise e\n",
        "                finally:\n",
        "                    if 'outputs' in locals(): del outputs\n",
        "                    if 'images' in locals(): del images\n",
        "                    if 'metadata' in locals(): del metadata\n",
        "                    if 'loss' in locals(): del loss\n",
        "                    if 'validation_height' in locals(): del validation_height\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            print(f\"End of epoch {epoch+1}\")\n",
        "            print_gpu_memory(f\"End of epoch {epoch+1}\")\n",
        "\n",
        "            avg_epoch_loss = sum(epoch_losses.values()) / len(train_loader)\n",
        "            scheduler.step(avg_epoch_loss)\n",
        "\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            logger.log_metrics({'epoch_loss': avg_epoch_loss}, step_type='epoch')\n",
        "            logger.log_metrics({'learning_rate': current_lr}, step_type='epoch')\n",
        "            for loss_type, loss_value in epoch_losses.items():\n",
        "                logger.log_metrics({loss_type: loss_value / len(train_loader)}, step_type='epoch')\n",
        "\n",
        "            print(f\"Epoch {epoch+1} summary:\")\n",
        "            print(f\"  Average loss: {avg_epoch_loss:.4f}\")\n",
        "            print(f\"  Learning rate: {current_lr:.2e}\")\n",
        "\n",
        "            if epoch >= 1:\n",
        "                logger.plot_training_progress()\n",
        "\n",
        "            if avg_epoch_loss < best_loss:\n",
        "                best_loss = avg_epoch_loss\n",
        "                logger.save_checkpoint(\n",
        "                    model=model,\n",
        "                    optimizer=optimizer,\n",
        "                    scheduler=scheduler,\n",
        "                    additional_info={\n",
        "                        'epoch_loss': avg_epoch_loss,\n",
        "                        'best_model': True,\n",
        "                        'height_calculator_state': combined_model.get_height_calculator_states()\n",
        "                    }\n",
        "                )\n",
        "            elif (epoch + 1) % save_frequency == 0:\n",
        "                logger.save_checkpoint(\n",
        "                    model=model,\n",
        "                    optimizer=optimizer,\n",
        "                    scheduler=scheduler,\n",
        "                    additional_info={\n",
        "                        'epoch_loss': avg_epoch_loss,\n",
        "                        'height_calculator_state': combined_model.get_height_calculator_states()\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {str(e)}\")\n",
        "        print_gpu_memory(\"At error\")\n",
        "        raise e\n",
        "\n",
        "    return combined_model, logger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FomRxCMku-4Y"
      },
      "source": [
        "## Collation and Packing\n",
        "\n",
        "## Purpose of Collation\n",
        "When batching sequences for training, we need to handle:\n",
        "1. Variable-length sequences (2-5 frames)\n",
        "2. Missing/invalid data\n",
        "3. Multiple aligned data streams (images, flight data, timestamps)\n",
        "4. Memory efficiency\n",
        "5. RNN processing requirements\n",
        "\n",
        "### 1. Initial Data Unpacking\n",
        "```\n",
        "Input Batch Structure:\n",
        "- image_sequences: (B × T × C × H × W)\n",
        "- flight_data_list: (B × T × D_flight)\n",
        "- validation_heights: (B × 1)\n",
        "- resized_center_coords: (B × 2)\n",
        "- image_paths: (B × T)\n",
        "- timestamp_sequences: (B × T)\n",
        "```\n",
        "\n",
        "### 2. Sequence Validation and Truncation\n",
        "For each sequence:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\text{Valid Index Set} = {i : \\text{GPS_MSL_Alt}_i \\neq \\text{NaN}} \\\\\n",
        "& \\text{last_valid_idx} = \\max(\\text{Valid Index Set}) \\\\\n",
        "& \\text{start_idx} = \\max(0, \\text{last_valid_idx} - \\text{max_sequence_length} + 1) \\\\\n",
        "& \\text{sequence_length} = \\min(\\text{end_idx} - \\text{start_idx}, \\text{max_sequence_length})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "### 3. Sequence Length Requirements\n",
        "```\n",
        "Constraints:\n",
        "2 ≤ sequence_length ≤ 5\n",
        "```\n",
        "\n",
        "- Minimum 2 frames needed for temporal patterns\n",
        "- Maximum 5 frames for computational efficiency\n",
        "- Must have valid GPS altitude data\n",
        "\n",
        "### 4. Sequence Sorting and Packing\n",
        "\n",
        "#### Why Pack Sequences?\n",
        "1. **Memory Efficiency**\n",
        "   - No need to pad all sequences to maximum length\n",
        "   - Only stores actual data points\n",
        "\n",
        "2. **Computational Efficiency**\n",
        "   - RNN computations only on valid timesteps\n",
        "   - Avoids processing padding tokens\n",
        "\n",
        "3. **Better Gradient Flow**\n",
        "   - No gradients through padding\n",
        "   - More stable training\n",
        "\n",
        "#### Packing Process:\n",
        "1. Sort by length (descending):\n",
        "   $$\\text{sequences} = \\text{sort}(\\text{sequences}, \\text{key}=\\text{len}, \\text{reverse}=\\text{True})$$\n",
        "\n",
        "2. Pack sequences:\n",
        "   ```\n",
        "   PackedSequence Structure:\n",
        "   - data: All sequence elements concatenated\n",
        "   - batch_sizes: Number of sequences at each timestep\n",
        "   - sorted_indices: Original sorting order\n",
        "   - unsorted_indices: Restore original order\n",
        "   ```\n",
        "\n",
        "Example:\n",
        "```\n",
        "Original Sequences:\n",
        "Seq1: [A1, A2, A3]\n",
        "Seq2: [B1, B2]\n",
        "Seq3: [C1, C2, C3, C4]\n",
        "\n",
        "Sorted:\n",
        "C1, C2, C3, C4\n",
        "A1, A2, A3, --\n",
        "B1, B2, --, --\n",
        "\n",
        "Packed:\n",
        "data: [C1, A1, B1, C2, A2, B2, C3, A3, C4]\n",
        "batch_sizes: [3, 3, 2, 1]\n",
        "```\n",
        "\n",
        "### 5. Final Output Structure\n",
        "```python\n",
        "{\n",
        "    'images': PackedSequence,      # Packed image sequences\n",
        "    'metadata': PackedSequence,    # Packed flight data\n",
        "    'validation_height': Tensor,   # Single height per sequence\n",
        "    'center_coords': Tensor,       # Coordinate pairs\n",
        "    'sequence_lengths': Tensor,    # Length of each sequence\n",
        "    'image_paths': List,          # For debugging/visualization\n",
        "    'timestamps': List            # Temporal alignment\n",
        "}\n",
        "```\n",
        "\n",
        "This structure ensures:\n",
        "1. Memory efficient storage\n",
        "2. Aligned multimodal data\n",
        "3. Efficient RNN processing\n",
        "4. Easy access to metadata\n",
        "5. Debugging capability\n",
        "\n",
        "# Sequence Truncation Strategy\n",
        "\n",
        "## Problem Statement\n",
        "Two main issues with sequence lengths:\n",
        "1. **Memory Issues**\n",
        "   - Long sequences (5+ frames) cause Out-of-Memory (OOM) errors\n",
        "   - Each image is 384x384x3, consuming significant GPU memory when batched\n",
        "\n",
        "2. **Data Quality Issues**\n",
        "   - Some sequences are abnormally long (1000+ frames)\n",
        "   - Usually indicates data collection or synchronization problems\n",
        "   - Can be caused by:\n",
        "     * Missing LiDAR measurements\n",
        "     * GPS data gaps\n",
        "     * Timestamp misalignment\n",
        "\n",
        "## Truncation Implementation\n",
        "\n",
        "### 1. Valid Data Detection\n",
        "```\n",
        "For each sequence:\n",
        "1. Check GPS_MSL_Alt for NaN values\n",
        "2. Find last valid index where data exists\n",
        "3. If no valid data found, skip sequence\n",
        "```\n",
        "\n",
        "### 2. Sequence Windowing\n",
        "```\n",
        "start_idx = max(0, last_valid_idx - max_sequence_length + 1)\n",
        "end_idx = last_valid_idx + 1\n",
        "```\n",
        "\n",
        "Key points:\n",
        "- Always keeps the most recent valid frames\n",
        "- Works backward from last valid measurement\n",
        "- Ensures LiDAR measurement aligns with final frame\n",
        "\n",
        "### 3. Length Constraints\n",
        "```\n",
        "min_sequence_length = 2  # Minimum for temporal patterns\n",
        "max_sequence_length = 5  # Maximum for memory management\n",
        "```\n",
        "\n",
        "Benefits:\n",
        "- Prevents OOM errors\n",
        "- Ensures consistent batch sizes\n",
        "- Maintains temporal coherence\n",
        "- Filters out problematic sequences\n",
        "\n",
        "## Example Scenarios\n",
        "\n",
        "1. **Normal Case**:\n",
        "   ```\n",
        "   Original: [F1, F2, F3, F4, F5, F6, F7, F8]  // 8 frames\n",
        "   Last valid: F8\n",
        "   Truncated: [F4, F5, F6, F7, F8]  // 5 frames\n",
        "   ```\n",
        "\n",
        "2. **Bad Data Case**:\n",
        "   ```\n",
        "   Original: [F1, F2, ..., F1000]  // 1000 frames\n",
        "   Last valid: F1000\n",
        "   Truncated: [F996, F997, F998, F999, F1000]  // 5 frames\n",
        "   ```\n",
        "\n",
        "3. **Short Sequence**:\n",
        "   ```\n",
        "   Original: [F1, F2, F3]  // 3 frames\n",
        "   Last valid: F3\n",
        "   Result: [F1, F2, F3]  // Kept as is\n",
        "   ```\n",
        "\n",
        "4. **Invalid Sequence**:\n",
        "   ```\n",
        "   Original: [F1(NaN), F2(NaN), F3(NaN)]  // All invalid\n",
        "   Result: Sequence discarded\n",
        "   ```\n",
        "\n",
        "This truncation strategy effectively:\n",
        "- Manages memory usage\n",
        "- Maintains data quality\n",
        "- Preserves temporal relationships\n",
        "- Handles edge cases gracefully"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3Jt3F62o75I"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, max_sequence_length=5, min_sequence_length=2):\n",
        "    \"\"\"\n",
        "    Custom collate function for cloud image sequences that handles variable lengths and maintains data alignment.\n",
        "\n",
        "    Key operations:\n",
        "    1. Truncates sequences to most recent valid frames\n",
        "    2. Filters sequences based on length requirements\n",
        "    3. Sorts by sequence length for packed sequence efficiency\n",
        "    4. Packs sequences for RNN processing\n",
        "\n",
        "    Args:\n",
        "        batch: List of tuples containing:\n",
        "            - image_sequences: List of image tensors [T, C, H, W]\n",
        "            - flight_data: Flight metadata [T, D]\n",
        "            - validation_heights: LiDAR measurements\n",
        "            - resized_center_coords: Image center points\n",
        "            - image_paths: Paths to source images\n",
        "            - timestamp_sequences: Frame timestamps\n",
        "        max_sequence_length (int): Maximum frames to keep (default: 5)\n",
        "        min_sequence_length (int): Minimum frames required (default: 2)\n",
        "\n",
        "    Returns:\n",
        "        dict: Collated batch with:\n",
        "            - images: PackedSequence of image tensors\n",
        "            - metadata: PackedSequence of flight data\n",
        "            - validation_height: LiDAR heights tensor\n",
        "            - center_coords: Center coordinates tensor\n",
        "            - sequence_lengths: Sequence length tensor\n",
        "            - image_paths: Source image paths\n",
        "            - timestamps: Frame timestamps\n",
        "\n",
        "        Returns None if no valid sequences found.\n",
        "\n",
        "    Note:\n",
        "        Sequences are truncated from the end, keeping the most recent\n",
        "        frames that have valid metadata (non-NaN GPS altitude).\n",
        "    \"\"\"\n",
        "    metadata_manager = FlightMetadataManager()\n",
        "\n",
        "    # Unpack with timestamps\n",
        "    image_sequences, flight_data_list, validation_heights, resized_center_coords_list, image_paths_list, timestamp_sequences = zip(*batch)\n",
        "\n",
        "    # Truncate sequences, keeping the most recent valid frames\n",
        "    truncated_sequences = []  # Will store tuples of (images, flight_data, image_paths, timestamps)\n",
        "    sequence_lengths = []\n",
        "    valid_indices = []  # Track which sequences we're keeping\n",
        "\n",
        "    for idx, (imgs, flight_data, image_paths, timestamps) in enumerate(zip(image_sequences, flight_data_list, image_paths_list, timestamp_sequences)):\n",
        "        # Find last valid index (non-NaN in metadata)\n",
        "        valid_metadata_indices = torch.where(~torch.isnan(flight_data[:, metadata_manager.get_index('GPS_MSL_Alt')]))[0]\n",
        "        if len(valid_metadata_indices) == 0:\n",
        "            last_valid_idx = -1\n",
        "        else:\n",
        "            last_valid_idx = valid_metadata_indices[-1].item()\n",
        "\n",
        "        # Truncate to last max_sequence_length valid frames\n",
        "        start_idx = max(0, last_valid_idx - max_sequence_length + 1)\n",
        "        end_idx = last_valid_idx + 1\n",
        "\n",
        "        # Only include sequences that meet the minimum length requirement\n",
        "        sequence_length = min(end_idx - start_idx, max_sequence_length)\n",
        "        if sequence_length >= min_sequence_length:\n",
        "            # Store truncated sequences together to maintain alignment\n",
        "            truncated_sequences.append((\n",
        "                imgs[start_idx:end_idx],\n",
        "                flight_data[start_idx:end_idx],\n",
        "                image_paths[start_idx:end_idx],\n",
        "                timestamps[start_idx:end_idx]  # Add timestamps\n",
        "            ))\n",
        "            sequence_lengths.append(sequence_length)\n",
        "            valid_indices.append(idx)\n",
        "\n",
        "    # If no valid sequences, return None or raise an error\n",
        "    if not truncated_sequences:\n",
        "        return None  # Or raise ValueError(\"No sequences meet the minimum length requirement\")\n",
        "\n",
        "    # Sort by length in descending order\n",
        "    sorted_indices = np.argsort(sequence_lengths)[::-1]\n",
        "    sequence_lengths = [sequence_lengths[i] for i in sorted_indices]\n",
        "    truncated_sequences = [truncated_sequences[i] for i in sorted_indices]\n",
        "    # Only include validation heights and center coords for valid sequences\n",
        "    validation_heights = [validation_heights[valid_indices[i]] for i in sorted_indices]\n",
        "    resized_center_coords = [resized_center_coords_list[valid_indices[i]] for i in sorted_indices]\n",
        "\n",
        "    # Separate aligned sequences\n",
        "    sorted_images, sorted_flight_data, sorted_image_paths, sorted_timestamps = zip(*truncated_sequences)\n",
        "\n",
        "    # Pack sequences\n",
        "    packed_images = pack_sequence([seq for seq in sorted_images])\n",
        "    packed_metadata = pack_sequence([torch.as_tensor(data, dtype=torch.float32)\n",
        "                                   for data in sorted_flight_data])\n",
        "\n",
        "    return {\n",
        "        'images': packed_images,\n",
        "        'metadata': packed_metadata,\n",
        "        'validation_height': torch.as_tensor(validation_heights, dtype=torch.float32),\n",
        "        'center_coords': torch.as_tensor(resized_center_coords, dtype=torch.long),\n",
        "        'sequence_lengths': torch.tensor(sequence_lengths),\n",
        "        'image_paths': sorted_image_paths,\n",
        "        'timestamps': sorted_timestamps  # Add timestamps to output\n",
        "    }\n",
        "\n",
        "# Wrapper method to add extra parameters to the collate function as well as filter out sequences that are too short\n",
        "def filter_collate_fn(batch):\n",
        "    collated = collate_fn(batch, max_sequence_length=5, min_sequence_length=2)\n",
        "    if collated is None:\n",
        "        print(\"Skipping batch with no valid sequences\")\n",
        "        return None\n",
        "    return collated\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_cloud2cloud_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    collate_fn=filter_collate_fn,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "if train_model:\n",
        "  # Train with memory tracking\n",
        "  trained_model, logger = train_cloud_motion_model(\n",
        "      model=model,\n",
        "      train_loader=train_loader,\n",
        "      num_epochs=num_epochs,\n",
        "      device=device,\n",
        "      learning_rate=1e-8,\n",
        "      save_frequency=2\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFzneWody0j1"
      },
      "source": [
        "## Load Model (If not training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdLa7ZrSy0GY"
      },
      "outputs": [],
      "source": [
        "def load_model_checkpoint(checkpoint_path, device='cuda'):\n",
        "    \"\"\"\n",
        "    Load a saved model checkpoint.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path: Path to the checkpoint file\n",
        "        device: Device to load model to ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        combined_model: Loaded CombinedModel instance\n",
        "        epoch: Epoch number when checkpoint was saved\n",
        "    \"\"\"\n",
        "\n",
        "    # # Add numpy scalar to safe globals\n",
        "    # add_safe_globals([\n",
        "    #     np.core.multiarray.scalar,\n",
        "    #     np.dtype,\n",
        "    #     np.dtypes.Float64DType\n",
        "    # ])\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
        "\n",
        "    # Print what's saved in the checkpoint\n",
        "    print(\"Checkpoint contains:\", checkpoint.keys())\n",
        "\n",
        "    # Initialize models\n",
        "    motion_model = CloudMotionModel()\n",
        "    combined_model = CombinedModel(motion_model)\n",
        "    combined_model = combined_model.to(device)\n",
        "\n",
        "    # Load states\n",
        "    combined_model.raft.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if 'height_calculator_state' in checkpoint:\n",
        "        combined_model.load_height_calculator_states(\n",
        "            checkpoint['height_calculator_state']\n",
        "        )\n",
        "\n",
        "    return combined_model, checkpoint['epoch']\n",
        "\n",
        "if not train_model:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Loading model on device: {device}\")\n",
        "    trained_model, epoch = load_model_checkpoint(\n",
        "        '/content/drive/MyDrive/motion_maps/motion_model_epoch_2.pt',\n",
        "        device=device\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzT72cgXUZzz"
      },
      "source": [
        "# Post training Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visualize_multiple_sequences"
      ],
      "metadata": {
        "id": "sAvOW22d038-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdCvi8ebiSWe"
      },
      "outputs": [],
      "source": [
        "def visualize_multiple_sequences(model, dataset, num_sequences=5, randomize=True,\n",
        "                               starting_sequence_index=0, num_sequences_to_skip=1):\n",
        "    \"\"\"\n",
        "    Generate visualizations for multiple sequences of cloud height estimation results.\n",
        "\n",
        "    Creates comprehensive visualizations showing:\n",
        "    1. Motion field estimation and comparison\n",
        "    2. Height field reconstruction with uncertainty\n",
        "    3. Calibration parameter evolution\n",
        "\n",
        "    Args:\n",
        "        model: Trained CloudMotionModel\n",
        "        dataset: CloudDataset instance\n",
        "        num_sequences (int): Number of sequences to visualize\n",
        "        randomize (bool): If True, randomly sample sequences\n",
        "                          If False, use consecutive sequences\n",
        "        starting_sequence_index (int): First sequence index if not random\n",
        "        num_sequences_to_skip (int): Stride between sequences if not random\n",
        "\n",
        "    Visualization components per sequence:\n",
        "        - Original image sequence\n",
        "        - Motion field comparison (refined vs original RAFT)\n",
        "        - Height field reconstruction with uncertainty\n",
        "        - Diagnostic statistics\n",
        "        - Calibration parameters\n",
        "\n",
        "    Note:\n",
        "        Handles GPU memory management and matplotlib cleanup.\n",
        "        Catches and reports visualization errors per sequence.\n",
        "        Maintains model in eval mode during visualization.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Get sequence indices to visualize\n",
        "    if randomize:\n",
        "        sequence_indices = np.random.choice(\n",
        "            len(dataset),\n",
        "            size=min(num_sequences, len(dataset)),\n",
        "            replace=False\n",
        "        )\n",
        "    else:\n",
        "        end_idx = min(\n",
        "            starting_sequence_index + (num_sequences * num_sequences_to_skip),\n",
        "            len(dataset)\n",
        "        )\n",
        "        sequence_indices = range(\n",
        "            starting_sequence_index,\n",
        "            end_idx,\n",
        "            num_sequences_to_skip\n",
        "        )\n",
        "\n",
        "    # Create a temporary DataLoader for single sequences\n",
        "    temp_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=1,\n",
        "        collate_fn=lambda b: collate_fn(b, max_sequence_length=5)\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq_idx, idx in enumerate(sequence_indices):\n",
        "            try:\n",
        "                # Get the sequence\n",
        "                batch_data = temp_loader.dataset[idx]\n",
        "                batch = collate_fn([batch_data])\n",
        "\n",
        "                # Move data to device\n",
        "                images = batch['images'].to(device)\n",
        "                metadata = batch['metadata'].to(device)\n",
        "                sequence_lengths = batch['sequence_lengths'].to(device)\n",
        "                validation_height = batch['validation_height'].to(device)\n",
        "\n",
        "                # Get model outputs\n",
        "                outputs = model(\n",
        "                    images=images,\n",
        "                    metadata=metadata,\n",
        "                    validation_height=validation_height,\n",
        "                    sequence_lengths=sequence_lengths\n",
        "                )\n",
        "\n",
        "                print(f\"\\nSequence {seq_idx + 1}/{len(sequence_indices)} (Dataset index: {idx})\")\n",
        "                print(f\"Sequence length: {sequence_lengths[0].item()}\")\n",
        "                if 'image_paths' in batch:\n",
        "                    print(\"\\nImage paths:\")\n",
        "                    for path in batch['image_paths'][0]:\n",
        "                        print(f\"  {path}\")\n",
        "\n",
        "                # Get and explicitly display visualizations\n",
        "                viz_outputs = visualize_training_progress(\n",
        "                    outputs=outputs,\n",
        "                    batch_data=batch,\n",
        "                    dataset=dataset,\n",
        "                    model=model\n",
        "                )\n",
        "\n",
        "                # Clear any remaining matplotlib state\n",
        "                plt.close('all')\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error visualizing sequence {idx}: {str(e)}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "            finally:\n",
        "                # Ensure cleanup\n",
        "                plt.close('all')\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d7u4mCEiX11"
      },
      "outputs": [],
      "source": [
        "# Visualize random sequences\n",
        "visualize_multiple_sequences(\n",
        "    model=trained_model,\n",
        "    dataset=full_cloud2cloud_dataset,\n",
        "    num_sequences=1,\n",
        "    randomize=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITzc6EHHcOtC"
      },
      "outputs": [],
      "source": [
        "# Visualize consecutive sequences\n",
        "visualize_multiple_sequences(\n",
        "    model=trained_model,\n",
        "    dataset=full_cloud2cloud_dataset,\n",
        "    num_sequences=1,\n",
        "    randomize=False,\n",
        "    starting_sequence_index=0,\n",
        "    num_sequences_to_skip=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u2c-iug6MQl"
      },
      "source": [
        "# Stitching Process\n",
        "\n",
        "## Data Collection and Preprocessing\n",
        "\n",
        "`collect_height_fields` processes sequences to obtain:\n",
        "   - Height fields\n",
        "   - Confidence maps\n",
        "   - Flight metadata\n",
        "\n",
        "A constraint of `collect_height_fields` is that it requires temporal ordering; obviously to stitch height fields together, it's necessary that sequences maintain their chronological order.\n",
        "\n",
        "However, due to varying sequence lengths, the default collation process sorts sequences by length for efficient packing (longer sequences first). While this optimizes memory usage, it disrupts temporal order.\n",
        "\n",
        "By using batch_size=1:\n",
        "1. Each sequence stays in chronological order\n",
        "2. We can track motion evolution over time\n",
        "3. Height fields align with their timestamps\n",
        "\n",
        "The tradeoff:\n",
        "- Pros: Simpler code, preserved temporal order\n",
        "- Cons: Must process sequences one at a time, slower execution\n",
        "\n",
        "This can be seen in the DataLoader `stitch_loader` internal to the function which enforces the batch size of 1.\n",
        "\n",
        "```python\n",
        "# Creates batch_size=1 loader to maintain temporal order\n",
        "stitch_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,  # Needed for temporal ordering\n",
        "    shuffle=False,\n",
        "    collate_fn=filter_collate_fn,\n",
        "    num_workers=1,\n",
        "    pin_memory=True\n",
        ")\n",
        "```\n",
        "\n",
        "## Global Coordinate System Creation\n",
        "\n",
        "The `create_global_grid` function creates a unified coordinate system to stitch together multiple cloud height fields:\n",
        "\n",
        "First, it uses the haversine formula to calculate true ground distances between measurement points. Taking the first measurement as a reference point, it calculates how far each subsequent measurement is in meters (north-south and east-west).\n",
        "\n",
        "These real-world distances are converted to pixel coordinates using a scale factor (0.05 pixels per meter). This creates a grid where each height field's position corresponds to its actual geographic location relative to others.\n",
        "\n",
        "The function then finds the total bounds needed to contain all height fields by looking at their positions and dimensions. It shifts everything so the minimum x and y coordinates start at zero, adjusting all positions and measurement points accordingly.\n",
        "\n",
        "### Haversine Distance\n",
        "The haversine formula calculates great-circle distances between latitude/longitude points on a sphere (Earth):\n",
        "\n",
        "\\begin{align*}\n",
        "a &= \\sin^2(\\frac{\\Delta\\phi}{2}) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2(\\frac{\\Delta\\lambda}{2}) \\\\\n",
        "d &= 2R \\arcsin(\\sqrt{a})\n",
        "\\end{align*}\n",
        "\n",
        "where:\n",
        "- $\\phi$ is latitude\n",
        "- $\\lambda$ is longitude\n",
        "- $R$ is Earth's radius (6371km)\n",
        "\n",
        "### Grid Creation Process\n",
        "\n",
        "1. **Find Spatial Bounds**\n",
        "   \\begin{align*}\n",
        "   \\text{For each height field } H_i:& \\\\\n",
        "   dx_i &= \\text{haversine}(\\text{ref}_{\\text{lat}}, \\text{ref}_{\\text{lon}}, \\text{ref}_{\\text{lat}}, \\text{lon}_i) \\\\\n",
        "   dy_i &= \\text{haversine}(\\text{ref}_{\\text{lat}}, \\text{ref}_{\\text{lon}}, \\text{lat}_i, \\text{ref}_{\\text{lon}}) \\\\\n",
        "   \\text{pos}_i &= (dx_i \\cdot \\text{scale}, dy_i \\cdot \\text{scale})\n",
        "   \\end{align*}\n",
        "\n",
        "2. **Calculate Grid Dimensions**\n",
        "   \\begin{align*}\n",
        "   x_{\\text{min}} &= \\min_i(\\text{pos}_i^x) \\\\\n",
        "   x_{\\text{max}} &= \\max_i(\\text{pos}_i^x + w_i) \\\\\n",
        "   y_{\\text{min}} &= \\min_i(\\text{pos}_i^y) \\\\\n",
        "   y_{\\text{max}} &= \\max_i(\\text{pos}_i^y + h_i)\n",
        "   \\end{align*}\n",
        "\n",
        "   where $w_i, h_i$ are height field dimensions\n",
        "\n",
        "3. **Adjust Measurement Points**\n",
        "   $$\n",
        "   \\text{For each point } p:\n",
        "   \\begin{align*}\n",
        "   p_{\\text{adj}}^x &= p^x - x_{\\text{min}} \\\\\n",
        "   p_{\\text{adj}}^y &= p^y - y_{\\text{min}}\n",
        "   \\end{align*}\n",
        "   $$\n",
        "\n",
        "The `create_global_grid` function returns:\n",
        "- Grid dimensions\n",
        "- Adjusted positions of all height fields\n",
        "- Measurement point locations\n",
        "- Reference coordinates and scale\n",
        "- Relative positions for stitching\n",
        "\n",
        "This creates a pixel-space coordinate system where all height fields can be positioned relative to each other while maintaining their true geographical relationships.\n",
        "\n",
        "## Field Stitching\n",
        "\n",
        "`stitch_height_fields` combines fields using weighted averaging:\n",
        "\n",
        "1. Create output arrays:\n",
        "\n",
        "The process begins by creating arrays sized to encompass all height fields. Two main arrays track the sum of weighted heights and sum of weights separately. This dual-array approach allows proper handling of invalid regions marked by NaN values in the input fields.\n",
        "\n",
        "   \\begin{align*}\n",
        "   H_{\\text{sum}} &\\in \\mathbb{R}^{h \\times w} \\\\\n",
        "   W_{\\text{sum}} &\\in \\mathbb{R}^{h \\times w}\n",
        "   \\end{align*}\n",
        "\n",
        "2. Generate confidence mask for each valid region:\n",
        "\n",
        "The system generates confidence masks using a Gaussian falloff from measurement points. Starting with a base confidence of 0.2, confidence increases to 0.8 near measurement points. The σ parameter controls how quickly confidence decreases with distance from measured points.\n",
        "\n",
        "   $$C(x,y) = 0.2 + 0.6 \\exp(-\\frac{d^2}{2\\sigma^2})$$\n",
        "   where $d$ is distance from measurement points\n",
        "\n",
        "The values 0.2 and 0.6 in the confidence mask equation are chosen to create a bounded confidence range:\n",
        "- Minimum confidence: 0.2 far from measurements\n",
        "- Maximum confidence: 0.8 (0.2 + 0.6) at measurement points\n",
        "\n",
        "These specific values allow the weighting to:\n",
        "1. Never completely discard data (0.2 floor)\n",
        "2. Never fully trust a single measurement (0.8 ceiling)\n",
        "3. Provide enough dynamic range (0.6) for smooth blending\n",
        "\n",
        "The values represent a design choice balancing between measurement trust and field continuity.\n",
        "\n",
        "3. Add weighted contributions:\n",
        "\n",
        "Each height field is positioned in the global grid according to its geographic coordinates. The system calculates weights based on distance to measurement points and combines them with local uncertainty values. These weighted contributions accumulate in the running sums, naturally handling overlapping regions.\n",
        "\n",
        "   \\begin{align*}\n",
        "   H_{\\text{sum}}(x,y) &+= h(x,y) \\cdot w(x,y) \\cdot c(x,y) \\\\\n",
        "   W_{\\text{sum}}(x,y) &+= w(x,y) \\cdot c(x,y)\n",
        "   \\end{align*}\n",
        "\n",
        "4. Calculate final heights:\n",
        "   $$H_{\\text{final}}(x,y) = \\frac{H_{\\text{sum}}(x,y)}{W_{\\text{sum}}(x,y)}$$\n",
        "\n",
        "## Post-processing\n",
        "\n",
        "After combining fields, the system divides weighted sums to calculate final heights. It then identifies sharp discontinuities in the height field and applies selective smoothing in moderate confidence regions while preserving strong gradients where confidence is high.\n",
        "\n",
        "1. Detect and smooth discontinuities using Gaussian filter\n",
        "2. Preserve strong gradients in high-confidence regions\n",
        "3. Create uncertainty visualization overlay\n",
        "\n",
        "`stitch_height_fields` produces a stitched height field, comprehensive confidence map, and metadata package containing the sequences used, measurement points, geographic bounds, and reference coordinates.\n",
        "\n",
        "## Visualization\n",
        "\n",
        "Two complementary visualizations:\n",
        "\n",
        "1. `visualize_stitching_steps`:\n",
        "   - 2D overview\n",
        "   - LiDAR measurement points\n",
        "   - Data validity percentages\n",
        "   - Height ranges\n",
        "\n",
        "2. `visualize_stitched_height_field_3d`:\n",
        "   - Interactive 3D surface\n",
        "   - Uncertainty overlay\n",
        "   - Synchronized views\n",
        "   - Diagnostic statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXcgFJ9s6UUx"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VDbA064z9w5"
      },
      "outputs": [],
      "source": [
        "class AlignmentError(Exception):\n",
        "    \"\"\"Custom exception for alignment failures.\"\"\"\n",
        "    pass\n",
        "\n",
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"\n",
        "    Calculate great-circle distance between coordinates.\n",
        "\n",
        "    Uses haversine formula for spherical Earth approximation:\n",
        "    d = 2R * arcsin(sqrt(sin²(Δφ/2) + cos(φ₁)cos(φ₂)sin²(Δλ/2)))\n",
        "\n",
        "    Args:\n",
        "        lat1, lon1: First point coordinates in degrees\n",
        "        lat2, lon2: Second point coordinates in degrees\n",
        "\n",
        "    Returns:\n",
        "        float: Distance in meters\n",
        "    \"\"\"\n",
        "    R = 6371000  # Earth's radius in meters\n",
        "\n",
        "    # Convert to radians\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "\n",
        "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "\n",
        "    return R * c\n",
        "\n",
        "def gaussian_weight(dist, sigma=100):\n",
        "    \"\"\"\n",
        "    Calculate Gaussian weights based on distance.\n",
        "\n",
        "    Weight = exp(-0.5 * (dist/sigma)²)\n",
        "\n",
        "    Args:\n",
        "        dist (float): Distance from measurement point\n",
        "        sigma (float): Standard deviation controlling falloff rate\n",
        "\n",
        "    Returns:\n",
        "        float: Weight in [0,1], maximum at dist=0\n",
        "    \"\"\"\n",
        "    return np.exp(-0.5 * (dist / sigma)**2)\n",
        "\n",
        "def create_confidence_mask(height, width, measurement_points, max_dist=200):\n",
        "    \"\"\"\n",
        "    Generate confidence mask based on distance from measurements.\n",
        "\n",
        "    Creates mask with base confidence 0.2 increasing to 0.8 near\n",
        "    measurement points with Gaussian falloff.\n",
        "\n",
        "    Args:\n",
        "        height, width: Output dimensions\n",
        "        measurement_points: List of measurement locations\n",
        "        max_dist: Maximum influence distance\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Confidence values [0.2, 0.8]\n",
        "    \"\"\"\n",
        "    confidence = np.zeros((height, width))\n",
        "    y_grid, x_grid = np.mgrid[:height, :width]\n",
        "\n",
        "    for point in measurement_points:\n",
        "        # Calculate distance from this measurement point\n",
        "        dist_sq = ((x_grid - point['x'])**2 + (y_grid - point['y'])**2) / (max_dist**2)\n",
        "\n",
        "        # Add confidence contribution with gaussian falloff\n",
        "        point_conf = np.exp(-0.5 * dist_sq)\n",
        "        confidence = np.maximum(confidence, point_conf)\n",
        "\n",
        "    # Normalize confidence to [0.2, 0.8] range\n",
        "    confidence = 0.2 + 0.6 * confidence\n",
        "\n",
        "    return confidence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## collect_height_fields"
      ],
      "metadata": {
        "id": "dQ5HV0BX1REC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIEGq4Pr0mDc"
      },
      "outputs": [],
      "source": [
        "def collect_height_fields(trained_model, dataset, start_batch=0, max_batches=10):\n",
        "    \"\"\"\n",
        "    Collect height fields and metadata while preserving temporal order.\n",
        "\n",
        "    Uses batch_size=1 to maintain chronological sequence ordering, since\n",
        "    packed sequence sorting would otherwise disrupt temporal relationships.\n",
        "\n",
        "    Args:\n",
        "        trained_model: Trained cloud motion model\n",
        "        dataset: Source dataset\n",
        "        start_batch: First batch to process\n",
        "        max_batches: Maximum number of batches to process\n",
        "\n",
        "    Returns:\n",
        "        tuple: (height_fields, confidence_maps, metadata_list)\n",
        "            height_fields: List of height field arrays\n",
        "            confidence_maps: List of confidence value arrays\n",
        "            metadata_list: List of per-sequence metadata\n",
        "    \"\"\"\n",
        "    # Create stitching-specific loader with enforced batch_size=1\n",
        "    stitch_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        collate_fn=filter_collate_fn,\n",
        "        num_workers=1,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    height_fields = []\n",
        "    confidence_maps = []\n",
        "    metadata_list = []\n",
        "    metadata_manager = FlightMetadataManager()\n",
        "\n",
        "    print(\"\\nCollecting sequences:\")\n",
        "    sequence_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(stitch_loader):\n",
        "            if batch_idx < start_batch:\n",
        "                continue\n",
        "            if batch_idx >= start_batch + max_batches:\n",
        "                break\n",
        "            if batch_data is None:\n",
        "                continue\n",
        "\n",
        "            # Now we can safely assume batch_size=1\n",
        "            timestamp = batch_data['timestamps'][0]\n",
        "            print(f\"\\nSequence {sequence_count} (Batch {batch_idx}):\")\n",
        "            print(f\"  Start: {timestamp[0]}\")\n",
        "            print(f\"  End: {timestamp[-1]}\")\n",
        "\n",
        "            # Rest of processing remains the same\n",
        "            outputs = trained_model(\n",
        "                images=batch_data['images'].to(device),\n",
        "                metadata=batch_data['metadata'].to(device),\n",
        "                validation_height=batch_data['validation_height'].to(device),\n",
        "                sequence_lengths=batch_data['sequence_lengths'].to(device)\n",
        "            )\n",
        "\n",
        "            # Extract results (safe to use index 0)\n",
        "            height_fields.append(outputs['height_field_finetuned'][0].cpu().numpy())\n",
        "            confidence_maps.append(outputs['height_uncertainty_finetuned'][0].cpu().numpy())\n",
        "\n",
        "            # Build metadata for sequence\n",
        "            metadata_unpacked, lengths = torch.nn.utils.rnn.pad_packed_sequence(batch_data['metadata'])\n",
        "            seq_length = lengths[0]\n",
        "            seq_metadata = metadata_unpacked[:seq_length, 0].cpu().numpy()\n",
        "            center_x, center_y = batch_data['center_coords'][0]\n",
        "\n",
        "            seq_metadata_list = []\n",
        "            for frame_idx in range(seq_length):\n",
        "                frame_dict = {\n",
        "                    'timestamp': timestamp[frame_idx],\n",
        "                    'Lat': float(seq_metadata[frame_idx][metadata_manager.get_index('Lat')]),\n",
        "                    'Lon': float(seq_metadata[frame_idx][metadata_manager.get_index('Lon')]),\n",
        "                    'True_Hdg': float(seq_metadata[frame_idx][metadata_manager.get_index('True_Hdg')]),\n",
        "                    'True_Airspeed': float(seq_metadata[frame_idx][metadata_manager.get_index('True_Airspeed')]),\n",
        "                    'GPS_MSL_Alt': float(seq_metadata[frame_idx][metadata_manager.get_index('GPS_MSL_Alt')])\n",
        "                }\n",
        "\n",
        "                if frame_idx == seq_length - 1:\n",
        "                    frame_dict['center_x'] = int(center_x)\n",
        "                    frame_dict['center_y'] = int(center_y)\n",
        "\n",
        "                seq_metadata_list.append(frame_dict)\n",
        "\n",
        "            metadata_list.append(seq_metadata_list)\n",
        "            sequence_count += 1\n",
        "\n",
        "            # Memory cleanup\n",
        "            del outputs\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\nCollected {sequence_count} sequences\")\n",
        "    return height_fields, confidence_maps, metadata_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMzDjE646fgK"
      },
      "source": [
        "## create_global_grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_global_grid(height_fields, metadata_list):\n",
        "    \"\"\"\n",
        "    Create unified coordinate system for stitching height fields.\n",
        "\n",
        "    Converts geographic coordinates to pixel space while preserving\n",
        "    relative positions and distances between measurements.\n",
        "\n",
        "    Args:\n",
        "        height_fields: List of height field arrays\n",
        "        metadata_list: List of metadata with lat/lon coordinates\n",
        "\n",
        "    Returns:\n",
        "        dict: Grid information containing:\n",
        "            - min/max coordinates\n",
        "            - field positions in pixel space\n",
        "            - measurement point locations\n",
        "            - reference coordinates and scale factor\n",
        "    \"\"\"\n",
        "    # Calculate total bounds\n",
        "    min_x, max_x = float('inf'), float('-inf')\n",
        "    min_y, max_y = float('inf'), float('-inf')\n",
        "    global_positions = []\n",
        "    measurement_points = []\n",
        "\n",
        "    # Get reference lat/lon (from first measurement)\n",
        "    ref_lat = metadata_list[0][-1]['Lat']\n",
        "    ref_lon = metadata_list[0][-1]['Lon']\n",
        "    pixels_per_meter = 0.05  # Adjust this based on your image resolution\n",
        "\n",
        "    # First pass: calculate positions using lat/lon\n",
        "    for i, (height, meta) in enumerate(zip(height_fields, metadata_list)):\n",
        "        h, w = height.shape\n",
        "\n",
        "        # Convert lat/lon to meters from reference point\n",
        "        dx = haversine_distance(ref_lat, ref_lon, ref_lat, meta[-1]['Lon']) * \\\n",
        "             (1 if meta[-1]['Lon'] > ref_lon else -1)\n",
        "        dy = haversine_distance(ref_lat, ref_lon, meta[-1]['Lat'], ref_lon) * \\\n",
        "             (1 if meta[-1]['Lat'] > ref_lat else -1)\n",
        "\n",
        "        # Convert to pixel coordinates\n",
        "        pos_x = int(dx * pixels_per_meter)\n",
        "        pos_y = int(dy * pixels_per_meter)\n",
        "\n",
        "        # Store position\n",
        "        global_positions.append((pos_x, pos_y))\n",
        "\n",
        "        # Store measurement point\n",
        "        measurement_points.append({\n",
        "            'x': pos_x + w // 2,  # Center of image\n",
        "            'y': pos_y + h // 2,\n",
        "            'height': meta[-1].get('validation_height', 0),\n",
        "            'lat': meta[-1]['Lat'],\n",
        "            'lon': meta[-1]['Lon']\n",
        "        })\n",
        "\n",
        "        # Update bounds\n",
        "        min_x = min(min_x, pos_x)\n",
        "        max_x = max(max_x, pos_x + w)\n",
        "        min_y = min(min_y, pos_y)\n",
        "        max_y = max(max_y, pos_y + h)\n",
        "\n",
        "    # Adjust positions relative to minimum bounds\n",
        "    adjusted_positions = []\n",
        "    adjusted_points = []\n",
        "\n",
        "    for (pos_x, pos_y), point in zip(global_positions, measurement_points):\n",
        "        adj_x = pos_x - min_x\n",
        "        adj_y = pos_y - min_y\n",
        "\n",
        "        adjusted_positions.append((adj_x, adj_y))\n",
        "        adjusted_points.append({\n",
        "            'x': point['x'] - min_x,\n",
        "            'y': point['y'] - min_y,\n",
        "            'height': point['height'],\n",
        "            'lat': point['lat'],\n",
        "            'lon': point['lon']\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        'min_x': 0,\n",
        "        'max_x': max_x - min_x,\n",
        "        'min_y': 0,\n",
        "        'max_y': max_y - min_y,\n",
        "        'positions': adjusted_positions,\n",
        "        'measurement_points': adjusted_points,\n",
        "        'reference': {\n",
        "            'lat': ref_lat,\n",
        "            'lon': ref_lon,\n",
        "            'pixels_per_meter': pixels_per_meter\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "qYhLWSIE1bds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## stitch_height_fields"
      ],
      "metadata": {
        "id": "lQsYiAYA1j53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR-ZSqTW6qcL"
      },
      "outputs": [],
      "source": [
        "def stitch_height_fields(height_fields, confidence_maps, metadata_list,\n",
        "                        max_size_pixels=(4096, 4096)):\n",
        "    \"\"\"\n",
        "    Combine multiple height fields into single continuous surface.\n",
        "\n",
        "    Uses weighted averaging with confidence-based blending and\n",
        "    selective smoothing of discontinuities.\n",
        "\n",
        "    Args:\n",
        "        height_fields: List of height field arrays\n",
        "        confidence_maps: List of confidence arrays\n",
        "        metadata_list: List of metadata with positions\n",
        "        max_size_pixels: Maximum output dimensions\n",
        "\n",
        "    Returns:\n",
        "        tuple: (merged_height, merged_conf, metadata)\n",
        "            merged_height: Combined height field\n",
        "            merged_conf: Merged confidence map\n",
        "            metadata: Stitching information\n",
        "    \"\"\"\n",
        "    # Get global coordinate system with measurement points\n",
        "    grid = create_global_grid(height_fields, metadata_list)\n",
        "\n",
        "    # Create output arrays\n",
        "    width = int(grid['max_x'] - grid['min_x'])\n",
        "    height = int(grid['max_y'] - grid['min_y'])\n",
        "\n",
        "    if width > max_size_pixels[1] or height > max_size_pixels[0]:\n",
        "        print(f\"Warning: Size {width}x{height} exceeds limit {max_size_pixels}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Initialize arrays for weighted averaging\n",
        "    height_sum = np.zeros((height, width))\n",
        "    weight_sum = np.zeros((height, width))\n",
        "    merged_height = np.full((height, width), np.nan)\n",
        "\n",
        "    print(f\"\\nCreating global height field of size: ({height}, {width})\")\n",
        "\n",
        "    # Create global confidence mask based on measurement points\n",
        "    merged_conf = create_confidence_mask(\n",
        "        height, width,\n",
        "        grid['measurement_points']\n",
        "    )\n",
        "\n",
        "    # For each sequence\n",
        "    for idx, ((pos_x, pos_y), height_field, conf_map) in enumerate(\n",
        "        zip(grid['positions'], height_fields, confidence_maps)):\n",
        "\n",
        "        h, w = height_field.shape\n",
        "\n",
        "        # Get valid region\n",
        "        valid_mask = ~np.isnan(height_field)\n",
        "\n",
        "        if np.any(valid_mask):\n",
        "            # Get measurement point for this sequence\n",
        "            meas_point = grid['measurement_points'][idx]\n",
        "\n",
        "            # Calculate weights based on distance from measurement point\n",
        "            y_grid, x_grid = np.mgrid[pos_y:pos_y+h, pos_x:pos_x+w]\n",
        "            dist_sq = ((x_grid - meas_point['x'])**2 +\n",
        "                      (y_grid - meas_point['y'])**2)\n",
        "            weights = gaussian_weight(np.sqrt(dist_sq))\n",
        "\n",
        "            # Get target region in merged array\n",
        "            region_slice = (\n",
        "                slice(pos_y, pos_y + h),\n",
        "                slice(pos_x, pos_x + w)\n",
        "            )\n",
        "\n",
        "            # Add weighted heights to sum\n",
        "            height_sum[region_slice][valid_mask] += (\n",
        "                height_field[valid_mask] *\n",
        "                weights[valid_mask] *\n",
        "                conf_map[valid_mask]\n",
        "            )\n",
        "\n",
        "            # Add weights to sum\n",
        "            weight_sum[region_slice][valid_mask] += (\n",
        "                weights[valid_mask] * conf_map[valid_mask]\n",
        "            )\n",
        "\n",
        "    # Calculate final height field\n",
        "    valid_weights = weight_sum > 0\n",
        "    if np.any(valid_weights):\n",
        "        merged_height[valid_weights] = (\n",
        "            height_sum[valid_weights] / weight_sum[valid_weights]\n",
        "        )\n",
        "\n",
        "        # Find areas with sharp height changes (potential banding)\n",
        "        gradient_y = np.abs(np.diff(merged_height, axis=0))\n",
        "        gradient_x = np.abs(np.diff(merged_height, axis=1))\n",
        "\n",
        "        # Create mask for areas with sharp changes\n",
        "        threshold = np.nanpercentile(gradient_y, 95)  # Adjust percentile as needed\n",
        "        band_mask_y = np.zeros_like(merged_height, dtype=bool)\n",
        "        band_mask_y[:-1, :] = gradient_y > threshold\n",
        "        band_mask_y[1:, :] |= gradient_y > threshold\n",
        "\n",
        "        threshold = np.nanpercentile(gradient_x, 95)\n",
        "        band_mask_x = np.zeros_like(merged_height, dtype=bool)\n",
        "        band_mask_x[:, :-1] = gradient_x > threshold\n",
        "        band_mask_x[:, 1:] |= gradient_x > threshold\n",
        "\n",
        "        band_mask = band_mask_x | band_mask_y\n",
        "\n",
        "        # Only smooth the banding areas with moderate confidence\n",
        "        smooth_mask = band_mask & (merged_conf > 0.3) & (merged_conf < 0.7)\n",
        "\n",
        "        if np.any(smooth_mask):\n",
        "            # Very gentle smoothing\n",
        "            smoothed = gaussian_filter(\n",
        "                np.nan_to_num(merged_height, nan=np.nanmean(merged_height)),\n",
        "                sigma=0.5  # Reduced sigma\n",
        "            )\n",
        "\n",
        "            # Apply smoothing only to masked areas\n",
        "            merged_height[smooth_mask] = (\n",
        "                merged_height[smooth_mask] * 0.7 +  # Weighted blend\n",
        "                smoothed[smooth_mask] * 0.3\n",
        "            )\n",
        "\n",
        "    return merged_height, merged_conf, {\n",
        "        'measurement_points': grid['measurement_points'],\n",
        "        'used_sequences': list(range(len(height_fields))),\n",
        "        'bounds': {\n",
        "            'min_x': int(grid['min_x']),\n",
        "            'max_x': int(grid['max_x']),\n",
        "            'min_y': int(grid['min_y']),\n",
        "            'max_y': int(grid['max_y'])\n",
        "        },\n",
        "        'reference': grid['reference']\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rZB6fPsbKqb"
      },
      "source": [
        "# Collect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2of14tbPEKCt"
      },
      "outputs": [],
      "source": [
        "# Collect and visualize\n",
        "height_fields, confidence_maps, metadata_list = collect_height_fields(\n",
        "    trained_model,\n",
        "    full_cloud2cloud_dataset,  # Pass dataset, not loader\n",
        "    start_batch=200,\n",
        "    max_batches=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpmhWm4ebMuJ"
      },
      "source": [
        "# Stitch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3LDyo1B6Jrh"
      },
      "outputs": [],
      "source": [
        "# First, run the stitching\n",
        "final_height, final_confidence, stitching_info = stitch_height_fields(\n",
        "    height_fields, confidence_maps, metadata_list,\n",
        "    max_size_pixels=(4096, 4096)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize"
      ],
      "metadata": {
        "id": "hrNhDilK1sa3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRfS_LQ5dJrS"
      },
      "outputs": [],
      "source": [
        "def visualize_stitching_steps(final_height, final_confidence, stitching_info):\n",
        "    \"\"\"\n",
        "    Create 2D visualization of stitched height field result.\n",
        "\n",
        "    Shows height field, confidence overlay, measurement points,\n",
        "    and diagnostic information.\n",
        "\n",
        "    Args:\n",
        "        final_height: Stitched height field array\n",
        "        final_confidence: Confidence map array\n",
        "        stitching_info: Metadata and measurement points\n",
        "\n",
        "    Returns:\n",
        "        matplotlib.figure.Figure: Visualization figure\n",
        "    \"\"\"\n",
        "    print(f\"\\nGenerating visualization:\")\n",
        "    print(f\"  Used sequences: {stitching_info['used_sequences']}\")\n",
        "    print(f\"  Shape: {final_height.shape}\")\n",
        "\n",
        "    # Create figure with subplots\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "    gs = gridspec.GridSpec(2, 2, height_ratios=[1, 0.05])\n",
        "\n",
        "    # Height field subplot\n",
        "    ax_height = plt.subplot(gs[0, 0])\n",
        "    height_img = ax_height.imshow(final_height, cmap='viridis')\n",
        "    ax_height.set_title(f'Merged Height Field\\n{len(stitching_info[\"used_sequences\"])} sequences')\n",
        "\n",
        "    # Add measurement point markers\n",
        "    legend_handles = []\n",
        "    for i, point in enumerate(stitching_info['measurement_points']):\n",
        "        marker = ax_height.plot(point['x'], point['y'], 'r+',\n",
        "                              label=f'Measurement {i}')[0]\n",
        "        legend_handles.append(marker)\n",
        "\n",
        "    if legend_handles:\n",
        "        ax_height.legend(handles=legend_handles, bbox_to_anchor=(1.05, 1),\n",
        "                        loc='upper left')\n",
        "\n",
        "    # Confidence map subplot\n",
        "    ax_conf = plt.subplot(gs[0, 1])\n",
        "    conf_img = ax_conf.imshow(final_confidence, cmap='RdYlGn')\n",
        "    ax_conf.set_title('Confidence Map')\n",
        "\n",
        "    # Colorbar\n",
        "    ax_colorbar = plt.subplot(gs[1, :])\n",
        "    plt.colorbar(height_img, cax=ax_colorbar, orientation='horizontal',\n",
        "                label='Height (m)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX_k0cS06Pql"
      },
      "outputs": [],
      "source": [
        "# Create the 2D visualization\n",
        "visualize_stitching_steps(\n",
        "    final_height, final_confidence, stitching_info\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOiG8_GFofO-"
      },
      "outputs": [],
      "source": [
        "def visualize_stitched_height_field_3d(height_field, confidence_map, measurement_points, title=\"Stitched Cloud Height Field\"):\n",
        "    \"\"\"\n",
        "    Create interactive 3D visualization of stitched result.\n",
        "\n",
        "    Generates Plotly figure with height surface, confidence overlay,\n",
        "    measurement markers, and diagnostic text.\n",
        "\n",
        "    Args:\n",
        "        height_field: Stitched height field array\n",
        "        confidence_map: Confidence values array\n",
        "        measurement_points: List of LiDAR measurements\n",
        "        title: Plot title\n",
        "\n",
        "    Returns:\n",
        "        plotly.graph_objects.Figure: Interactive 3D visualization\n",
        "    \"\"\"\n",
        "    h, w = height_field.shape\n",
        "    x = np.arange(w)\n",
        "    y = np.arange(h)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "\n",
        "    # Create figure\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add height field surface\n",
        "    fig.add_trace(go.Surface(\n",
        "        x=X,\n",
        "        y=Y,\n",
        "        z=height_field,\n",
        "        colorscale='viridis',\n",
        "        colorbar=dict(\n",
        "            title='Height (m)',\n",
        "            len=0.75,\n",
        "            y=0.5,\n",
        "            thickness=20\n",
        "        ),\n",
        "        opacity=0.8,\n",
        "        showscale=True,\n",
        "        name='Height Field'\n",
        "    ))\n",
        "\n",
        "    # Add confidence overlay\n",
        "    fig.add_trace(go.Surface(\n",
        "        x=X,\n",
        "        y=Y,\n",
        "        z=height_field,\n",
        "        surfacecolor=confidence_map,\n",
        "        colorscale='RdYlGn',\n",
        "        colorbar=dict(\n",
        "            title='Confidence',\n",
        "            len=0.75,\n",
        "            y=0.5,\n",
        "            x=1.1,\n",
        "            thickness=20\n",
        "        ),\n",
        "        opacity=0.3,\n",
        "        showscale=True,\n",
        "        name='Confidence'\n",
        "    ))\n",
        "\n",
        "    # Add measurement points\n",
        "    point_xs = [p['x'] for p in measurement_points]\n",
        "    point_ys = [p['y'] for p in measurement_points]\n",
        "    point_zs = [height_field[int(p['y']), int(p['x'])] for p in measurement_points]\n",
        "\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=point_xs,\n",
        "        y=point_ys,\n",
        "        z=point_zs,\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=5,\n",
        "            color='red',\n",
        "            symbol='diamond'\n",
        "        ),\n",
        "        name='LiDAR Measurements'\n",
        "    ))\n",
        "\n",
        "    # Set camera based on which dimension is longer\n",
        "    if h > w:  # Vertical orientation\n",
        "        camera = dict(eye=dict(x=2, y=0, z=1.5))\n",
        "    else:  # Horizontal orientation\n",
        "        camera = dict(eye=dict(x=0, y=2, z=1.5))\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        scene=dict(\n",
        "            xaxis_title='X Distance (pixels)',\n",
        "            yaxis_title='Y Distance (pixels)',\n",
        "            zaxis_title='Height (m)',\n",
        "            aspectratio=dict(x=w/max(h,w), y=h/max(h,w), z=0.3),\n",
        "            camera=camera\n",
        "        ),\n",
        "        width=1200,\n",
        "        height=800,\n",
        "        margin=dict(t=50, b=0, l=0, r=0)\n",
        "    )\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKW6SuVC5st5"
      },
      "outputs": [],
      "source": [
        "# Create the 3D visualization\n",
        "fig = visualize_stitched_height_field_3d(\n",
        "    final_height,\n",
        "    final_confidence,\n",
        "    stitching_info['measurement_points'],\n",
        "    title=\"Stitched Cloud Height Field with Confidence Overlay\"\n",
        ")\n",
        "\n",
        "# Display the 3D figure\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}